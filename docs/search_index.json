[
["index.html", "A Practical Guide for Machine Learning and R Shiny Preface Structure of the book", " A Practical Guide for Machine Learning and R Shiny Cardy Moten III 2018-02-25 Preface For the next two days, we will focus on the “how” of machine learning. This does not mean we are simply glossing over the “why” of machine learning, but quite frankly, we will not have the time in this short course. After we cover machine learning we will spend two days learning about R Shiny and will build a couple of web applications with it. Having said all that, a good textbook reference to start learning the about the “why” of machine learning mathematics is the book The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman (2009). For a more hands-on approach, read Neural Networks and Deep Learning by Nielsen (2015). Structure of the book Chapter 1 provides a general overveiw of machine learning. Chapters 2 - 5 covers machine learning algorithms and a practical exercise. Chapters 6 focuses on R shiny. References "],
["ml-overview.html", "Chapter 1 Machine Learning Overview 1.1 What is Machine Learning? 1.2 What machine learning is not 1.3 What do I need to know to get started with machine learning?", " Chapter 1 Machine Learning Overview 1.1 What is Machine Learning? In simple terms, machine learning is a statistical modeling technique that focuses on algorithmic-driven models instead of data-driven models. Figure 1.1 shows a comparison of these two approaches and is an adaptation of a figure in this article by Breiman (2001). Figure 1.1: A statistical model comparison. The model at the top illustrates the original problem to be solved; which is how to best represent the relationship between a vector in input variables \\(x\\) and a corresponding response variable \\(y\\)? The data-modeling approach in the lower left assumes that a stochastic data model represents this relationship (Breiman 2001). The analyst using this method will then estimate the parameters of the model and validate the model using goodness of fit tests. Thus, a general function representing this approach would be \\[\\text{response variables} = f(\\text{predictor variables, random noise, parameters})\\] In comparison to the data-modeling approach, the analytic-modeling approach doesn’t make any assumptions about the relationship between \\(x\\) and \\(y\\). Instead, the focus is on finding a function \\(f(x)\\) that will input a variable \\(x\\) to predict a value for \\(y\\) and is the model in the lower right of Figure 1.1. 1.2 What machine learning is not While machine learning has made significant breakthroughs in many areas (web search, image recognition, game AI, medical diagnosis, prediction, and classification to name a few), it is a technique that is not without limitations.For this course, we will cover a few of these limitations, but you can read a more in-depth critical analysis presented by Marcus (2018). Machine learning models require a lot of data For machine learning algorithms to learn well, they need lots of data. A review of the types of use cases that these models have achieved a lot of success will give you an appreciation for the amount of data needed to train, validate, and use these models into a production environment. Machine learning models are not a substitute for domain expertise When appropriately used, machine learning models can help us to find hidden patterns in our data that are not possible on our own. While this is a great benefit, it does not alleviate the burden of understanding what these results mean, and whether they even matter when it comes to finding an adequate solution to a business problem. Machine learning models require constant maintenance This statement should not come as a surprise since machine learning models are an extension of statistics in some ways. In general machine learning problems work best with applications that are similar to their training context. Even then, mistakes can happen, and the modeler must make adjustments. 1.3 What do I need to know to get started with machine learning? Once you are familiar with the concepts in machine learning, you will realize that most of the ideas are similar to other statistical methods. The key here is learning the following thoughts. The role of data in machine learning Machine learning algorithms primarily use tabular data. Nevertheless, machine learning literature has its origin in computer science, and some of the terms used to describe data are slightly different than traditional statistics. Primarily, the regressor variables in machine learning are called features. Features are used to make predictions on target or outcome measures. The difference between parametric and nonparametric machine learning algorithms Parametric Models Parametric machine learning models are based on modeling assumptions about the mapping of the feature variables to the target variable. Some examples of parametric machine learning models are Linear Regression, Logistic Regression, and Linear Discriminant Analysis. In general parametric models are simpler, faster to compute, and require fewer data. A key drawback, however, is that parametric models are rigid and do not adjust well to variations in the data (Brownlee 2017). Nonparametric Models Nonparametric machine learning models are not based on any modeling assumptions about the mapping of the feature variables to the target variable. Some examples of nonparametric models are \\(k\\)-Nearest Neighbors, Naive Bayes, Support Vector Machines, Neural Networks, and Decision Trees. In general nonparametric models more complex, slower to compute, and require more data than parametric models. A principal drawback of nonparametric models is their inconsistent results (Brownlee 2017). The difference between supervised, unsupervised and semi-supervised learning The key takeaway here is that supervised learning models predict a value; unsupervised learning models describe associations and patterns in a dataset, and semi-supervised learning models are combinations of the two (Hastie, Tibshirani, and Friedman 2009). Furthermore, supervised learning involves data that has labeled features and target variables. In contrast, unsupervised learning uses data that does not have any feature or target variable labels. The bias-variance tradeoff The company EliteDataScience created an infographic that provides a vivid explanation of the importance of the bias-variance tradeoff. The key to this concept is to understand that bias is the degree to which the predictions or classifications are different from the actual value. Parametric models tend to have higher bias than nonparametric models. Conversely, variance refers to how sensitive the model is to noise in the data. For variance error, nonparametric models will tend to be more sensitive to different sets of training data than parametric models. Another critical aspect is that bias and variance have an inverse relationship. In other words, models that usually have a high bias also have low variance. By contrast, models that have low bias will usually exhibit high variance. Hence, it is essential that you tune your modeling parameters and try as many different models as possible to evaluate model performance. Overfitting and underfitting and what to do about it For underfitted models, the answer is simple; don’t use the model. Overfitting, however, is a different problem. In this case, the model fits the data too well to apply for any general setting. Thus, you will have to spend some time making adjustments to the model and determining how best to engineer the features to reduce the risk of overfitting. References "],
["lin-algs.html", "Chapter 2 Linear Algorithms 2.1 Gradient Descent 2.2 Practical Exercises 2.3 Linear Discriminant Analysis 2.4 Practical Exercise", " Chapter 2 Linear Algorithms 2.1 Gradient Descent One of the most common concepts for all machine learning algorithms is optimization. Of the many optimization methods, the most widely used optimization method in machine learning is gradient descent. This extensive use of gradient descent is because gradient descent is straightforward to learn and compatible with any machine learning algorithm. For this section, we will use gradient descent with linear and logistic regression. Linear Regression Model Before discussing the gradient descent algorithm, let’s review the linear regression. Recall the general linear regression equation is: \\[ y\\ =\\ h\\left(x\\right)\\ =\\ \\sum_{i=1}^Nw_ix_i\\ +\\ b\\ +\\ \\epsilon \\] where \\(y\\) is the target variable, \\(x\\) is the feature variables, \\(w_i\\) are the weights of the \\(i\\)th feature variable, \\(b\\) is the bias, and \\(\\epsilon\\) is the irreducible error. We use machine learning algorithms to estimate the bias and the weights of the feature variables. Cost Function Simple Linear Regression In order to optimize the weight and the bias variable, we need to optimize the cost (loss) function. For linear regression this function, \\(J\\left(w,b\\right)\\) is the Mean Squared Error (MSE) and we can calculate it by: \\[ MSE\\ =\\ J\\left(w,b\\right)\\ =\\ \\frac{1}{m}\\sum_{i=1}^m\\left(y_i-\\left(wx_i+b\\right)\\right)^2 \\] Since we are adjusting the cost function by the weight and the bias parameters, we must take the partial derivative with respect to each of these to calculate the gradient. \\[ J&#39;\\left(w,b\\right)\\ = \\begin{bmatrix} \\frac{\\partial J}{\\partial w}\\\\ \\frac{\\partial J}{\\partial b} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{m}\\sum-2\\cdot x_i\\left(y_i-\\left(wx_i+b\\right)\\right)\\ =\\ \\delta_w\\\\ \\frac{1}{m}\\sum_{ }^{ }-2\\cdot\\left(y_i-\\left(wx_i+b\\right)\\right)\\ =\\ \\delta_b \\end{bmatrix} \\] Multiple Linear Regression For multiple linear regression, we introduce a parameter matrix \\(\\theta\\) that contains the bias and weight parameters where: \\[ \\begin{align} h_{\\theta}\\left(x\\right) &amp;=\\theta_0\\ +\\ \\theta_1x_1\\ +\\ \\dots\\ +\\ \\theta_nx_n\\\\ h\\left(x\\right)\\ &amp;=\\ \\sum_{i=0}^n\\theta_ix_i\\ =\\ \\theta^Tx \\end{align} \\]The cost function is: \\[ J\\left(\\theta\\right)\\ =\\ \\frac{1}{2m}\\sum_{i=1}^m\\left(h_{\\theta}(x^{\\left(i\\right)})-y^{\\left(i\\right)}\\right)^2 \\] In matrix form, the cost function becomes \\[ J\\left(\\theta\\right)\\ =\\ \\frac{1}{2m}\\left(X\\theta\\ -\\ y\\right)^T\\left(X\\theta-y\\right) \\] where \\(X\\) is a \\(m\\ \\times\\ n\\) design matrix, \\(\\theta\\) is a \\(n\\ \\times\\ 1\\) parameter matrix and \\(y\\) is a \\(m\\ \\times\\ 1\\) vector of observed targets. Taking the derivative with respect to \\(\\theta\\) yields: \\[ J&#39;\\left(\\theta\\right)\\ =\\ \\frac{1}{m}X^T\\left(X\\theta-y\\right) \\] See the video below for an example derivation of the derivative of the cost matrix: Logistic Regression Another application of the gradient descent algorithm is for logistic regression. Recall that we use logistic regression when the target variable is categorical, and there are only two possible classifications. We show the general equation as \\[ h_{\\theta}\\left(x\\right)\\ =\\ g\\left(\\theta^Tx\\right)\\ =\\ \\frac{1}{1+e^{-\\theta^Tx}} \\] and \\[ g\\left(z\\right)\\ =\\ \\frac{1}{1+e^{-z}} \\] The above equation is called a sigmoid or logistic function. Essentially, we first perform a linear regression on the weights and bias and then feed that predicted value into the sigmoid function to map a real value between 0 and 1. sigmoid &lt;- function(z){ res &lt;- 1 / (1 + exp(-z)) res } Figure 2.1: Example Sigmoid function plot. The cost function for logistic regression will differ now, that the function we are analyzing is non-linear. First let’s assume the following: \\[ \\begin{align} P\\left(y\\ =\\ 1|x;\\theta\\right)\\ &amp;=\\ h_{\\theta}\\left(x\\right)\\\\ P\\left(y\\ =\\ 0\\ |x;\\theta\\right)\\ &amp;=\\ 1-h_{\\theta}\\left(x\\right)\\\\ P\\left(y|x;\\theta\\right)\\ &amp;=\\ \\left(h_{\\theta}\\left(x\\right)\\right)^y\\left(1-h_{\\theta}\\left(x\\right)\\right)^{1-y} \\end{align} \\] We now can find the log cross-entropy cost by: \\[ J\\left(\\theta\\right)\\ =\\ -\\frac{1}{m}\\sum_{i=0}^m\\left[y^{\\left(i\\right)}\\log\\left(h_{\\theta}(x^{\\left(i\\right)})\\right)\\ +\\ \\left(1-y^{\\left(i\\right)}\\right)\\log\\left(1-h_{\\theta}(x^{\\left(i\\right)})\\right)\\right] \\] where \\(h_\\theta(x)\\) is the sigmoid function. When taking the derivative with respect to \\(\\theta\\), recall that \\(g&#39;\\left(z\\right)\\ =\\ g\\left(z\\right)\\left(1-g\\left(z\\right)\\right)\\). Thus, \\[ \\begin{align} J&#39;\\left(\\theta\\right)\\ &amp;=\\ \\frac{\\partial}{\\partial J}-\\frac{1}{m}\\sum_{i=0}^m\\left[y^{\\left(i\\right)}\\log\\left(h_{\\theta}(x^{\\left(i\\right)})\\right)\\ +\\ \\left(1-y^{\\left(i\\right)}\\right)\\log\\left(1-h_{\\theta}(x^{\\left(i\\right)})\\right)\\right]\\\\ &amp;=-\\frac{1}{m}\\left(y\\frac{1}{g\\left(\\theta^Tx\\right)}-\\left(1-y\\right)\\frac{1}{1-g\\left(\\theta^Tx\\right)}\\right)\\frac{\\partial}{\\partial\\theta_j}g\\left(\\theta^Tx\\right)\\\\ &amp;=-\\frac{1}{m}\\left(y\\frac{1}{g\\left(\\theta^Tx\\right)}-\\left(1-y\\right)\\frac{1}{1-g\\left(\\theta^Tx\\right)}\\right)g\\left(\\theta^Tx\\right)\\left(1-g\\left(\\theta^Tx\\right)\\right)\\frac{\\partial}{\\partial\\theta_j}\\theta^Tx\\\\ &amp;=-\\frac{1}{m}\\left(y\\left(1-g\\left(\\theta^Tx\\right)\\right)-\\left(1-y\\right)g\\left(\\theta^Tx\\right)\\right)x_j\\\\ &amp;=-\\frac{1}{m}\\left(y-g\\left(\\theta^Tx\\right)\\right)x_j\\\\ &amp;=\\frac{1}{m}\\left(h_{\\theta}\\left(x\\right)-y\\right)x_j\\\\ \\end{align} \\] What is interesting to note is that this gradient function looks precisely like the gradient function for linear regression. The difference, however, is that the function \\(h_\\theta(x)\\) is a sigmoid function and not a linear function of the weights and bias parameters. For further details of the above derivations see Ng (2000) and Fortuner (2017). Gradient Descent Algorithm Finally, to solve for the optimal weight and bias, we will add a learning parameter, \\(\\alpha\\), to adjust the steps of the gradient. Simple Linear Regression The algorithm we will use is: \\[ \\text{Repeat until convergence } \\{\\\\ w\\ :=\\ w-\\alpha\\delta_w\\\\ b\\ :=\\ b\\ -\\ \\alpha\\delta_b\\\\ \\} \\] Multiple Linear Regression For multiple linear regression the algorithm changes to: \\[ \\text{Repat until convergence } \\{\\\\ \\theta_j\\ :=\\ \\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^m\\left(h_{\\theta}(x^{\\left(i\\right)})-y^{\\left(i\\right)}\\right)x_j^{\\left(i\\right)}\\\\ \\} \\] In this algorithm we are simultaneously updating the weights, \\(\\theta_j\\), for all \\(j\\ \\in\\left(0,\\dots,n\\right)\\). Recall that \\(\\theta_0\\) is the bias term and \\(x_0^1\\ =\\ 1\\). In matrix form, our algorithm will look like this: \\[ \\text{Repat until convergence } \\{\\\\ \\delta=\\ \\frac{1}{m}X^T\\left(X\\theta-y\\right)\\\\ \\theta:=\\ \\theta-\\alpha\\delta\\\\ \\} \\] Logistic Regression The matrix form of the stochastic gradient descent algorithm has the form: \\[ \\text{Repat until convergence } \\{\\\\ \\delta=\\ \\frac{1}{m}X^T\\left(sigmoid\\left(X\\theta\\right)-y\\right)\\\\ \\theta:=\\ \\theta-\\alpha\\delta\\\\ \\} \\] Gradient Descent Intuition Figure 2.2 demonstrates the basic intuition behind the gradient algorithm. Fundamentally, if we pick a point along the graph of the cost function, and the gradient is negative, the algorithm will update by moving the more to the right. Conversely, if the gradient is positive, the algorithm will move the cost value more to the left. Figure 2.2: A simple example of gradient descent. Figure 2.3 is a multi-dimensional view of the cost function, and the underlying concept is still the same. Figure 2.3: A surface plot of a quadratic cost function. Figure 2.4 is a plot of the log-entropy function for logistic regression. Figure 2.4: A simple example of gradient descent. While the ideal cost function to minimize would be a convex function, this is not always practical and there are ways to deal with that as discussed in the following video. 2.2 Practical Exercises This practical exercieses are based on code provided by Fortuner (2017) and Brownlee (2017). 2.2.1 Simple Linear Regression Data Suppose we have the following dataset in which we have a unique Company ID, radio advertising expenses in dollars, and annual sales as a result of those expenses in dollars. data &lt;- read.csv(&quot;data/Advertising-Radio.csv&quot;,header=TRUE) head(data) ## company radio sales ## 1 1 37.8 22.1 ## 2 2 39.3 10.4 ## 3 3 45.9 9.3 ## 4 4 41.3 18.5 ## 5 5 10.8 12.9 ## 6 6 48.9 7.2 A view of the data shows that there appears to be a positive correlation between radio advertising spending and sales. plot(data$radio,data$sales,xlab= &quot;Radio&quot;, ylab = &quot;Sales&quot;, col=&quot;dodgerblue&quot;,pch=20) Making Predictions For this model, we want to predict sales based on the amount spent for radio advertising. Thus our formula will be \\[\\text{Sales} = \\text{Weight} \\times \\text{Radio} + \\text{Bias}\\] The gradient descent algorithm will attempt to learn the optimal values for the Weight and Bias. Simple Regression Function simple_regress &lt;- function(features,weight,bias){ return(weight*features + bias) } Cost function Code cost_function &lt;- function(features,targets,weight,bias){ num_items &lt;- length(targets) total_error &lt;- 0 for(i in seq_along(1:num_items)){ total_error &lt;- total_error + (targets[i] - (weight * features[i] + bias))^2 } return(total_error/num_items) } Gradient Descent Code update_weight &lt;- function(features,targets,weight,bias,learning_rate){ delta_weight &lt;- 0 delta_bias &lt;- 0 num_items &lt;- length(targets) for(i in seq_along(1:num_items)){ #Calculate gradients error &lt;- (targets[i] - (weight * features[i] + bias)) delta_weight &lt;- delta_weight + -2 * features[i] * error delta_bias &lt;- delta_bias + -2 * error } weight &lt;- weight - learning_rate * (delta_weight/num_items) bias &lt;- bias - learning_rate * (delta_bias/num_items) res &lt;- c(weight,bias) res } Training the model We are now ready to train the final model. To do this we will iterate over a set number of trials and update the weight and bias parameters at each iteration. We will also track the cost history. train &lt;- function(features,targets,weight,bias,learning_rate,iters){ cost_history &lt;- numeric(iters) coef_history &lt;- list() for(i in seq_along(1:iters)){ tmp_coef &lt;- update_weight(features,targets,weight,bias,learning_rate) weight &lt;- tmp_coef[1] bias &lt;- tmp_coef[2] coef_history[[i]] &lt;- c(bias,weight) cost &lt;- cost_function(features,targets,weight = weight, bias = bias) cost_history[i] &lt;- cost if(i == 1 | i %% 10 == 0){ cat(&quot;iter: &quot;, i, &quot;weight: &quot;, weight, &quot;bias: &quot;, bias, &quot;cost: &quot;, cost, &quot;\\n&quot;) } } res &lt;- list(Weight = weight, Bias = bias, Cost = cost_history,Coefs = coef_history) res } fit &lt;- train(features = data$radio,targets = data$sales,weight = 0.03, bias = 0.0014, learning_rate = 0.001,iters = 30) ## iter: 1 weight: 0.7255664 bias: 0.02804636 cost: 86.42445 ## iter: 10 weight: 0.484637 bias: 0.06879067 cost: 42.72917 ## iter: 20 weight: 0.4837035 bias: 0.1219333 cost: 42.44643 ## iter: 30 weight: 0.4820883 bias: 0.1747496 cost: 42.1673 The plot below shows how the train() funtion iterated through the coefficient history. plot(data$radio,data$sales,xlab= &quot;Radio&quot;, ylab = &quot;Sales&quot;, col=&quot;dodgerblue&quot;,pch=20,main=&quot;Final Plot With Coefficient History&quot;) for(i in 1:30){ abline(coef=fit$Coefs[[i]], col = rgb(0.8,0,0,0.3)) } abline(coef = c(fit$Bias,fit$Weight),col=&quot;red&quot;) This is a plot of the cost history. plot(fit$Cost,type=&quot;l&quot;,col=&quot;blue&quot;, xlab = &quot;Iteration&quot;,ylab=&quot;Cost&quot;,main = &quot;Error Rate Per Iteration&quot;) Exercises Run the command res &lt;- lm(data$sales ~ data$radio) and note the values for the weight and bias. Plot the fitted line from res with the data and comapre that line to the trained model. Adjust the fit object to obtain an estimate close to the noted parameters. 2.2.2 Multiple Linear Regression For this exercise, we will predict total sales based on TV, Radio, and Newspaper advertising costs. Data multi_data &lt;- read.csv(&quot;data/Advertising.csv&quot;, header = TRUE) head(multi_data) ## company TV radio newspaper sales ## 1 1 230.1 37.8 69.2 22.1 ## 2 2 44.5 39.3 45.1 10.4 ## 3 3 17.2 45.9 69.3 9.3 ## 4 4 151.5 41.3 58.5 18.5 ## 5 5 180.8 10.8 58.4 12.9 ## 6 6 8.7 48.9 75.0 7.2 Since we are now dealing with multiple variables, we will need to view a pairs plot # Code for panel.cor found at https://www.r-bloggers.com/scatter-plot-matrices-in-r/ panel.cor &lt;- function(x, y, digits = 2, cex.cor, ...) { usr &lt;- par(&quot;usr&quot;); on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) # correlation coefficient r &lt;- cor(x, y) txt &lt;- format(c(r, 0.123456789), digits = digits)[1] txt &lt;- paste(&quot;r= &quot;, txt, sep = &quot;&quot;) text(0.5, 0.6, txt) # p-value calculation p &lt;- cor.test(x, y)$p.value txt2 &lt;- format(c(p, 0.123456789), digits = digits)[1] txt2 &lt;- paste(&quot;p= &quot;, txt2, sep = &quot;&quot;) if(p&lt;0.01) txt2 &lt;- paste(&quot;p= &quot;, &quot;&lt;0.01&quot;, sep = &quot;&quot;) text(0.5, 0.4, txt2) } pairs(multi_data[,2:5],lower.panel = panel.smooth, upper.panel = panel.cor) 2.2.2.1 Cost Function Code multi_cost &lt;- function(features,target,theta){ sum((features %*% theta - target)^2) / (2*length(target)) } Training Function Code multi_train &lt;- function(features,target,theta,learn_rate,iters){ cost_history &lt;- double(iters) for(i in seq_along(cost_history)){ error &lt;- (features %*% theta - target) delta &lt;- t(features) %*% error / length(target) theta &lt;- theta - learn_rate * delta cost_history[i] &lt;- multi_cost(features,target,theta) } res &lt;- list(Coefs = theta, Costs = cost_history) res } Results To make computing the gradient easier, we will normalize the feature data such that \\(x \\in [-1,1]\\). normalize_data &lt;- function(data){ cols &lt;- ncol(data) for(i in 1:cols){ tmp_mean &lt;- mean(data[,i]) tmp_range &lt;- range(data[,i])[2] - range(data[,i])[1] res &lt;- (data[,i] - tmp_mean) / tmp_range data[,i] &lt;- res } data } multi_features &lt;- multi_data[,2:4] multi_features &lt;- normalize_data(multi_features) multi_target &lt;- multi_data[,5] features_matrix &lt;- cbind(1,as.matrix(multi_features)) theta &lt;- matrix(0,nrow=ncol(features_matrix)) rownames(theta) &lt;- c(&quot;Intercept&quot;,names(multi_features)) num_iters &lt;- 1000 learn_rate &lt;- 0.0005 multi_fit &lt;- multi_train(features_matrix,multi_target,theta,learn_rate,num_iters) multi_fit$Coefs ## [,1] ## Intercept 5.5184872 ## TV 0.5767349 ## radio 0.4366550 ## newspaper 0.1098191 plot(multi_fit$Costs,type=&quot;l&quot;,col=&quot;blue&quot;,xlab=&quot;Iteration&quot;,ylab=&quot;Cost&quot;,main=&quot;Error Rate Per Iteration&quot;) test_fit &lt;- lm(sales ~ TV + radio + newspaper, data = multi_data) Exercises Tune the iterations and the learning rate and attempt to reduce the model cost. Run the command test_fit &lt;- lm(sales ~ TV + radio + sales, data = multi_data) Compute the cost of test_fit (Hint: use names(test_fit) to find out how to extract the coefficients of the model ) 2.2.3 Logistic Regression During this exercise, we will classify whether studens will pass (1) or fail (0) a test based on the amount of hours spent studying and hours slept. Data log_data &lt;- read.csv(&quot;data/data_classification.csv&quot;,header=TRUE) head(log_data) ## studied slept passed ## 1 4.855064 9.63996157 1 ## 2 8.625440 0.05892653 0 ## 3 3.828192 0.72319923 0 ## 4 7.150955 3.89942042 1 ## 5 6.477900 8.19818055 1 ## 6 1.922270 1.33142727 0 The plot below shows the current data color_vec &lt;- ifelse(log_data$passed==1,&quot;orange&quot;,&quot;blue&quot;) plot(log_data$slept,log_data$studied,col=color_vec,xlab=&quot;Hours Slept&quot;,ylab=&quot;Hours Studied&quot;) legend(&quot;topright&quot;,c(&quot;Pass&quot;,&quot;Fail&quot;),col=c(&quot;orange&quot;,&quot;blue&quot;),pch=c(1,1)) Generate a vector of predictions log_predict &lt;- function(features, theta){ z &lt;- features %*% theta res &lt;- sigmoid(z) res } Cost function code log_cost &lt;- function(features, theta, targets){ m &lt;- length(targets) g &lt;- log_predict(features,theta) res &lt;- (1/m) * sum((-targets * log(g)) - ((1-targets) * log(1-g))) res } Training model code log_train &lt;- function(features,theta,targets,learn_rate, iters){ cost_history &lt;- double(iters) for(i in seq_along(cost_history)){ preds &lt;- log_predict(features,theta) error &lt;- (preds - targets) delta &lt;- t(features) %*% error / length(targets) theta &lt;- theta - learn_rate * delta cost &lt;- log_cost(features,theta,targets) cost_history[i] &lt;- cost if(i == 1 | i %% 1000 == 0){ cat(&quot;iter: &quot;, i, &quot;cost: &quot;, cost, &quot;\\n&quot;) } } res &lt;- list(Coefs = theta, Costs = cost_history) res } Decision boundary code boundary &lt;- function(prob){ res &lt;- ifelse(prob&gt;=.5,1,0) res } Classification accuracy code log_accuracy &lt;- function(preds,targets){ diff &lt;- preds - targets res &lt;- 1 - sum(diff)/length(diff) } Results log_features &lt;- log_data[,1:2] log_targets &lt;- log_data[,3] log_design &lt;- cbind(1,as.matrix(log_features)) log_theta &lt;- matrix(0,nrow=ncol(log_design)) rownames(log_theta) &lt;- c(&quot;Intercept&quot;,names(log_features)) learn_rate &lt;- 0.02 num_iters &lt;- 3000 log_fit &lt;- log_train(log_design,log_theta,log_targets,learn_rate,num_iters) ## iter: 1 cost: 0.6582674 ## iter: 1000 cost: 0.4469503 ## iter: 2000 cost: 0.3773433 ## iter: 3000 cost: 0.3408168 log_fit$Coefs ## [,1] ## Intercept -3.8281563 ## studied 0.4802045 ## slept 0.3435157 plot(log_fit$Costs,type=&quot;l&quot;,col=&quot;blue&quot;) predictions &lt;- log_predict(log_design,log_fit$Coefs) classifications &lt;- boundary(predictions) fit_accuracy &lt;- log_accuracy(classifications,log_targets) fit_accuracy ## [1] 0.91 plot(predictions,col=color_vec) abline(h=0.5,lty=2) title(&quot;Actual Classification vs Predicted Probability&quot;) legend(&quot;topright&quot;,c(&quot;Pass&quot;,&quot;Fail&quot;),col=c(&quot;orange&quot;,&quot;blue&quot;),pch=c(1,1),horiz = TRUE) Exercises Tune the iterations and the learning rate and attempt to improve the model accuracy. Run the command test_log &lt;- glm(passed~slept+studied,family = 'binomial',data=log_data) Compare the cost and accuracy of test_log with log_fit. 2.3 Linear Discriminant Analysis Another method to classify target variables is to use linear discriminant analysis (LDA). Similar to logistic regression we want to find \\(\\Pr\\left(Y\\ =\\ k\\ |X\\ =\\ x\\right)\\). Simply put, we want to determine the probability that the target variable \\(Y\\) maps to \\(K\\ \\ge\\ 2\\) classes given a value \\(X\\ =\\ x\\). Using Bayes theorem, we can find this probability by \\[ \\Pr\\left(Y\\ =\\ k\\ |X\\ =\\ x\\right)\\ =\\ p_k\\left(x\\right)\\ =\\ \\frac{\\pi_kf_k\\left(x\\right)}{\\sum_{l=1}^K\\pi_lf_l\\left(x\\right)} \\] Where \\(\\pi_k\\) is the probability of \\(Y=k\\) and \\(f_k(x)\\) is the likelihood function of \\(P\\left(X\\ =\\ x\\ |Y\\ =\\ k\\right)\\). In most cases \\(f_k(x)\\) is assumed to be Normal with mean \\(\\mu_k\\) and standard deviation \\(\\sigma_k\\). A reasonable question to ask is why we would use LDA when we could use logistic regression? There are a few reasons: Logistic regression is for binary classification. You will need to use LDA and other non-linear variants for more than two classes. Logistic regression parameter estimates are brittle with well-separated classes. LDA is more robust to this type of data. Logistic regression is also brittle with small samples. LDA performs better especially if the predictors are approximately normally distributed. See Brownlee (2017) and James et al. (2013) for more details. LDA Intuition Figure 2.5, adapted from James et al. (2013), shows the fundamental principle of LDA. On the left are two normal densities. The dashed vertical line indicates the Bayes decision boundary for classification of new data. In this example, an observation’s classification is green if its value is less than zero and red otherwise. On the right are 20 observations drawn from each class. The solid vertical line is the LDA boundary while the dashed vertical line is the Bayes decision boundary. Thus, we can observe that the LDA boundary will vary from the Bayes decision boundary. Also, we can note that some observations will overlap between classes. Figure 2.5: Normal densities with a Bayes decsion boundry adapted from James et al. (2013). Investigating this overlap deeper, Figure 2.6, adapted from Kuhn and Johnson (2013), shows the goal of LDA. Mostly, the purpose of LDA is to determine a boundary that maximizes the variance between groups of data Figure 2.6: A comparison of between and within group variance adapted from Kuhn and Johnson (2013). LDA Estimates for One Predictor When we have only one predictor, we want to obtain estimates for \\(f_k(x)\\) and \\(p_k(x)\\) and classify an observation for into a class in which \\(p_k(x)\\) has the greatest value. As stated previously, we will assume \\(f_k(x)\\) is Gaussian which means \\[ f\\left(x\\right)\\ =\\ \\frac{1}{\\sqrt{2\\pi\\sigma_k}}\\exp\\left(-\\frac{1}{2\\sigma_k^2}\\left(x-\\mu_k\\right)^2\\right) \\] where \\(\\mu_k\\) and \\(\\sigma_k^{2}\\) are the mean and variance for class \\(k\\). We will also assume the variance is the same for all \\(K\\) classes which means \\(\\sigma_1^2\\ =\\ \\dots\\ =\\ \\sigma_k^2\\) (James et al. 2013). Using these assumptions our Bayes formulation is now \\[ \\begin{align} p_k\\left(x\\right)\\ &amp;=\\ \\frac{\\pi_kf_k\\left(x\\right)}{\\sum_{l=1}^K\\pi_lf_l\\left(x\\right)}\\\\ &amp;= \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(x-\\mu_k\\right)^2\\right)}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(x-\\mu_l\\right)^2\\right)} \\end{align} \\] To determine which class has the highest likelihood for a particular observation, we will convert \\(p_k(x)\\) into a scoring function \\(\\delta_k(x)\\) which is called the discriminant scoring function. The key to understanding the derivation of \\(\\delta_k(x)\\) is that we will keep only the parameters that affect the maximum classification probability and ignore those parameters that are constant for all \\(K\\) classes. \\[ p_k\\left(x\\right)\\ =\\ \\frac{\\pi_{k\\ }\\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(x-\\mu_k\\right)^2\\right)}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(x-\\mu_l\\right)^2\\right)\\ } \\] Observing the original form of \\(p_k(x)\\) we notice that the denominator is the same for all classes so we can safely ignore it. \\[ p_k^{&#39;}\\left(x\\right)\\ =\\ \\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(x-\\mu_k\\right)^2\\right) \\] Next we will take the log of \\(p_k^{&#39;}(x)\\) \\[ p_k^{&#39;&#39;}\\left(x\\right)\\ =\\ \\ln\\left(\\pi_k\\right)\\ +\\ \\ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma}}\\right)\\ +\\ \\ln\\left(\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(x-\\mu_k\\right)^2\\right)\\right) \\] Using the similar logic we used previously, we notice the \\(ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma}}\\right)\\) term is constant across all \\(K\\) classes, and we can omit this term. \\[ \\begin{align} p_k^{&#39;&#39;&#39;}\\ &amp;=\\ \\ln\\left(\\pi_k\\right)\\ +\\ \\ln\\left(\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(x-\\mu_k\\right)^2\\right)\\right)\\\\ &amp;=\\ \\ln\\left(\\pi_k\\right)\\ +\\ -\\frac{1}{2\\sigma^2}\\left(x-\\mu_k\\right)^2\\\\ &amp;=\\ \\ln\\left(\\pi_k\\right)\\ +\\ -\\frac{1}{2\\sigma^2}\\left(x^2\\ -\\ 2x\\mu_k\\ +\\ \\mu_k^2\\right)\\\\ &amp;=\\ \\ln\\left(\\pi_k\\right)\\ -\\frac{x^2}{2\\sigma^2}\\ +\\ \\frac{2x\\mu_k}{2\\sigma^2}\\ -\\ \\frac{\\mu_k^2}{2\\sigma^2} \\end{align} \\] We can eliminate the \\(-\\frac{x^2}{2\\sigma^2}\\) term since it is constant across all \\(K\\) classes leaving us with a final value of \\[ \\delta_k\\left(x\\right)\\ =\\ \\frac{x\\mu_k}{\\sigma^2}\\ -\\ \\frac{\\mu_k^2}{2\\sigma^2}\\ +\\ \\ln\\left(\\pi_k\\right) \\] In practice, the parameters \\(\\mu_k\\), \\(\\sigma_k^{2}\\) and \\(\\pi_k\\) are estimated from the data by the following methods: \\[ \\begin{align} \\hat{\\pi_k}\\ &amp;=\\ \\frac{n_k}{n}\\\\ \\hat{\\mu_k}\\ &amp;=\\ \\frac{1}{n_k}\\sum_{i:y_i=k}^{ }x_i\\\\ \\hat{\\sigma^2}\\ &amp;=\\ \\frac{1}{n-K}\\sum_{k=1}^K\\sum_{i:y_i=k}^{ }\\left(x-\\hat{\\mu_k}\\right)^2 \\end{align} \\] To get a little more insight into this discriminant scoring function, suppose \\(K\\ =\\ 2\\) and \\(\\pi_1\\ =\\ \\pi_2\\), then we will assign an observation to class 1 if \\(2x(\\mu_1 - \\mu_2)\\ =\\ \\mu_1^{2}\\ - \\mu_2^{2}\\) and class 2 otherwise. Also the Bayes decision boundary will be set at \\[ \\begin{align} 2x\\left(\\mu_1\\ -\\ \\mu_2\\right)\\ &amp;=\\ \\mu_1^2\\ -\\ \\mu_2^2\\\\ x\\ &amp;=\\ \\frac{\\mu_1^2\\ -\\ \\mu_2^2}{2\\left(\\mu_1\\ -\\ \\mu_2\\right)}\\\\ x\\ &amp;=\\ \\frac{\\mu_1\\ +\\ \\mu_2}{2} \\end{align} \\] LDA With Miltiple Predictors For this case, we assume that \\(X = (X_1,X_2, \\dots, X_p)\\) are drawn from a multivariate Gaussian distribution. Thus the density function \\(f_k(x)\\) will take the form \\[ f_k\\left(x\\right)\\ =\\ \\frac{1}{\\left(2\\pi\\right)^{\\frac{p}{2}}\\left|\\Sigma\\right|^{\\frac{1}{2}}}\\exp\\left(-\\frac{1}{2}\\left(x-\\mu_k\\right)^T\\Sigma^{-1}\\left(x-\\mu_k\\right)\\right) \\] The main difference here than the one predictor model is the common covariance matrix \\(\\Sigma\\). Performing the same algebra as previous, now using matrices, the discriminant function \\(\\delta_k(x)\\) becomes \\[ \\delta_k\\left(x\\right)\\ =\\ x^T\\Sigma^{-1}\\mu_k\\ -\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k+\\log\\left(\\pi_k\\right) \\] 2.4 Practical Exercise This contrived example consists of normally distributed values for data separated into two distinct classes as shown by the plot below. Data lda_data &lt;- read.csv(&quot;data/lda-toy.csv&quot;,header=TRUE) head(lda_data) ## x y ## 1 4.667798 0 ## 2 5.509199 0 ## 3 4.702792 0 ## 4 5.956707 0 ## 5 5.738622 0 ## 6 5.027283 0 col_vec &lt;- ifelse(lda_data$y==0,&quot;orange&quot;,&quot;blue&quot;) plot(1:40,lda_data$x,xlab = &quot;predictor&quot;,ylab = &quot;value&quot;, col = col_vec) LDA Scoring We will score each \\(x\\) value and determine is score using the discriminat scoring function \\(\\delta_k(x)\\). Afterwards, we will predict a class for the scored value and compare it to the actual class value \\(y\\). num_class &lt;- 2 ldazero_index &lt;- which(lda_data$y==0) prob_zero &lt;- length(lda_data$y[ldazero_index])/length(lda_data$y) prob_zero ## [1] 0.5 prob_one &lt;- length(lda_data$y[-ldazero_index])/length(lda_data$y) prob_one ## [1] 0.5 mu_zero &lt;- sum(lda_data$x[ldazero_index])/length(lda_data$x[ldazero_index]) mu_zero ## [1] 4.975416 mu_one &lt;- sum(lda_data$x[-ldazero_index])/length(lda_data$x[-ldazero_index]) mu_one ## [1] 20.08706 squaredev_zero &lt;- sum((lda_data$x[ldazero_index]-mu_zero)^2) squaredev_one &lt;- sum((lda_data$x[-ldazero_index]-mu_one)^2) squaredev_zero ## [1] 10.15823 squaredev_one ## [1] 21.49317 lda_var &lt;- 1/(length(lda_data$x) - num_class) * sum(squaredev_one,squaredev_zero) lda_var ## [1] 0.8329315 Now we create the discriminant scoring function disc_score &lt;- function(x,mu,sigma,prob){ res &lt;- (x*(mu/sigma)) - (mu^2/(2*sigma)) + log(prob) res } disc_score(lda_data$x[1],mu_zero,lda_var,prob_zero) ## [1] 12.32936 disc_score(lda_data$x[1],mu_one,lda_var,prob_one) ## [1] -130.3349 Finally we will make predictions and compare them to our training set data score_zero &lt;- disc_score(lda_data$x,mu_zero,lda_var,prob_zero) score_one &lt;- disc_score(lda_data$x,mu_one,lda_var,prob_one) preds &lt;- numeric(length(lda_data$x)) for(i in seq_along(preds)){ if(score_zero[i] &gt; score_one[i]){ next } else{ preds[i] &lt;- 1 } } table(lda_data$y,preds) ## preds ## 0 1 ## 0 20 0 ## 1 0 20 An examination of the table shows that we achieved 100% accuracy. References "],
["non-lin-algs.html", "Chapter 3 Non-linear Algorithms 3.1 Classification and Regression Trees (CART) 3.2 Naive Bayes 3.3 k-Nearest Neigbors 3.4 Support Vector Machines", " Chapter 3 Non-linear Algorithms We now focus our attention on non-linear machine learning algorithms. As we learn about these algorithms, you should notice that many of these are an extension of the linear algorithms we learned in Chapter 2. 3.1 Classification and Regression Trees (CART) The first algorithm we will examine is the CART algorithm. This algorithm is crucial as it forms the basis for ensemble algorithms such as Random Forests and Bagged Decision Trees which we will learn in Chapter 4. CART models are also used for both regression and classification problems. What are CART models? CART models are simply decision trees. That is to say; the CART algorithm searches for points in the data to split the data into rectangular sections that increase the prediction accuracy. The more splits that are made within the data produces smaller and smaller segments up to a designated stopping point to prevent overfitting. A simple example will illustrate the intuition behind the CART algorithm. Figure 3.1 demonstrates a simple CART model. Reviewing this output, we can see the definition of the model being if Predictor A &gt;= 1.7 then if Predictor B &gt;= 202.1 the Outcome = 1.3 else Outcome = 5.6 else Outcome = 2.5 Figure 3.1: Example output and decision tree model adapted from Kuhn and Johnson (2013). Using the above decision algorithm, we can make future predictions based on the split values of Predictor A and B. How does a CART model learn from data? Regression Trees For regression trees, CART models search through all the data points for each predictor to determine the optimal split point that partitions the data into two groups and the sum of squared errors (SSE) is the lowest possible value for that split. In the previous example, that value was 1.7 for Predictor A. From that first split; the method is repeated within each new region until the model reaches a designated stopping point, for instance, \\(n &lt; 20\\) data points in any new region. \\[ SSE\\ =\\ \\sum_{i\\in S_1}^{ }\\left(y_i-\\overline{y_1}\\right)^2\\ +\\ \\sum_{i\\in S_2}^{ }\\left(y_i-\\overline{y_2}\\right)^2 \\] Classification Trees A frequently used measure for classification trees is the GINI index and is computed by \\[ G\\ =\\ \\sum_{k=1}^Kp_k\\times\\left(1-p_k\\right) \\] where \\(p_k\\) is the classification probability of the \\(k\\)th class. The optimal split point search process is similar to the regression method, except now the algorithm searches for the best split point based on the lowest Gini index indicating the purest node for that split. In this case, purity refers to a node having more of one particular class than another. Two-class Example To illustrate how to compute the Gini index, we will walk through a simple two-class example. The first step is to sort the sample based on the predictor values and then find the midpoint of the optimal split point. This split would create a contingency table like the one below. For this table, \\(n_{11}\\) is the proportion of sample observations that are in group 1(samples that are greater than the split value) class 1. The same logic follows for the other three split values. The bold-faced values are the sub-totals of the split groups and the classifications. Class1 Class2 \\(&gt;\\) split \\(n_{11}\\) \\(n_{12}\\) \\(\\mathbf{n_{&gt;split}}\\) \\(\\leq\\) split \\(n_{21}\\) \\(n_{22}\\) \\(\\mathbf{n_{\\leq split}}\\) \\(\\mathbf{n_{class1}}\\) \\(\\mathbf{n_{class2}}\\) \\(\\mathbf{n}\\) Before the split, the initial Gini index is \\[G = 2\\left(\\frac{n_{class1}}{n}\\right)\\left(\\frac{n_{class2}}{n}\\right)\\]. After the split, the Gini index changes to \\[ \\begin{align} G &amp;=\\ 2\\left[\\left(\\frac{n_{11}}{n_{&gt;split}}\\right)\\left(\\frac{n_{12}}{n_{&gt;split}}\\right)\\left(\\frac{n_{&gt;split}}{n}\\right)\\ +\\ \\left(\\frac{n_{21}}{n_{\\leq split}}\\right)\\left(\\frac{n_{22}}{n_{\\leq split}}\\right)\\left(\\frac{n_{\\leq split}}{n}\\right)\\right]\\\\ &amp;=\\ 2\\left[\\left(\\frac{n_{11}}{n}\\right)\\left(\\frac{n_{12}}{n_{&gt;split}}\\right)\\ +\\ \\left(\\frac{n_{21}}{n}\\right)\\left(\\frac{n_{22}}{n_{\\leq split}}\\right)\\right] \\end{align} \\] We can see from the above equation that the Gini index now depends upon the proportion of samples of each class within a region that is weighted by the proportion of sample points in each split group. We compare the new Gini index value to the previous value of the Gini index. If the new value is smaller than the previous value, the model makes the split the proposed split otherwise. Another frequently used method is the Information (Entropy) index and is calculated by \\[ I =\\ \\sum_{k=1}^{K}-p_klog_2\\left(p_k\\right) \\] Similar to the Gini index for K = 2 classes, the information before a split is \\[ I(\\text{before split}) = -\\left[\\frac{n_{class1}}{n}\\ \\times log_2\\left(\\frac{n_{class1}}{n}\\right)\\right]\\ - \\left[\\frac{n_{class2}}{n}\\ \\times log_2\\left(\\frac{n_{class2}}{n}\\right)\\right] \\] To determine how well a split improved the model, we will compute the information gain statistic. An increase in gain is an advantage, and a decrease in gain is a disadvantage. The calculation of gain is \\[ gain(\\text{split}) =\\ I(\\text{before split})\\ -\\ I(\\text{after split}) \\] To calculate the information index after the split, do the following \\[ \\begin{align} I(&gt;split) &amp;=\\ -\\left[\\frac{n_{11}}{n_{&gt;split}}\\ \\times\\ log_2\\left(\\frac{n_{11}}{n_{&gt;split}}\\right)\\right]\\ - \\left[\\frac{n_{12}}{n_{&gt;split}}\\ \\times\\ log_2\\left(\\frac{n_{12}}{n_{&gt;split}}\\right)\\right]\\\\ I(\\leq split) &amp;=\\ -\\left[\\frac{n_{21}}{n_{\\leq split}}\\ \\times\\ log_2\\left(\\frac{n_{21}}{n_{\\leq split}}\\right)\\right]\\ - \\left[\\frac{n_{22}}{n_{\\leq split}}\\ \\times\\ log_2\\left(\\frac{n_{22}}{n_{\\leq split}}\\right)\\right]\\\\ I(\\text{after split}) &amp;=\\ \\frac{n_{&gt;split}}{n}\\ I(&gt;split)\\ +\\ \\frac{n_{\\leq split}}{n}\\ I(\\leq split) \\end{align} \\] Gini Example We will now work through an example problem using the Gini index. Figure 3.2 shows the results of predicted classes with regions for a two-class model. There are a total of 208 observations: 111 observations for Class 1 and 97 observations for Class 2. Using this information, we can compute the Gini index before any splits. n_obs &lt;- 208 n_class_one &lt;- 111 n_class_two &lt;- 97 gini_before &lt;- 2 * (n_class_one/n_obs) * (n_class_two/n_obs) Based on the above calculation the pre-split Gini index is 0.498. Figure 3.2: Example classification model results. The contingency table for Predictor B of the above figure is below. Using this information, we can compute the post-split Gini index Class1 Class2 \\(B &gt; 0.197\\) 91 30 \\(B \\leq 0.197\\) 20 67 n11 &lt;- 91; n12 &lt;- 30; n21 &lt;- 20; n22 &lt;- 67; n_group_one &lt;- 121; n_group_two &lt;- 87; group_one_prop &lt;- (n11/n_obs)*(n12/n_group_one) group_two_prop &lt;- (n21/n_obs)*(n22/n_group_two) gini_after &lt;- 2 * sum(group_one_prop,group_two_prop) The final Gini index post-split is now 0.365 which indicates an improvement in classification purity. We can also observe that any value \\(\\leq 0.197\\) will receive a classification of 2 and a classification of 1 otherwise with regards to this particular split point. Pre-processing requirements? CART models do not require any special pre-processing of the data, but you can center and scale values based on skewness and other factors. 3.1.1 Practical Exerecise Libraries This exercise will use the AppliedPredictiveModeling, rpart, caret, and partykit packages. library(AppliedPredictiveModeling) library(rpart) library(caret) library(partykit) library(mlbench) library(kernlab) Regression Tree Data For this exercise, we will use the solubility dataset described in Kuhn and Johnson (2013). In short, the features of this dataset are: 208 binary “fingerprints” that indicate the presence or absence of a particular chemical sub-structure; 16 count descriptors (such as the number of bonds or the number of Bromine atoms); 4 continuous descriptors (such as molecular weight or surface area) (Kuhn and Johnson 2014). The authors centered and scaled the data to account for skewness. The target variable is a vector of log10 solubility values. The goal of this exercise is to predict the solubility value based on the set of features. Below is a view of some of the features and target values data(solubility) str(solTrainXtrans[,c(1:10,209:228)]) ## &#39;data.frame&#39;: 951 obs. of 30 variables: ## $ FP001 : int 0 0 1 0 0 1 0 1 1 1 ... ## $ FP002 : int 1 1 1 0 0 0 1 0 0 1 ... ## $ FP003 : int 0 0 1 1 1 1 0 1 1 1 ... ## $ FP004 : int 0 1 1 0 1 1 1 1 1 1 ... ## $ FP005 : int 1 1 1 0 1 0 1 0 0 1 ... ## $ FP006 : int 0 1 0 0 1 0 0 0 1 1 ... ## $ FP007 : int 0 1 0 1 0 0 0 1 1 1 ... ## $ FP008 : int 1 1 1 0 0 0 1 0 0 0 ... ## $ FP009 : int 0 0 0 0 1 1 1 0 1 0 ... ## $ FP010 : int 0 0 1 0 0 0 0 0 0 0 ... ## $ MolWeight : num 5.34 5.9 5.33 4.92 5.44 ... ## $ NumAtoms : num 3.37 3.91 3.53 3.3 3.47 ... ## $ NumNonHAtoms : num 2.83 3.3 2.77 2.4 2.77 ... ## $ NumBonds : num 3.43 3.97 3.53 3.3 3.47 ... ## $ NumNonHBonds : num 4.01 4.87 3.71 3.08 3.71 ... ## $ NumMultBonds : num 5.26 4.68 3.24 1.38 2.94 ... ## $ NumRotBonds : num 0 1.609 1.609 0.693 1.792 ... ## $ NumDblBonds : num 0 0 0.567 0.805 0 ... ## $ NumAromaticBonds : num 2.83 2.56 1.95 0 1.95 ... ## $ NumHydrogen : num 3.86 5.32 4.73 4.47 4.47 ... ## $ NumCarbon : num 4.18 5.09 4.02 3.51 3.32 ... ## $ NumNitrogen : num 0.585 0.642 0 0 0.694 ... ## $ NumOxygen : num 0 0.693 1.099 0 0 ... ## $ NumSulfer : num 0 0.375 0 0 0 0.375 0 0 0 0 ... ## $ NumChlorine : num 0 0 0 0 0.375 ... ## $ NumHalogen : num 0 0 0 0 0.375 ... ## $ NumRings : num 1.386 1.609 0.693 0.693 0.693 ... ## $ HydrophilicFactor: num -1.607 -0.441 -0.385 -2.373 -0.071 ... ## $ SurfaceArea1 : num 6.81 9.75 8.25 0 9.91 ... ## $ SurfaceArea2 : num 6.81 12.03 8.25 0 9.91 ... str(solTrainY) ## num [1:951] -3.97 -3.98 -3.99 -4 -4.06 -4.08 -4.08 -4.1 -4.1 -4.11 ... Create and Analyze Regression Tree The rpart() function in R is a widely used method for computing trees using the CART method, and we will use this function. Another package party uses the conditional inference framework to form its trees. set.seed(100) rpartTune &lt;- train(solTrainXtrans, solTrainY, method = &quot;rpart2&quot;, tuneLength = 10, trControl= trainControl(method = &quot;cv&quot;) ) rpartTune$results ## maxdepth RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 1 1.617667 0.3745252 1.2657915 0.11511437 0.05777279 0.08191460 ## 2 2 1.433114 0.5067404 1.1326186 0.07599686 0.04909341 0.04940391 ## 3 3 1.357672 0.5568291 1.0657348 0.07354389 0.05231774 0.06091190 ## 4 4 1.263596 0.6166997 0.9974476 0.10201869 0.05547696 0.07947602 ## 5 5 1.192831 0.6581800 0.9429124 0.11324197 0.05669830 0.08594278 ## 6 6 1.142654 0.6853056 0.9009065 0.10585813 0.05990671 0.08607556 ## 7 7 1.111858 0.7020728 0.8707216 0.10580483 0.06389863 0.08126706 ## 8 8 1.094535 0.7110088 0.8545809 0.11400541 0.06474333 0.09512021 ## 9 9 1.091880 0.7116190 0.8465921 0.11938842 0.06737339 0.10068304 ## 10 10 1.068799 0.7236716 0.8232469 0.12842861 0.07102897 0.10641491 #Build the initial model training_data &lt;- data.frame(cbind(solTrainXtrans,solTrainY)) training_model &lt;- rpart(solTrainY ~., data = training_data, control = rpart.control(maxdepth = 10)) #Examine the tree complexity plotcp(training_model) training_model$cptable ## CP nsplit rel error xerror xstd ## 1 0.37300506 0 1.0000000 1.0010223 0.05357024 ## 2 0.13770014 1 0.6269949 0.6314019 0.03143820 ## 3 0.06971510 2 0.4892948 0.4945930 0.02321245 ## 4 0.06180269 3 0.4195797 0.4434574 0.02133679 ## 5 0.04729111 4 0.3577770 0.3838988 0.01904376 ## 6 0.02650301 5 0.3104859 0.3514391 0.01837681 ## 7 0.01789274 6 0.2839829 0.3062709 0.01570413 ## 8 0.01553523 7 0.2660901 0.2989517 0.01566820 ## 9 0.01178134 8 0.2505549 0.2910022 0.01551438 ## 10 0.01150195 9 0.2387736 0.2879867 0.01543448 ## 11 0.01000000 10 0.2272716 0.2761810 0.01525549 #Add min(xerror+xstd) and find the smallest tree w/xerror &lt; min(xerror+xstd) which(training_model$cptable[,4] &lt; min(training_model$cptable[,4]+training_model$cptable[,5])) ## 9 10 11 ## 9 10 11 #Prune the tree training_model &lt;- rpart(solTrainY ~., data = training_data, cp = .014) model_tree &lt;- as.party Figure 3.3 displays the final results that we can use for interpretation of the model. To create the plot just use the code plot(model_tree). You could also use the prp function from the rpart.plot package. Using that package the prp plot would be prp(training_model,type=4,extra=106,box.col = c(&quot;#deebf7&quot;,&quot;#fff7bc&quot;)[training_model$frame$yval],cex = 0.6) Figure 3.3: Final CART model regression results. Classification Tree For this exercise, we will use the PimaIndianDiabetes2 data from the mlbench package based on the dataset from the UCI Machine Learning Repository. Click the link for a description of the dataset. In this exercise, we will build a classification tree that will classify a person as “pos” or “neg” for diabetes from a set of input features based on personal characteristics. Data I already created a training and test dataset from the original data. There are some missing values, so we will only use the complete cases for this example since the algorithms won’t work with missing data. pima_train &lt;- read.csv(&quot;data/pima-train.csv&quot;,header=TRUE) pima_train &lt;- pima_train[complete.cases(pima_train),] str(pima_train) ## &#39;data.frame&#39;: 311 obs. of 9 variables: ## $ pregnant: int 1 0 2 5 0 1 1 3 11 10 ... ## $ glucose : int 89 137 197 166 118 103 115 126 143 125 ... ## $ pressure: int 66 40 70 72 84 30 70 88 94 70 ... ## $ triceps : int 23 35 45 19 47 38 30 41 33 26 ... ## $ insulin : int 94 168 543 175 230 83 96 235 146 115 ... ## $ mass : num 28.1 43.1 30.5 25.8 45.8 43.3 34.6 39.3 36.6 31.1 ... ## $ pedigree: num 0.167 2.288 0.158 0.587 0.551 ... ## $ age : int 21 33 53 51 31 33 32 27 51 41 ... ## $ diabetes: Factor w/ 2 levels &quot;neg&quot;,&quot;pos&quot;: 1 2 2 2 2 1 2 1 2 2 ... Create and Analyze Classification Tree #Gini Index set.seed(33) train_ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 3) gini_tune &lt;- train(as.factor(diabetes) ~., data = pima_train, method = &quot;rpart&quot;, trControl=train_ctrl, tuneLength = 10, parms=list(split=&#39;gini&#39;)) gini_tune ## CART ## ## 311 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 280, 280, 280, 280, 280, 280, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.00000000 0.7790703 0.4994923 ## 0.03219107 0.7740995 0.4937550 ## 0.06438214 0.7699731 0.4670725 ## 0.09657321 0.7580735 0.4369132 ## 0.12876428 0.7591151 0.4401497 ## 0.16095535 0.7410013 0.4188441 ## 0.19314642 0.7420430 0.4219648 ## 0.22533749 0.7420430 0.4219648 ## 0.25752856 0.7420430 0.4219648 ## 0.28971963 0.7150448 0.3111903 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0. pima_gini_model &lt;- rpart(as.factor(diabetes) ~., data = pima_train, cp = .004) pima_gini_tree &lt;- as.party(pima_gini_model) #Information Index set.seed(33) info_tune &lt;- train(as.factor(diabetes) ~., data = pima_train, method = &quot;rpart&quot;, trControl=train_ctrl, tuneLength = 10, parms=list(split=&#39;information&#39;)) info_tune ## CART ## ## 311 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 280, 280, 280, 280, 280, 280, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.00000000 0.7768907 0.5019724 ## 0.03219107 0.7720542 0.4896788 ## 0.06438214 0.7731340 0.4906999 ## 0.09657321 0.7430847 0.4352926 ## 0.12876428 0.7441263 0.4385291 ## 0.16095535 0.7366644 0.4268724 ## 0.19314642 0.7377061 0.4299930 ## 0.22533749 0.7377061 0.4299930 ## 0.25752856 0.7377061 0.4299930 ## 0.28971963 0.6912097 0.2398796 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0. pima_info_model &lt;- rpart(as.factor(diabetes)~., data = pima_train, cp = .004) pima_info_tree &lt;- as.party(pima_info_model) Figure 3.4: Gini Index CART model classification results. Figure 3.5: Information Index CART model classification results. 3.2 Naive Bayes Recall Baye’s Theorem from Chapter 2 \\[ \\Pr\\left(Y\\ =\\ k\\ |X\\right)\\ =\\ \\frac{P\\left(X|Y\\ =\\ k\\right)P\\left(Y\\right)}{P\\left(X\\right)} \\] where we want to answer the question “what is the probability of a particular target classification given the observed features?” Upon calculating the posterior probability for each classification, you can then select the classification with the highest probability. In the literature, this calculation is the maximum a posteriori (MAP), and we find it by \\[ \\begin{align} MAP(Y) &amp;=\\ max\\left(P(Y \\vert X\\right)\\\\ &amp;=\\ max\\left(\\frac{P\\left(X|Y\\ =\\ k\\right)P\\left(Y\\right)}{P\\left(X\\right)}\\right)\\\\ &amp;=\\ max\\left(P\\left(X|Y\\ =\\ k\\right)P\\left(Y\\right)\\right) \\end{align} \\] We can ignore the denominator of the original equation because the \\(P(X)\\) is a constant for terms. Also, the reason why this method is called Naive Bayes is that the features are assumed to be independent. To put it another way, instead of computing \\(P(x_1,x_2,\\dots,x_p\\ \\vert Y)\\), the independence assumption simplifies this calculation to \\[ P\\left(X\\vert Y\\ =\\ k\\right) = \\prod_{j=1}^{P}P\\left(X \\vert Y\\ = k\\right) \\] Another aspect of the Naive Bayes method is the distribution of the features. A Gaussian distribution will be used for continuous features, and kern density estimates for discrete features. 3.2.1 Practical Exerecise We will use the Pima data for this exercise. Naive Bayes Model We will use the naiveBayes function from the klaR package along with the caret package. set.seed(33) library(klaR) nb_tune &lt;- train(as.factor(diabetes) ~ ., data=pima_train, method = &quot;nb&quot;, trControl = trainControl(method=&quot;none&quot;), tuneGrid = data.frame(fL=0, usekernel=FALSE, adjust=1)) nb_preds &lt;- predict(nb_tune,pima_train,type = &quot;raw&quot;) confusionMatrix(nb_preds,as.factor(pima_train$diabetes)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 169 35 ## pos 35 72 ## ## Accuracy : 0.7749 ## 95% CI : (0.7244, 0.8201) ## No Information Rate : 0.6559 ## P-Value [Acc &gt; NIR] : 3.352e-06 ## ## Kappa : 0.5013 ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.8284 ## Specificity : 0.6729 ## Pos Pred Value : 0.8284 ## Neg Pred Value : 0.6729 ## Prevalence : 0.6559 ## Detection Rate : 0.5434 ## Detection Prevalence : 0.6559 ## Balanced Accuracy : 0.7507 ## ## &#39;Positive&#39; Class : neg ## 3.3 k-Nearest Neigbors The basic idea of the \\(k\\)-nearest neighbors (KNN) algorithm is to create a distance matrix of the all the feature variables and choose the \\(k\\) most adjacent data points closest to an evaluated point. Since KNN uses the entire dataset, no learning is necessary from the algorithm. The primary choice of the modeler is what decision metric to use. The primary parameter used is a variation of the Minkowski distance metric. You can compute this metric by \\[ \\left(\\sum_{i=1}^{P}\\vert x_{ai} - x_{bi} \\vert^q\\right)^\\frac{1}{q} \\] where \\(\\mathbf{x_a}\\) and \\(\\mathbf{x_b}\\) are two sample points in the dataset. When \\(q\\ =\\ 1\\) this distance metric is the Manhattan (city-block) distance, and when \\(q\\ =\\ 2\\) this distance is the Euclidean distance. You will use Euclidean distance for continuous predictors and Manhattan distance for categorical or binary predictors. Curse of Dimensionality Just like other machine learning methods the KNN method has its disadvantages. One disadvantage deals with high dimensional data. In essence, distances in higher dimensions are larger which ultimately mean that similar points are not necessarily local to each other. Figure 3.6 demonstrates this problem. The figure on the left shows a unit hypercube with a sub-cube that captures a fraction of the data of the original hypercube. The sub-figure on the right shows how much of the range of each coordinate you need to capture within the sub-cube. For instance, if you want to capture 10% of the data, you will need to capture 80% of the range of coordinates for a 10-dimension dataset. This percentage increases exponentially with additional dimensions. Figure 3.6: Illustration of dimensionality curse adapted from Hastie, Tibshirani, and Friedman (2009. 3.3.1 Practical Exerecise Regression Data We will use the solubility data for this exercise. knn_data &lt;- solTrainXtrans[,-nearZeroVar(solTrainXtrans)] Create the model set.seed(100) knn_reg_model &lt;- train(knn_data, solTrainY, method = &quot;knn&quot;, #Center and scaling will occur for new predictors preProc = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = data.frame(.k = 1:20), trControl = trainControl(method = &quot;cv&quot;) ) knn_reg_model$finalModel ## 4-nearest neighbor regression model The final model selected was a model based on a value of \\(k\\ =\\ 4\\). Figure 3.7 shows graphically why this model was the best of the 20 analyzed. Figure 3.7: Plots of RMSE and RSquared for values of k. Classification We will again use the Pima data. set.seed(202) pima_knn &lt;- train(as.factor(diabetes)~., data = pima_train, method = &quot;knn&quot;, metric = &quot;ROC&quot;, preProc = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = data.frame(.k=1:50), trControl = trainControl(method = &quot;cv&quot;, classProbs = TRUE, summaryFunction = twoClassSummary)) pima_knn ## k-Nearest Neighbors ## ## 311 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## Pre-processing: centered (8), scaled (8) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 280, 281, 280, 279, 280, 281, ... ## Resampling results across tuning parameters: ## ## k ROC Sens Spec ## 1 0.6758874 0.7990476 0.5527273 ## 2 0.7472781 0.7840476 0.5318182 ## 3 0.7854405 0.8530952 0.5327273 ## 4 0.7962013 0.8435714 0.5709091 ## 5 0.7963950 0.8585714 0.5700000 ## 6 0.7987013 0.8733333 0.5800000 ## 7 0.8115682 0.8785714 0.5681818 ## 8 0.8184437 0.8978571 0.5218182 ## 9 0.8219935 0.8926190 0.5045455 ## 10 0.8193268 0.8926190 0.5127273 ## 11 0.8269091 0.8780952 0.5318182 ## 12 0.8344470 0.8876190 0.4945455 ## 13 0.8369232 0.8973810 0.4936364 ## 14 0.8384621 0.8971429 0.4745455 ## 15 0.8404080 0.8976190 0.4936364 ## 16 0.8378268 0.8973810 0.5036364 ## 17 0.8421396 0.9121429 0.4663636 ## 18 0.8384405 0.9119048 0.4745455 ## 19 0.8382392 0.9019048 0.4745455 ## 20 0.8325335 0.9019048 0.4936364 ## 21 0.8388268 0.9119048 0.5018182 ## 22 0.8414740 0.9119048 0.4845455 ## 23 0.8419794 0.9166667 0.4836364 ## 24 0.8454058 0.9169048 0.4836364 ## 25 0.8452814 0.9169048 0.4636364 ## 26 0.8434935 0.9169048 0.4563636 ## 27 0.8491331 0.9216667 0.4554545 ## 28 0.8474957 0.9314286 0.4645455 ## 29 0.8446385 0.9216667 0.4363636 ## 30 0.8437154 0.9266667 0.4363636 ## 31 0.8392987 0.9266667 0.4263636 ## 32 0.8399816 0.9216667 0.4272727 ## 33 0.8390368 0.9266667 0.4272727 ## 34 0.8381483 0.9266667 0.4190909 ## 35 0.8404892 0.9316667 0.4281818 ## 36 0.8392532 0.9316667 0.4381818 ## 37 0.8412446 0.9269048 0.4281818 ## 38 0.8404448 0.9319048 0.4281818 ## 39 0.8436937 0.9466667 0.4281818 ## 40 0.8449589 0.9416667 0.4281818 ## 41 0.8423431 0.9366667 0.4381818 ## 42 0.8405693 0.9366667 0.4372727 ## 43 0.8415855 0.9464286 0.4372727 ## 44 0.8449123 0.9561905 0.4463636 ## 45 0.8451190 0.9511905 0.4281818 ## 46 0.8455054 0.9416667 0.4372727 ## 47 0.8452846 0.9464286 0.4281818 ## 48 0.8459361 0.9464286 0.4372727 ## 49 0.8466537 0.9414286 0.4181818 ## 50 0.8445725 0.9514286 0.4090909 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was k = 27. pima_knn$finalModel ## 27-nearest neighbor model ## Training set outcome distribution: ## ## neg pos ## 204 107 The final model was computed from a total of \\(k\\ =\\ 27\\) neighbors and a comparison of the ROC metric for each neighbor is shown in Figure 3.8. Figure 3.8: Receiver-Operator Characteristic (ROC) curve results. Exercise Create a new KNN model using only three predictor variables for the Pima data and compare those results to the current KNN model. Use the plot from the regression tree as a guide to determine which variables to choose. 3.4 Support Vector Machines Support vector machines (SVM) started as a method for classification but has extensions for regression as well. The goal of using SVM is to compute a regression line or decision boundary. Regression For regression, the SVM seeks to minimize the cost function. \\[ Cost\\sum_{i=1}^nL_\\epsilon \\left(y_i - \\hat{y_i}\\right)\\ +\\ \\sum_{j=1}^{P}\\beta_{j}^2 \\] where \\(Cost\\) is the residual cost penalty and \\(L_\\epsilon (\\cdot)\\) is the margin threshold. Furthermore, we transform the prediction equation for \\(\\hat{y}\\) to \\[ \\begin{align} \\hat{y} &amp;=\\ \\beta_0\\ +\\ \\beta_1u_1\\ +\\ \\dots\\ +\\ \\beta_pu_p\\\\ &amp;=\\ \\beta_0\\ +\\ \\sum_{j=1}^{P}\\beta_ju_j\\\\ &amp;=\\ \\beta_0\\ +\\ \\sum_{j=1}^{P}\\sum_{i=1}^{n}\\alpha_ix_{ij}u_j\\\\ &amp;=\\ \\beta_0\\ +\\ \\sum_{i=1}^{n}\\alpha_i\\left(\\sum_{j=1}^{P}x_{ij}u_j\\right) \\end{align} \\] where \\(\\alpha_i\\) is an unknown parameter estimated by the SVM algorithm. We can generalize the above equation for \\(\\hat{y}\\) even further to matrix form with \\[ f(u) = \\beta_0\\ +\\ \\sum_{i=1}^{n}\\alpha_iK\\left(x_i,\\mathbf{u}\\right) \\] where \\(K\\left(\\cdot\\right)\\) is a kernel function. For SVM, there are a few models to choose for the kernel function to encompass linear and nonlinear functions of the predictors. The most common kernel functions are \\[ \\begin{align} \\text{linear} &amp;=\\ x_i^{T}\\mathbf{u}\\\\ \\text{polynomial} &amp;=\\ \\left(\\phi(x^T\\mathbf{u})\\ +\\ 1\\right)^{degree}\\\\ \\text{radial basis function} &amp;=\\ e^{\\left(-\\sigma\\vert\\vert x\\ -\\ \\mathbf{u} \\vert\\vert^2\\right)}\\\\ \\text{hyperbolic tangent} &amp;=\\ tanh\\left(\\phi(x^T\\mathbf{u})\\ +\\ 1\\right) \\end{align} \\] To demonstrate why you would use SVM for some regression situations, examine Figure 3.9 below. The top of the figure shows a comparison of a simple linear regression model using least squares and SVM. The item to notice in this plot is the outlier at the top left of the plot. You will see that the least squares model is more sensitive to this value, while SVM is not. The middle plot shows the relationship of the residuals to the predictive values. What is important to note with this plot is that the grey points are values that contributed to the regression line estimation, which the values shown by the red crosses did not contribute. This lack of contribution is because these non-contributing values are within the \\(\\pm\\ \\epsilon\\ =\\ 0.01\\) threshold set by the modeler. We explain why this happens in a little bit. The bottom plot shows a nonlinear application of SVM to a sine wave model. Again, there are outlier values in the bottom left of the plot, and the least squares method is more sensitive to these values than SVM. The reason for this is because least squares and other regression models like logistic regression are more global in their behavior, while SVM uses more local contributions from the data for its estimations. Figure 3.9: Example SVM regressions compared to least squares adapted from Kuhn and Johnson (2013) As we stated previously, the samples that contribute to the calculation of the regression curve, also called support vectors, are the samples outside of the \\(\\epsilon\\) threshold. At first, this finding may seem counterintuitive, so examining Figure 3.10 could help with building an intuition for this. Specifically looking at the bottom right model, we can see that the residuals that are within the \\(\\epsilon\\) threshold are in fact zero and all other residuals contribute linearly to the regression equation. Figure 3.10: Plots of various contributions to the regression line of model residuals adapted from Kuhn and Johnson (2013) Classification For the classification version of SVM, we want to compute an optimal decision boundary between classes that are separable and even classes that are not separable. Figure 3.11 shows how SVM attempts to solve this problem. On the left, you will notice two separable classes with infinite classification boundaries. On the right, is how SVM solves this problem. Explicitly, a boundary called the maximum margin classifier is computed. What’s unique about this boundary is that the boundary itself(solid black line) has margins that are set to the closets points for each class. The result of this is that now, unlike the regression model, samples that are closest to the boundary contribute to the classification model and those that are furthest away from the boundary do not contribute at all. The equations for the classification version of SVM are similar to the regression equations. The general decision boundary equation is \\[ D(u) = \\beta_0\\ +\\ \\sum_{i=1}^{n}y_i\\alpha_iK(x_i,\\mathbf{u}) \\] The critical thing to notice is how the classification equation now includes the actual class of \\(y_i\\) which is usually a value of -1 or 1. The kernel functions are \\[ \\begin{align} \\text{linear} &amp;=\\ x_i^{T}\\mathbf{u}\\\\ \\text{polynomial} &amp;=\\ \\left(scale(x^T\\mathbf{u})\\ +\\ 1\\right)^{degree}\\\\ \\text{radial basis function} &amp;=\\ e^{\\left(-\\sigma\\vert\\vert x\\ -\\ \\mathbf{u} \\vert\\vert^2\\right)}\\\\ \\text{hyperbolic tangent} &amp;=\\ tanh\\left(scale(x^T\\mathbf{u})\\ +\\ 1\\right) \\end{align} \\] Figure 3.11: Datasets with separable classes adapted from Kuhn and Johnson (2013) Optimizaton Forumulation Ultimately, both the regression and classification equations are reformulated into a quadratic programming problem. While it is beyond the scope of this lesson to detail this formulation, the reader should review Hastie, Tibshirani, and Friedman (2009), specifically the section discussing SVMs. What’s key to understanding this formulation is that the tuning of the SVM depends primarily on the cost parameter when estimating the other parameters. 3.4.1 Practical Exerecise Regression Data We will use the solubility data for this portion. Model set.seed(33) solubility_svm &lt;- train(solTrainXtrans, solTrainY, method = &quot;svmRadial&quot;, preProc = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 14, trControl = trainControl(method = &quot;cv&quot;)) solubility_svm ## Support Vector Machines with Radial Basis Function Kernel ## ## 951 samples ## 228 predictors ## ## Pre-processing: centered (228), scaled (228) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 855, 856, 855, 855, 858, 857, ... ## Resampling results across tuning parameters: ## ## C RMSE Rsquared MAE ## 0.25 0.8014848 0.8681906 0.6039660 ## 0.50 0.7102286 0.8890663 0.5327464 ## 1.00 0.6613592 0.9000814 0.4934836 ## 2.00 0.6314286 0.9066489 0.4684522 ## 4.00 0.6186151 0.9091987 0.4541152 ## 8.00 0.6064508 0.9127569 0.4457087 ## 16.00 0.6018292 0.9140239 0.4427055 ## 32.00 0.6018878 0.9138726 0.4429424 ## 64.00 0.6007705 0.9141895 0.4423807 ## 128.00 0.5998743 0.9145279 0.4443426 ## 256.00 0.6025174 0.9136982 0.4463239 ## 512.00 0.6039626 0.9132093 0.4469188 ## 1024.00 0.6042640 0.9131080 0.4474508 ## 2048.00 0.6062670 0.9125208 0.4497784 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.002740343 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were sigma = 0.002740343 and C = 128. solubility_svm$finalModel ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: eps-svr (regression) ## parameter : epsilon = 0.1 cost C = 128 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 0.0027403433192481 ## ## Number of Support Vectors : 638 ## ## Objective Function Value : -726.094 ## Training error : 0.009299 Classification Data We will us the Pima dataset Model set.seed(202) sigmaRange &lt;- sigest(as.factor(diabetes) ~.,data=pima_train) svmGrid &lt;- expand.grid(.sigma = sigmaRange[1], .C = 2^(seq(-4,4))) set.seed(386) pima_svm &lt;- train(as.factor(diabetes)~., data = pima_train, method = &quot;svmRadial&quot;, metric = &quot;ROC&quot;, preProc = c(&quot;center&quot;,&quot;scale&quot;), tuneGrid = svmGrid, fit = FALSE, trControl = trainControl(method = &quot;cv&quot;, classProbs = TRUE, summaryFunction = twoClassSummary)) pima_svm ## Support Vector Machines with Radial Basis Function Kernel ## ## 311 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## Pre-processing: centered (8), scaled (8) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 280, 279, 279, 279, 280, 280, ... ## Resampling results across tuning parameters: ## ## C ROC Sens Spec ## 0.0625 0.8434134 0.7795238 0.6945455 ## 0.1250 0.8434134 0.7842857 0.6945455 ## 0.2500 0.8503506 0.8426190 0.6090909 ## 0.5000 0.8476623 0.8573810 0.5909091 ## 1.0000 0.8478203 0.8678571 0.5727273 ## 2.0000 0.8427922 0.8528571 0.5736364 ## 4.0000 0.8400390 0.8626190 0.5154545 ## 8.0000 0.8388355 0.8828571 0.4872727 ## 16.0000 0.8367100 0.9066667 0.4972727 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.03176525 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 0.03176525 and C = 0.25. pima_svm$finalModel ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 0.25 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 0.0317652466552473 ## ## Number of Support Vectors : 197 ## ## Objective Function Value : -43.388 ## Probability model included. References "],
["ens-algs.html", "Chapter 4 Ensemble Algorithms 4.1 Bagging 4.2 Random Forest 4.3 AdaBoost 4.4 Gradient Boosting", " Chapter 4 Ensemble Algorithms 4.1 Bagging Bagging is short for Bootstrapped Aggregation. As you can guess from the name, the Bagging algorithm’s basis is the bootstrap. With bootstrapping, you can use resampling techniques to estimate an unknown parameter of the data. The bootstrap method computes this estimate by taking random samples, with replacement, of the data and calculating the estimated value a total of \\(B\\) times. The final step is to calculate the average of the estimates over the \\(B\\) bootstrap trials. With Bagging, the algorithm aggregates predictions from multiple machine learning models. Bagging provides an advantage for models that are high variance by reducing the variance of these models. In essence, Bagging mimics the phenomenon known as the “wisdom of the crowd”. In practice the algorithm is relatively simple Figure 4.1: Bagging algorithm from Kuhn and Johnson (2013). Since Bagging works well on models with high variance, it is most widely used on CART models. Fundamentally, the Bagging algorithm cast votes on multiple trees that are individually weak learners. However, the aggregated response or classification has an overall reduced error rate without a loss in bias. Practical Exercise For this and the remaining PEs in this chapter, we will use the same solubility and Pima datasets from Chapter 3. library(AppliedPredictiveModeling) library(rpart) library(caret) library(partykit) library(mlbench) library(kernlab) library(ipred) library(randomForest) library(gbm) data(solubility) pima_train &lt;- read.csv(&quot;data/pima-train.csv&quot;,header=TRUE) pima_train &lt;- pima_train[complete.cases(pima_train),] Regression set.seed(100) train_control &lt;- trainControl(method=&#39;cv&#39;, number=5, returnResamp=&#39;none&#39;) bag_regress &lt;- train(solTrainXtrans, solTrainY, method = &quot;treebag&quot;, trControl= train_control ) bag_regress$results ## parameter RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 none 0.9208562 0.8011265 0.7022581 0.0404631 0.03403587 0.03130357 Classification set.seed(100) bag_class &lt;- train(as.factor(diabetes) ~., data = pima_train, method = &quot;treebag&quot;, trControl= train_control ) bag_class$results ## parameter Accuracy Kappa AccuracySD KappaSD ## 1 none 0.7715822 0.4847493 0.04241899 0.08441819 4.2 Random Forest While we can significantly reduce model variance with Bagging, it has some drawbacks. In particular, Bagging creates trees on the entire feature space for each sample. Thus, most trees, especially at the top layers will look very similar, and as a result, most of the trees are not independent of each other. The Random Forest algorithm fixes this problem. Reviewing the algorithm in Figure 4.2, you will notice that instead of building a tree on the entire feature space, Random Forest trees are constructed using a random sample of \\(k &lt; P\\) of the original predictors. For classification, a general default for the number of predictors at each split point is \\(k\\ =\\ \\sqrt{P}\\). For regression, the default number of predictors at each split point is \\(k\\ =\\ \\frac{P}{3}\\). A side benefit of this algorithm is that Random Forest is more computationally efficient since trees are not built on the entire set of features. Figure 4.2: Random Forest algorithm from Kuhn and Johnson (2013). Practical Exercise Regresssion set.seed(41) rf_regress_model &lt;- randomForest(solTrainXtrans,solTrainY, importance=TRUE, ntrees=500) rf_regress_model ## ## Call: ## randomForest(x = solTrainXtrans, y = solTrainY, importance = TRUE, ntrees = 500) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 76 ## ## Mean of squared residuals: 0.4222374 ## % Var explained: 89.91 head(rf_regress_model$importance) ## %IncMSE IncNodePurity ## FP001 0.006445326 1.786854 ## FP002 0.010880283 2.427839 ## FP003 0.007691509 2.469004 ## FP004 0.025182610 17.044367 ## FP005 0.004678159 1.405516 ## FP006 0.009439474 4.459932 #We won&#39;t run this, but this is how you would train this model using CARET # mtry_min &lt;- floor(ncol(solTrainXtrans)/3) # mtry_max &lt;- ncol(solTrainXtrans) # mtry &lt;- seq(mtry_min,mtry_max) # train_control &lt;- trainControl(method=&#39;cv&#39;, number=5, search=&#39;grid&#39;) # model_metric &lt;- &quot;RMSE&quot; # tune_grid &lt;- expand.grid(.mtry=mtry) # # rf_random_regress &lt;- train(solTrainXtrans, solTrainY, # method = &quot;rf&quot;, # trControl= train_control, # metric = model_metric, # tuneGrid = tune_grid # ) Classification #Random search set.seed(41) mtry_min &lt;- floor(sqrt(ncol(pima_train)-1)) mtry_max &lt;- ncol(pima_train)-1 mtry &lt;- mtry_min train_control &lt;- trainControl(method=&#39;cv&#39;, number=5, search=&#39;random&#39;) model_metric &lt;- &quot;Accuracy&quot; tune_grid &lt;- expand.grid(.mtry=mtry) rf_random_class &lt;- train(as.factor(diabetes) ~., data = pima_train, method = &quot;rf&quot;, trControl= train_control, metric = model_metric ) rf_random_class$results ## mtry Accuracy Kappa AccuracySD KappaSD ## 1 2 0.7908606 0.5230442 0.02152436 0.04163186 ## 2 5 0.7909118 0.5159105 0.03077111 0.07472786 ## 3 8 0.7814921 0.4994510 0.03575589 0.08559938 Exercise Retrin the Pima random forest model using a grid search for the mtry parameter. 4.3 AdaBoost Similar to Bagging models, Boosting models create multiple learners; in particular, decision trees. Unlike Bagging models, however, Boosting models build an initial model, and then makes incremental improvements to the models at subsequent iterations. One of the most popular Boosting algorithms is the Adaboost algorithm. Figure 4.3 shows the steps for the Adaboost algorithm. The first model starts by weighing each observation uniformly. Next, a classifier model is generated followed by a weighted error. From this error, a scaling parameter \\(\\alpha_m\\) is computed. After the computation of \\(\\alpha_m\\), the weights are updated. Figure 4.3: Adaboost classification algorithm from Hastie, Tibshirani, and Friedman (2009). Before computing the weights, let’s take a deeper dive into the \\(\\alpha_m\\) parameter. Figure 4.4 shows how \\(\\alpha\\) changes based on possible weighted error rates. In essence, observations that have a classification that agrees with the actual value receive a higher positive \\(\\alpha_m\\) value; observations that have a \\(50/50\\) chance of being correct, receive a \\(\\alpha_m\\) value of zero; finally, observations that are misclassified have high negative values. Figure 4.4: Adaboost error rate. To compute the updated weights, the previous weight is multiplied by \\(e^{-\\alpha_m y_i G_m(x_i)}\\). Essentially, this exponential parameter will be larger for misclassified observations, and smaller for correctly classified observations. In effect, this will increase the weights for misclassified observations and reduce the weights for correctly classified observations at the next iteration. Lastly, the algorithm returns the sign of the final weighted sums. Practical Exercise set.seed(100) gbm_grid &lt;- expand.grid(n.trees=c(100, 500), shrinkage=c(0.01, 0.001), interaction.depth=c(1,5), n.minobsinnode=10) gbm_control &lt;- trainControl(method=&#39;cv&#39;, number=5, classProbs = TRUE, summaryFunction = twoClassSummary ) gbm_metric &lt;- &quot;ROC&quot; ada_class &lt;- train(as.factor(diabetes) ~., data = pima_train, method = &quot;gbm&quot;, distribution=&quot;adaboost&quot;, verbose=FALSE, trControl= gbm_control, metric = gbm_metric, tuneGrid = gbm_grid ) ada_class$results ## shrinkage interaction.depth n.minobsinnode n.trees ROC Sens ## 1 0.001 1 10 100 0.8149485 1.0000000 ## 5 0.010 1 10 100 0.8367787 0.9706098 ## 3 0.001 5 10 100 0.8511670 1.0000000 ## 7 0.010 5 10 100 0.8529044 0.9313415 ## 2 0.001 1 10 500 0.8273662 1.0000000 ## 6 0.010 1 10 500 0.8465590 0.8819512 ## 4 0.001 5 10 500 0.8510762 0.9802439 ## 8 0.010 5 10 500 0.8445998 0.8576829 ## Spec ROCSD SensSD SpecSD ## 1 0.0000000 0.08295399 0.00000000 0.0000000 ## 5 0.3545455 0.06764300 0.01084269 0.1338181 ## 3 0.0000000 0.07544806 0.00000000 0.0000000 ## 7 0.5419913 0.08249568 0.02053167 0.1284063 ## 2 0.0000000 0.07122291 0.00000000 0.0000000 ## 6 0.5792208 0.07931748 0.05469074 0.1377872 ## 4 0.2415584 0.07989993 0.02084617 0.1164474 ## 8 0.6173160 0.07781126 0.05620946 0.1037314 4.4 Gradient Boosting Using a similar stagewise approach, Gradient Boosting (GBM) builds an ensemble of weak models, but improves the models by optimizing an arbitrary loss function. The loss functions are usally squared loss \\(\\left(\\frac{1}{2}\\left[y_i\\ -\\ f(x_i)\\right]^2\\right)\\), absolute loss \\(\\left(\\vert y_i\\ -\\ f(x_i) \\vert\\right)\\), and the Huber Loss function for regression. Classification models use the deviance loss. Figure 4.5 shows the GBM algorithm for regression and Figure 4.6 shows the GBM algorithm for classifiction. Both algorithms work in a similar manner. First, the model initiates an optimal single terminal node tree (also known as a stump). Next, the model computes a negative gradient, fits a new regression tree, and updates the predicted values of each sample with a learning parameter \\(\\gamma\\). Figure 4.5: Gradient Boosting regression algorithm from Hastie, Tibshirani, and Friedman (2009). Figure 4.6: Gradient Boosting classification algorithm from Hastie, Tibshirani, and Friedman (2009). Practical Exercise set.seed(100) gbm_class &lt;- train(as.factor(diabetes) ~., data = pima_train, method = &quot;gbm&quot;, distribution=&quot;bernoulli&quot;, verbose=FALSE, trControl= gbm_control, metric = gbm_metric, tuneGrid = gbm_grid ) gbm_class$results ## shrinkage interaction.depth n.minobsinnode n.trees ROC Sens ## 1 0.001 1 10 100 0.8122234 1.0000000 ## 5 0.010 1 10 100 0.8359652 0.9559756 ## 3 0.001 5 10 100 0.8504862 1.0000000 ## 7 0.010 5 10 100 0.8506037 0.9262195 ## 2 0.001 1 10 500 0.8278197 0.9901220 ## 6 0.010 1 10 500 0.8481866 0.8818293 ## 4 0.001 5 10 500 0.8508756 0.9753659 ## 8 0.010 5 10 500 0.8439658 0.8626829 ## Spec ROCSD SensSD SpecSD ## 1 0.0000000 0.08469365 0.00000000 0.00000000 ## 5 0.3831169 0.06891182 0.02026189 0.08287528 ## 3 0.0000000 0.07402308 0.00000000 0.00000000 ## 7 0.5515152 0.08046038 0.03112146 0.14110330 ## 2 0.1861472 0.07441610 0.01352779 0.09213765 ## 6 0.5796537 0.07909138 0.08513077 0.16192154 ## 4 0.3536797 0.08044493 0.01768082 0.09446756 ## 8 0.6545455 0.07375167 0.05362804 0.11110455 Exercise Adjust the tuning grid and determine if we can improve the boosting models. "],
["ml-pe.html", "Chapter 5 Machine Learning Practical Exercise", " Chapter 5 Machine Learning Practical Exercise "],
["shiny-tut.html", "Chapter 6 R Shiny Tutorial", " Chapter 6 R Shiny Tutorial "],
["references.html", "References", " References "]
]
