[
["ens-algs.html", "Chapter 4 Ensemble Algorithms 4.1 Bagging 4.2 Random Forest 4.3 AdaBoost 4.4 Gradient Boosting", " Chapter 4 Ensemble Algorithms 4.1 Bagging Bagging is short for Bootstrapped Aggregation. As you can guess from the name, the Bagging algorithm’s basis is the bootstrap. With bootstrapping, you can use resampling techniques to estimate an unknown parameter of the data. The bootstrap method computes this estimate by taking random samples, with replacement, of the data and calculating the estimated value a total of \\(B\\) times. The final step is to calculate the average of the estimates over the \\(B\\) bootstrap trials. With Bagging, the algorithm aggregates predictions from multiple machine learning models. Bagging provides an advantage for models that are high variance by reducing the variance of these models. In essence, Bagging mimics the phenomenon known as the “wisdom of the crowd”. In practice the algorithm is relatively simple Figure 4.1: Bagging algorithm from Kuhn and Johnson (2013). Since Bagging works well on models with high variance, it is most widely used on CART models. Fundamentally, the Bagging algorithm cast votes on multiple trees that are individually weak learners. However, the aggregated response or classification has an overall reduced error rate without a loss in bias. Practical Exercise For this and the remaining PEs in this chapter, we will use the same solubility and Pima datasets from Chapter 3. library(AppliedPredictiveModeling) library(rpart) library(caret) library(partykit) library(mlbench) library(kernlab) library(ipred) library(randomForest) data(solubility) pima_train &lt;- read.csv(&quot;data/pima-train.csv&quot;,header=TRUE) pima_train &lt;- pima_train[complete.cases(pima_train),] Regression set.seed(100) train_control &lt;- trainControl(method=&#39;cv&#39;, number=5, returnResamp=&#39;none&#39;) bag_regress &lt;- train(solTrainXtrans, solTrainY, method = &quot;treebag&quot;, trControl= train_control ) bag_regress$results ## parameter RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 none 0.9208562 0.8011265 0.7022581 0.0404631 0.03403587 0.03130357 Classification set.seed(100) bag_class &lt;- train(as.factor(diabetes) ~., data = pima_train, method = &quot;treebag&quot;, trControl= train_control ) bag_class$results ## parameter Accuracy Kappa AccuracySD KappaSD ## 1 none 0.7715822 0.4847493 0.04241899 0.08441819 4.2 Random Forest While Bagging significantly reduced the variance compared to some other machine learning models, it has some drawbacks. In particular, Bagging creates trees on the entire feature space for each sample. Thus, most trees, especially at the top layers will look very similar and as a result most of the trees are not independent from each other. The Random Forest algorithm fixes this problem. Reviewing the algorithm in Figure 4.2, you will notice that instead of building a tree on the entire feature space, Random Forest trees are built using a random sample of \\(k &lt; P\\) of the original predictors. For classification, a general default for the number of predictors at each split point is \\(k\\ =\\ \\sqrt{P}\\). For regression, the default number of predictors at each split point is \\(k\\ =\\ \\frac{P}{3}\\). A side benefit of this algorithm is that Random Forest is more computatinally efficient since trees are not built on the entire set of features. Figure 4.2: Random Forest algorithm from Kuhn and Johnson (2013). Practical Exercise Regresssion set.seed(41) rf_regress_model &lt;- randomForest(solTrainXtrans,solTrainY, importance=TRUE, ntrees=500) rf_regress_model ## ## Call: ## randomForest(x = solTrainXtrans, y = solTrainY, importance = TRUE, ntrees = 500) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 76 ## ## Mean of squared residuals: 0.4222374 ## % Var explained: 89.91 head(rf_regress_model$importance) ## %IncMSE IncNodePurity ## FP001 0.006445326 1.786854 ## FP002 0.010880283 2.427839 ## FP003 0.007691509 2.469004 ## FP004 0.025182610 17.044367 ## FP005 0.004678159 1.405516 ## FP006 0.009439474 4.459932 #We won&#39;t run this, but this is how you would train this model using CARET # mtry_min &lt;- floor(ncol(solTrainXtrans)/3) # mtry_max &lt;- ncol(solTrainXtrans) # mtry &lt;- seq(mtry_min,mtry_max) # train_control &lt;- trainControl(method=&#39;cv&#39;, number=5, search=&#39;random&#39;) # model_metric &lt;- &quot;RMSE&quot; # tune_grid &lt;- expand.grid(.mtry=mtry) # # rf_random_regress &lt;- train(solTrainXtrans, solTrainY, # method = &quot;rf&quot;, # trControl= train_control, # metric = model_metric, # tuneGrid = tune_grid # ) Classification #Random search set.seed(41) mtry_min &lt;- floor(sqrt(ncol(pima_train)-1)) mtry_max &lt;- ncol(pima_train)-1 mtry &lt;- seq(mtry_min,mtry_max) train_control &lt;- trainControl(method=&#39;cv&#39;, number=5, search=&#39;random&#39;) model_metric &lt;- &quot;Accuracy&quot; tune_grid &lt;- expand.grid(.mtry=mtry) rf_random_class &lt;- train(as.factor(diabetes) ~., data = pima_train, method = &quot;rf&quot;, trControl= train_control, metric = model_metric, tuneGrid = tune_grid ) rf_random_class$results ## mtry Accuracy Kappa AccuracySD KappaSD ## 1 2 0.7940864 0.5291364 0.02769544 0.06039786 ## 2 3 0.7780581 0.4914831 0.03170987 0.06987462 ## 3 4 0.8005909 0.5439405 0.02218887 0.04789905 ## 4 5 0.7846155 0.5031842 0.02364283 0.06340367 ## 5 6 0.7813880 0.4970204 0.03093920 0.07522984 ## 6 7 0.7846667 0.5053203 0.03236422 0.08151918 ## 7 8 0.7814921 0.4994510 0.03575589 0.08559938 4.3 AdaBoost Figure 4.3: Adaboost classification algorithm from Kuhn and Johnson (2013). Practical Exercise 4.4 Gradient Boosting Figure 4.4: Gradient Boosting regression algorithm from Kuhn and Johnson (2013). Figure 4.5: Gradient Boosting classification algorithm from Kuhn and Johnson (2013). Practical Exercise "]
]
