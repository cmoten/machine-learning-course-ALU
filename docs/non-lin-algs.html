<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Practical Guide for Machine Learning and R Shiny</title>
  <meta name="description" content="Everything you need (and nothing more) to start a bookdown book.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="A Practical Guide for Machine Learning and R Shiny" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="<a href="https://cmoten.github.io/machine-learning-course-ALU" class="uri">https://cmoten.github.io/machine-learning-course-ALU</a>" />
  
  <meta property="og:description" content="Everything you need (and nothing more) to start a bookdown book." />
  <meta name="github-repo" content="cmoten/machine-learning-course-ALU" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Practical Guide for Machine Learning and R Shiny" />
  
  <meta name="twitter:description" content="Everything you need (and nothing more) to start a bookdown book." />
  

<meta name="author" content="Cardy Moten III">


<meta name="date" content="2018-03-07">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="lin-algs.html">
<link rel="next" href="ens-algs.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning & R Shiny</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ml-overview.html"><a href="ml-overview.html"><i class="fa fa-check"></i><b>1</b> Machine Learning Overview</a><ul>
<li class="chapter" data-level="1.1" data-path="ml-overview.html"><a href="ml-overview.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="ml-overview.html"><a href="ml-overview.html#what-machine-learning-is-not"><i class="fa fa-check"></i><b>1.2</b> What machine learning is not</a><ul>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-models-require-a-lot-of-data"><i class="fa fa-check"></i>Machine learning models require a lot of data</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-models-are-not-a-substitute-for-domain-expertise"><i class="fa fa-check"></i>Machine learning models are not a substitute for domain expertise</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-models-require-constant-maintenance"><i class="fa fa-check"></i>Machine learning models require constant maintenance</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ml-overview.html"><a href="ml-overview.html#what-do-i-need-to-know-to-get-started-with-machine-learning"><i class="fa fa-check"></i><b>1.3</b> What do I need to know to get started with machine learning?</a><ul>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#the-role-of-data-in-machine-learning"><i class="fa fa-check"></i>The role of data in machine learning</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#the-difference-between-parametric-and-nonparametric-machine-learning-algorithms"><i class="fa fa-check"></i>The difference between parametric and nonparametric machine learning algorithms</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#the-difference-between-supervised-unsupervised-and-semi-supervised-learning"><i class="fa fa-check"></i>The difference between supervised, unsupervised and semi-supervised learning</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#the-bias-variance-tradeoff"><i class="fa fa-check"></i>The bias-variance tradeoff</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#overfitting-and-underfitting-and-what-to-do-about-it"><i class="fa fa-check"></i>Overfitting and underfitting and what to do about it</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lin-algs.html"><a href="lin-algs.html"><i class="fa fa-check"></i><b>2</b> Linear Algorithms</a><ul>
<li class="chapter" data-level="2.1" data-path="lin-algs.html"><a href="lin-algs.html#gradient-descent"><i class="fa fa-check"></i><b>2.1</b> Gradient Descent</a><ul>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#linear-regression-model"><i class="fa fa-check"></i>Linear Regression Model</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#cost-function"><i class="fa fa-check"></i>Cost Function</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#logistic-regression"><i class="fa fa-check"></i>Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#gradient-descent-algorithm"><i class="fa fa-check"></i>Gradient Descent Algorithm</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#gradient-descent-intuition"><i class="fa fa-check"></i>Gradient Descent Intuition</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="lin-algs.html"><a href="lin-algs.html#practical-exercises"><i class="fa fa-check"></i><b>2.2</b> Practical Exercises</a><ul>
<li class="chapter" data-level="2.2.1" data-path="lin-algs.html"><a href="lin-algs.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>2.2.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="2.2.2" data-path="lin-algs.html"><a href="lin-algs.html#multiple-linear-regression-1"><i class="fa fa-check"></i><b>2.2.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="2.2.3" data-path="lin-algs.html"><a href="lin-algs.html#logistic-regression-1"><i class="fa fa-check"></i><b>2.2.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="lin-algs.html"><a href="lin-algs.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>2.3</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#lda-intuition"><i class="fa fa-check"></i>LDA Intuition</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#lda-estimates-for-one-predictor"><i class="fa fa-check"></i>LDA Estimates for One Predictor</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#lda-with-miltiple-predictors"><i class="fa fa-check"></i>LDA With Miltiple Predictors</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="lin-algs.html"><a href="lin-algs.html#practical-exercise"><i class="fa fa-check"></i><b>2.4</b> Practical Exercise</a><ul>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#data-2"><i class="fa fa-check"></i>Data</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#lda-scoring"><i class="fa fa-check"></i>LDA Scoring</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="non-lin-algs.html"><a href="non-lin-algs.html"><i class="fa fa-check"></i><b>3</b> Non-linear Algorithms</a><ul>
<li class="chapter" data-level="3.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>3.1</b> Classification and Regression Trees (CART)</a><ul>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#what-are-cart-models"><i class="fa fa-check"></i>What are CART models?</a></li>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#how-does-a-cart-model-learn-from-data"><i class="fa fa-check"></i>How does a CART model learn from data?</a></li>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#pre-processing-requirements"><i class="fa fa-check"></i>Pre-processing requirements?</a></li>
<li class="chapter" data-level="3.1.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#practical-exerecise"><i class="fa fa-check"></i><b>3.1.1</b> Practical Exerecise</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="non-lin-algs.html"><a href="non-lin-algs.html#naive-bayes"><i class="fa fa-check"></i><b>3.2</b> Naive Bayes</a><ul>
<li class="chapter" data-level="3.2.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#practical-exerecise-1"><i class="fa fa-check"></i><b>3.2.1</b> Practical Exerecise</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="non-lin-algs.html"><a href="non-lin-algs.html#k-nearest-neigbors"><i class="fa fa-check"></i><b>3.3</b> k-Nearest Neigbors</a><ul>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#curse-of-dimensionality"><i class="fa fa-check"></i>Curse of Dimensionality</a></li>
<li class="chapter" data-level="3.3.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#practical-exerecise-2"><i class="fa fa-check"></i><b>3.3.1</b> Practical Exerecise</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="non-lin-algs.html"><a href="non-lin-algs.html#support-vector-machines"><i class="fa fa-check"></i><b>3.4</b> Support Vector Machines</a><ul>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#regression-1"><i class="fa fa-check"></i>Regression</a></li>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#classification-1"><i class="fa fa-check"></i>Classification</a></li>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#optimizaton-forumulation"><i class="fa fa-check"></i>Optimizaton Forumulation</a></li>
<li class="chapter" data-level="3.4.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#practical-exerecise-3"><i class="fa fa-check"></i><b>3.4.1</b> Practical Exerecise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ens-algs.html"><a href="ens-algs.html"><i class="fa fa-check"></i><b>4</b> Ensemble Algorithms</a><ul>
<li class="chapter" data-level="4.1" data-path="ens-algs.html"><a href="ens-algs.html#bagging"><i class="fa fa-check"></i><b>4.1</b> Bagging</a><ul>
<li class="chapter" data-level="" data-path="ens-algs.html"><a href="ens-algs.html#practical-exercise-1"><i class="fa fa-check"></i>Practical Exercise</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ens-algs.html"><a href="ens-algs.html#random-forest"><i class="fa fa-check"></i><b>4.2</b> Random Forest</a><ul>
<li class="chapter" data-level="" data-path="ens-algs.html"><a href="ens-algs.html#practical-exercise-2"><i class="fa fa-check"></i>Practical Exercise</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ens-algs.html"><a href="ens-algs.html#adaboost"><i class="fa fa-check"></i><b>4.3</b> AdaBoost</a><ul>
<li class="chapter" data-level="" data-path="ens-algs.html"><a href="ens-algs.html#practical-exercise-3"><i class="fa fa-check"></i>Practical Exercise</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ens-algs.html"><a href="ens-algs.html#gradient-boosting"><i class="fa fa-check"></i><b>4.4</b> Gradient Boosting</a><ul>
<li class="chapter" data-level="" data-path="ens-algs.html"><a href="ens-algs.html#practical-exercise-4"><i class="fa fa-check"></i>Practical Exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ml-pe.html"><a href="ml-pe.html"><i class="fa fa-check"></i><b>5</b> Machine Learning Practical Exercise</a><ul>
<li class="chapter" data-level="5.1" data-path="ml-pe.html"><a href="ml-pe.html#modeling-workflow"><i class="fa fa-check"></i><b>5.1</b> Modeling Workflow</a></li>
<li class="chapter" data-level="5.2" data-path="ml-pe.html"><a href="ml-pe.html#performance-metrics"><i class="fa fa-check"></i><b>5.2</b> Performance Metrics</a></li>
<li class="chapter" data-level="5.3" data-path="ml-pe.html"><a href="ml-pe.html#practice-projects"><i class="fa fa-check"></i><b>5.3</b> Practice Projects</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="shiny-tut.html"><a href="shiny-tut.html"><i class="fa fa-check"></i><b>6</b> R Shiny Tutorial</a><ul>
<li class="chapter" data-level="6.1" data-path="shiny-tut.html"><a href="shiny-tut.html#what-we-will-learn"><i class="fa fa-check"></i><b>6.1</b> What we will learn</a></li>
<li class="chapter" data-level="6.2" data-path="shiny-tut.html"><a href="shiny-tut.html#shiny-basics"><i class="fa fa-check"></i><b>6.2</b> Shiny Basics</a></li>
<li class="chapter" data-level="6.3" data-path="shiny-tut.html"><a href="shiny-tut.html#design-principles"><i class="fa fa-check"></i><b>6.3</b> Design Principles</a></li>
<li class="chapter" data-level="" data-path="shiny-tut.html"><a href="shiny-tut.html#practical-exerecise-4"><i class="fa fa-check"></i>Practical Exerecise</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Practical Guide for Machine Learning and R Shiny</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="non-lin-algs" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Non-linear Algorithms</h1>
<p>We now focus our attention on non-linear machine learning algorithms. As we learn about these algorithms, you should notice that many of these are an extension of the linear algorithms we learned in Chapter <a href="lin-algs.html#lin-algs">2</a>.</p>
<div id="classification-and-regression-trees-cart" class="section level2">
<h2><span class="header-section-number">3.1</span> Classification and Regression Trees (CART)</h2>
<p>The first algorithm we will examine is the CART algorithm. This algorithm is crucial as it forms the basis for ensemble algorithms such as Random Forests and Bagged Decision Trees which we will learn in Chapter <a href="ens-algs.html#ens-algs">4</a>. CART models are also used for both regression and classification problems.</p>
<div id="what-are-cart-models" class="section level3 unnumbered">
<h3>What are CART models?</h3>
<p>CART models are simply decision trees. That is to say; the CART algorithm searches for points in the data to split the data into rectangular sections that increase the prediction accuracy. The more splits that are made within the data produces smaller and smaller segments up to a designated stopping point to prevent overfitting. A simple example will illustrate the intuition behind the CART algorithm. Figure <a href="non-lin-algs.html#fig:cart-example">3.1</a> demonstrates a simple CART model. Reviewing this output, we can see the definition of the model being</p>
<pre><code>if Predictor A &gt;= 1.7 then
   if Predictor B &gt;= 202.1 the Outcome = 1.3
   else Outcome = 5.6
else Outcome = 2.5</code></pre>
<div class="figure" style="text-align: center"><span id="fig:cart-example"></span>
<img src="img/cart-example.png" alt="Example output and decision tree model adapted from Kuhn and Johnson (2013)." width="90%" />
<p class="caption">
Figure 3.1: Example output and decision tree model adapted from Kuhn and Johnson (2013).
</p>
</div>
<p>Using the above decision algorithm, we can make future predictions based on the split values of Predictor A and B.</p>
</div>
<div id="how-does-a-cart-model-learn-from-data" class="section level3 unnumbered">
<h3>How does a CART model learn from data?</h3>
<div id="regression-trees" class="section level4 unnumbered">
<h4>Regression Trees</h4>
<p>For regression trees, CART models search through all the data points for each predictor to determine the optimal split point that partitions the data into two groups and the sum of squared errors (SSE) is the lowest possible value for that split. In the previous example, that value was 1.7 for Predictor A. From that first split; the method is repeated within each new region until the model reaches a designated stopping point, for instance, <span class="math inline">\(n &lt; 20\)</span> data points in any new region.</p>
<p><span class="math display">\[
SSE\ =\ \sum_{i\in S_1}^{ }\left(y_i-\overline{y_1}\right)^2\ +\ \sum_{i\in S_2}^{ }\left(y_i-\overline{y_2}\right)^2
\]</span></p>
</div>
<div id="classification-trees" class="section level4 unnumbered">
<h4>Classification Trees</h4>
<p>A frequently used measure for classification trees is the GINI index and is computed by</p>
<p><span class="math display">\[
G\ =\ \sum_{k=1}^Kp_k\times\left(1-p_k\right)
\]</span></p>
<p>where <span class="math inline">\(p_k\)</span> is the classification probability of the <span class="math inline">\(k\)</span>th class. The optimal split point search process is similar to the regression method, except now the algorithm searches for the best split point based on the lowest Gini index indicating the purest node for that split. In this case, purity refers to a node having more of one particular class than another.</p>
</div>
<div id="two-class-example" class="section level4 unnumbered">
<h4>Two-class Example</h4>
<p>To illustrate how to compute the Gini index, we will walk through a simple two-class example. The first step is to sort the sample based on the predictor values and then find the midpoint of the optimal split point. This split would create a contingency table like the one below. For this table, <span class="math inline">\(n_{11}\)</span> is the proportion of sample observations that are in group 1(samples that are greater than the split value) class 1. The same logic follows for the other three split values. The bold-faced values are the sub-totals of the split groups and the classifications.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">Class1</th>
<th align="center">Class2</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(&gt;\)</span> split</td>
<td align="center"><span class="math inline">\(n_{11}\)</span></td>
<td align="center"><span class="math inline">\(n_{12}\)</span></td>
<td><span class="math inline">\(\mathbf{n_{&gt;split}}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\leq\)</span> split</td>
<td align="center"><span class="math inline">\(n_{21}\)</span></td>
<td align="center"><span class="math inline">\(n_{22}\)</span></td>
<td><span class="math inline">\(\mathbf{n_{\leq split}}\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td align="center"><span class="math inline">\(\mathbf{n_{class1}}\)</span></td>
<td align="center"><span class="math inline">\(\mathbf{n_{class2}}\)</span></td>
<td><span class="math inline">\(\mathbf{n}\)</span></td>
</tr>
</tbody>
</table>
<p>Before the split, the initial Gini index is</p>
<p><span class="math display">\[G = 2\left(\frac{n_{class1}}{n}\right)\left(\frac{n_{class2}}{n}\right)\]</span>.</p>
<p>After the split, the Gini index changes to</p>
<p><span class="math display">\[
\begin{align}
G &amp;=\ 2\left[\left(\frac{n_{11}}{n_{&gt;split}}\right)\left(\frac{n_{12}}{n_{&gt;split}}\right)\left(\frac{n_{&gt;split}}{n}\right)\ +\ \left(\frac{n_{21}}{n_{\leq split}}\right)\left(\frac{n_{22}}{n_{\leq split}}\right)\left(\frac{n_{\leq split}}{n}\right)\right]\\
&amp;=\ 2\left[\left(\frac{n_{11}}{n}\right)\left(\frac{n_{12}}{n_{&gt;split}}\right)\ +\ \left(\frac{n_{21}}{n}\right)\left(\frac{n_{22}}{n_{\leq split}}\right)\right]
\end{align}
\]</span></p>
<p>We can see from the above equation that the Gini index now depends upon the proportion of samples of each class within a region that is weighted by the proportion of sample points in each split group. We compare the new Gini index value to the previous value of the Gini index. If the new value is smaller than the previous value, the model makes the split the proposed split otherwise.</p>
<p>Another frequently used method is the Information (Entropy) index and is calculated by</p>
<p><span class="math display">\[
I =\ \sum_{k=1}^{K}-p_klog_2\left(p_k\right)
\]</span></p>
<p>Similar to the Gini index for K = 2 classes, the information before a split is</p>
<p><span class="math display">\[
I(\text{before split}) = -\left[\frac{n_{class1}}{n}\ \times log_2\left(\frac{n_{class1}}{n}\right)\right]\ - \left[\frac{n_{class2}}{n}\ \times log_2\left(\frac{n_{class2}}{n}\right)\right]
\]</span></p>
<p>To determine how well a split improved the model, we will compute the information gain statistic. An increase in gain is an advantage, and a decrease in gain is a disadvantage. The calculation of gain is</p>
<p><span class="math display">\[
gain(\text{split}) =\ I(\text{before split})\ -\ I(\text{after split})
\]</span></p>
<p>To calculate the information index after the split, do the following</p>
<p><span class="math display">\[
\begin{align}
I(&gt;split) &amp;=\ -\left[\frac{n_{11}}{n_{&gt;split}}\ \times\ log_2\left(\frac{n_{11}}{n_{&gt;split}}\right)\right]\ - \left[\frac{n_{12}}{n_{&gt;split}}\ \times\ log_2\left(\frac{n_{12}}{n_{&gt;split}}\right)\right]\\
I(\leq split) &amp;=\ -\left[\frac{n_{21}}{n_{\leq split}}\ \times\ log_2\left(\frac{n_{21}}{n_{\leq split}}\right)\right]\ - \left[\frac{n_{22}}{n_{\leq split}}\ \times\ log_2\left(\frac{n_{22}}{n_{\leq split}}\right)\right]\\ 
I(\text{after split}) &amp;=\ \frac{n_{&gt;split}}{n}\ I(&gt;split)\ +\ \frac{n_{\leq split}}{n}\ I(\leq split)
\end{align}
\]</span></p>
<div id="gini-example" class="section level5 unnumbered">
<h5>Gini Example</h5>
<p>We will now work through an example problem using the Gini index. Figure <a href="non-lin-algs.html#fig:gini-example">3.2</a> shows the results of predicted classes with regions for a two-class model. There are a total of 208 observations: 111 observations for Class 1 and 97 observations for Class 2. Using this information, we can compute the Gini index before any splits.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n_obs &lt;-<span class="st"> </span><span class="dv">208</span>
n_class_one &lt;-<span class="st"> </span><span class="dv">111</span>
n_class_two &lt;-<span class="st"> </span><span class="dv">97</span>
gini_before &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(n_class_one<span class="op">/</span>n_obs) <span class="op">*</span><span class="st"> </span>(n_class_two<span class="op">/</span>n_obs)</code></pre></div>
<p>Based on the above calculation the pre-split Gini index is 0.498.</p>
<div class="figure" style="text-align: center"><span id="fig:gini-example"></span>
<img src="img/applied-pred-Ch14Fig01.png" alt="Example classification model results." width="90%" />
<p class="caption">
Figure 3.2: Example classification model results.
</p>
</div>
<p>The contingency table for Predictor B of the above figure is below. Using this information, we can compute the post-split Gini index</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">Class1</th>
<th align="center">Class2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(B &gt; 0.197\)</span></td>
<td align="center">91</td>
<td align="center">30</td>
</tr>
<tr class="even">
<td><span class="math inline">\(B \leq 0.197\)</span></td>
<td align="center">20</td>
<td align="center">67</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n11 &lt;-<span class="st"> </span><span class="dv">91</span>; n12 &lt;-<span class="st"> </span><span class="dv">30</span>; n21 &lt;-<span class="st"> </span><span class="dv">20</span>; n22 &lt;-<span class="st"> </span><span class="dv">67</span>;
n_group_one &lt;-<span class="st"> </span><span class="dv">121</span>; n_group_two &lt;-<span class="st"> </span><span class="dv">87</span>;
group_one_prop &lt;-<span class="st"> </span>(n11<span class="op">/</span>n_obs)<span class="op">*</span>(n12<span class="op">/</span>n_group_one)
group_two_prop &lt;-<span class="st"> </span>(n21<span class="op">/</span>n_obs)<span class="op">*</span>(n22<span class="op">/</span>n_group_two)
gini_after &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(group_one_prop,group_two_prop)</code></pre></div>
<p>The final Gini index post-split is now 0.365 which indicates an improvement in classification purity. We can also observe that any value <span class="math inline">\(\leq 0.197\)</span> will receive a classification of 2 and a classification of 1 otherwise with regards to this particular split point.</p>
</div>
</div>
</div>
<div id="pre-processing-requirements" class="section level3 unnumbered">
<h3>Pre-processing requirements?</h3>
<p>CART models do not require any special pre-processing of the data, but you can center and scale values based on skewness and other factors.</p>
</div>
<div id="practical-exerecise" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Practical Exerecise</h3>
<div id="libraries" class="section level4 unnumbered">
<h4>Libraries</h4>
<p>This exercise will use the <code>AppliedPredictiveModeling</code>, <code>rpart</code>, <code>caret</code>, and <code>partykit</code> packages.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(AppliedPredictiveModeling)
<span class="kw">library</span>(rpart)
<span class="kw">library</span>(caret)
<span class="kw">library</span>(partykit)
<span class="kw">library</span>(mlbench)
<span class="kw">library</span>(kernlab)</code></pre></div>
</div>
<div id="regression-tree" class="section level4 unnumbered">
<h4>Regression Tree</h4>
<div id="data-3" class="section level5 unnumbered">
<h5>Data</h5>
<p>For this exercise, we will use the solubility dataset described in <span class="citation">Kuhn and Johnson (<a href="#ref-kuhn2013applied">2013</a>)</span>. In short, the features of this dataset are:</p>
<ul>
<li>208 binary “fingerprints” that indicate the presence or absence of a particular chemical sub-structure;</li>
<li>16 count descriptors (such as the number of bonds or the number of Bromine atoms);</li>
<li>4 continuous descriptors (such as molecular weight or surface area) <span class="citation">(Kuhn and Johnson <a href="#ref-kuhn2014package">2014</a>)</span>.</li>
</ul>
<p>The authors centered and scaled the data to account for skewness. The target variable is a vector of log10 solubility values. The goal of this exercise is to predict the solubility value based on the set of features. Below is a view of some of the features and target values</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(solubility)
<span class="kw">str</span>(solTrainXtrans[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,<span class="dv">209</span><span class="op">:</span><span class="dv">228</span>)])</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    951 obs. of  30 variables:
##  $ FP001            : int  0 0 1 0 0 1 0 1 1 1 ...
##  $ FP002            : int  1 1 1 0 0 0 1 0 0 1 ...
##  $ FP003            : int  0 0 1 1 1 1 0 1 1 1 ...
##  $ FP004            : int  0 1 1 0 1 1 1 1 1 1 ...
##  $ FP005            : int  1 1 1 0 1 0 1 0 0 1 ...
##  $ FP006            : int  0 1 0 0 1 0 0 0 1 1 ...
##  $ FP007            : int  0 1 0 1 0 0 0 1 1 1 ...
##  $ FP008            : int  1 1 1 0 0 0 1 0 0 0 ...
##  $ FP009            : int  0 0 0 0 1 1 1 0 1 0 ...
##  $ FP010            : int  0 0 1 0 0 0 0 0 0 0 ...
##  $ MolWeight        : num  5.34 5.9 5.33 4.92 5.44 ...
##  $ NumAtoms         : num  3.37 3.91 3.53 3.3 3.47 ...
##  $ NumNonHAtoms     : num  2.83 3.3 2.77 2.4 2.77 ...
##  $ NumBonds         : num  3.43 3.97 3.53 3.3 3.47 ...
##  $ NumNonHBonds     : num  4.01 4.87 3.71 3.08 3.71 ...
##  $ NumMultBonds     : num  5.26 4.68 3.24 1.38 2.94 ...
##  $ NumRotBonds      : num  0 1.609 1.609 0.693 1.792 ...
##  $ NumDblBonds      : num  0 0 0.567 0.805 0 ...
##  $ NumAromaticBonds : num  2.83 2.56 1.95 0 1.95 ...
##  $ NumHydrogen      : num  3.86 5.32 4.73 4.47 4.47 ...
##  $ NumCarbon        : num  4.18 5.09 4.02 3.51 3.32 ...
##  $ NumNitrogen      : num  0.585 0.642 0 0 0.694 ...
##  $ NumOxygen        : num  0 0.693 1.099 0 0 ...
##  $ NumSulfer        : num  0 0.375 0 0 0 0.375 0 0 0 0 ...
##  $ NumChlorine      : num  0 0 0 0 0.375 ...
##  $ NumHalogen       : num  0 0 0 0 0.375 ...
##  $ NumRings         : num  1.386 1.609 0.693 0.693 0.693 ...
##  $ HydrophilicFactor: num  -1.607 -0.441 -0.385 -2.373 -0.071 ...
##  $ SurfaceArea1     : num  6.81 9.75 8.25 0 9.91 ...
##  $ SurfaceArea2     : num  6.81 12.03 8.25 0 9.91 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(solTrainY)</code></pre></div>
<pre><code>##  num [1:951] -3.97 -3.98 -3.99 -4 -4.06 -4.08 -4.08 -4.1 -4.1 -4.11 ...</code></pre>
</div>
<div id="create-and-analyze-regression-tree" class="section level5 unnumbered">
<h5>Create and Analyze Regression Tree</h5>
<p>The <code>rpart()</code> function in <code>R</code> is a widely used method for computing trees using the CART method, and we will use this function. Another package <code>party</code> uses the <a href="https://stats.stackexchange.com/questions/12140/conditional-inference-trees-vs-traditional-decision-trees">conditional inference framework</a> to form its trees.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">100</span>)
rpartTune &lt;-<span class="st"> </span><span class="kw">train</span>(solTrainXtrans, solTrainY,
                   <span class="dt">method =</span> <span class="st">&quot;rpart2&quot;</span>,
                   <span class="dt">tuneLength =</span> <span class="dv">10</span>,
                   <span class="dt">trControl=</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>)
                   )
rpartTune<span class="op">$</span>results</code></pre></div>
<pre><code>##    maxdepth     RMSE  Rsquared       MAE     RMSESD RsquaredSD      MAESD
## 1         1 1.617667 0.3745252 1.2657915 0.11511437 0.05777279 0.08191460
## 2         2 1.433114 0.5067404 1.1326186 0.07599686 0.04909341 0.04940391
## 3         3 1.357672 0.5568291 1.0657348 0.07354389 0.05231774 0.06091190
## 4         4 1.263596 0.6166997 0.9974476 0.10201869 0.05547696 0.07947602
## 5         5 1.192831 0.6581800 0.9429124 0.11324197 0.05669830 0.08594278
## 6         6 1.142654 0.6853056 0.9009065 0.10585813 0.05990671 0.08607556
## 7         7 1.111858 0.7020728 0.8707216 0.10580483 0.06389863 0.08126706
## 8         8 1.094535 0.7110088 0.8545809 0.11400541 0.06474333 0.09512021
## 9         9 1.091880 0.7116190 0.8465921 0.11938842 0.06737339 0.10068304
## 10       10 1.068799 0.7236716 0.8232469 0.12842861 0.07102897 0.10641491</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Build the initial model</span>

training_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">cbind</span>(solTrainXtrans,solTrainY))
training_model &lt;-<span class="st"> </span><span class="kw">rpart</span>(solTrainY <span class="op">~</span>., <span class="dt">data =</span> training_data,
                    <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">maxdepth =</span> <span class="dv">10</span>))

<span class="co">#Examine the tree complexity</span>
<span class="kw">plotcp</span>(training_model)</code></pre></div>
<p><img src="03-Non-Linear-Algorithms_files/figure-html/cart-tree-calc-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">training_model<span class="op">$</span>cptable</code></pre></div>
<pre><code>##            CP nsplit rel error    xerror       xstd
## 1  0.37300506      0 1.0000000 1.0010223 0.05357024
## 2  0.13770014      1 0.6269949 0.6314019 0.03143820
## 3  0.06971510      2 0.4892948 0.4945930 0.02321245
## 4  0.06180269      3 0.4195797 0.4434574 0.02133679
## 5  0.04729111      4 0.3577770 0.3838988 0.01904376
## 6  0.02650301      5 0.3104859 0.3514391 0.01837681
## 7  0.01789274      6 0.2839829 0.3062709 0.01570413
## 8  0.01553523      7 0.2660901 0.2989517 0.01566820
## 9  0.01178134      8 0.2505549 0.2910022 0.01551438
## 10 0.01150195      9 0.2387736 0.2879867 0.01543448
## 11 0.01000000     10 0.2272716 0.2761810 0.01525549</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Add min(xerror+xstd) and find the smallest tree w/xerror &lt; min(xerror+xstd)</span>
<span class="kw">which</span>(training_model<span class="op">$</span>cptable[,<span class="dv">4</span>] <span class="op">&lt;</span><span class="st"> </span><span class="kw">min</span>(training_model<span class="op">$</span>cptable[,<span class="dv">4</span>]<span class="op">+</span>training_model<span class="op">$</span>cptable[,<span class="dv">5</span>]))</code></pre></div>
<pre><code>##  9 10 11 
##  9 10 11</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Prune the tree</span>
training_model &lt;-<span class="st"> </span><span class="kw">rpart</span>(solTrainY <span class="op">~</span>., <span class="dt">data =</span> training_data,
                    <span class="dt">cp =</span> .<span class="dv">014</span>)
model_tree &lt;-<span class="st"> </span>as.party</code></pre></div>
<p>Figure <a href="non-lin-algs.html#fig:cart-plot">3.3</a> displays the final results that we can use for interpretation of the model. To create the plot just use the code <code>plot(model_tree)</code>. You could also use the <code>prp</code> function from the <code>rpart.plot</code> package. Using that package the <code>prp</code> plot would be</p>
<p><code>prp(training_model,type=4,extra=106,box.col = c(&quot;#deebf7&quot;,&quot;#fff7bc&quot;)[training_model$frame$yval],cex = 0.6)</code></p>
<div class="figure" style="text-align: center"><span id="fig:cart-plot"></span>
<img src="img/final-cart-plot.png" alt="Final CART model regression results." width="90%" />
<p class="caption">
Figure 3.3: Final CART model regression results.
</p>
</div>
</div>
</div>
<div id="classification-tree" class="section level4 unnumbered">
<h4>Classification Tree</h4>
<p>For this exercise, we will use the <code>PimaIndianDiabetes2</code> data from the <code>mlbench</code> package based on the dataset from the <a href="https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes">UCI Machine Learning Repository</a>. Click the link for a description of the dataset. In this exercise, we will build a classification tree that will classify a person as “pos” or “neg” for diabetes from a set of input features based on personal characteristics.</p>
<div id="data-4" class="section level5 unnumbered">
<h5>Data</h5>
<p>I already created a training and test dataset from the original data. There are some missing values, so we will only use the complete cases for this example since the algorithms won’t work with missing data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pima_train &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/pima-train.csv&quot;</span>,<span class="dt">header=</span><span class="ot">TRUE</span>)
pima_train &lt;-<span class="st"> </span>pima_train[<span class="kw">complete.cases</span>(pima_train),]
<span class="kw">str</span>(pima_train)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    311 obs. of  9 variables:
##  $ pregnant: int  1 0 2 5 0 1 1 3 11 10 ...
##  $ glucose : int  89 137 197 166 118 103 115 126 143 125 ...
##  $ pressure: int  66 40 70 72 84 30 70 88 94 70 ...
##  $ triceps : int  23 35 45 19 47 38 30 41 33 26 ...
##  $ insulin : int  94 168 543 175 230 83 96 235 146 115 ...
##  $ mass    : num  28.1 43.1 30.5 25.8 45.8 43.3 34.6 39.3 36.6 31.1 ...
##  $ pedigree: num  0.167 2.288 0.158 0.587 0.551 ...
##  $ age     : int  21 33 53 51 31 33 32 27 51 41 ...
##  $ diabetes: Factor w/ 2 levels &quot;neg&quot;,&quot;pos&quot;: 1 2 2 2 2 1 2 1 2 2 ...</code></pre>
</div>
<div id="create-and-analyze-classification-tree" class="section level5 unnumbered">
<h5>Create and Analyze Classification Tree</h5>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Gini Index</span>
<span class="kw">set.seed</span>(<span class="dv">33</span>)
train_ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>, <span class="dt">repeats =</span> <span class="dv">3</span>)
gini_tune &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="kw">as.factor</span>(diabetes) <span class="op">~</span>., <span class="dt">data =</span> pima_train, <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,
                   <span class="dt">trControl=</span>train_ctrl,
                   <span class="dt">tuneLength =</span> <span class="dv">10</span>, 
                   <span class="dt">parms=</span><span class="kw">list</span>(<span class="dt">split=</span><span class="st">&#39;gini&#39;</span>))
gini_tune</code></pre></div>
<pre><code>## CART 
## 
## 311 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 280, 280, 280, 280, 280, 280, ... 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa    
##   0.00000000  0.7790703  0.4994923
##   0.03219107  0.7740995  0.4937550
##   0.06438214  0.7699731  0.4670725
##   0.09657321  0.7580735  0.4369132
##   0.12876428  0.7591151  0.4401497
##   0.16095535  0.7410013  0.4188441
##   0.19314642  0.7420430  0.4219648
##   0.22533749  0.7420430  0.4219648
##   0.25752856  0.7420430  0.4219648
##   0.28971963  0.7150448  0.3111903
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pima_gini_model &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="kw">as.factor</span>(diabetes) <span class="op">~</span>., <span class="dt">data =</span> pima_train,
                    <span class="dt">cp =</span> .<span class="dv">004</span>)
pima_gini_tree &lt;-<span class="st"> </span><span class="kw">as.party</span>(pima_gini_model)

<span class="co">#Information Index</span>
<span class="kw">set.seed</span>(<span class="dv">33</span>)
info_tune &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="kw">as.factor</span>(diabetes) <span class="op">~</span>., <span class="dt">data =</span> pima_train, <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,
                   <span class="dt">trControl=</span>train_ctrl,
                   <span class="dt">tuneLength =</span> <span class="dv">10</span>, 
                   <span class="dt">parms=</span><span class="kw">list</span>(<span class="dt">split=</span><span class="st">&#39;information&#39;</span>))
info_tune</code></pre></div>
<pre><code>## CART 
## 
## 311 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 280, 280, 280, 280, 280, 280, ... 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa    
##   0.00000000  0.7768907  0.5019724
##   0.03219107  0.7720542  0.4896788
##   0.06438214  0.7731340  0.4906999
##   0.09657321  0.7430847  0.4352926
##   0.12876428  0.7441263  0.4385291
##   0.16095535  0.7366644  0.4268724
##   0.19314642  0.7377061  0.4299930
##   0.22533749  0.7377061  0.4299930
##   0.25752856  0.7377061  0.4299930
##   0.28971963  0.6912097  0.2398796
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pima_info_model &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="kw">as.factor</span>(diabetes)<span class="op">~</span>., <span class="dt">data =</span> pima_train,
                         <span class="dt">cp =</span> .<span class="dv">004</span>)
pima_info_tree &lt;-<span class="st"> </span><span class="kw">as.party</span>(pima_info_model)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:cart-gini-plot"></span>
<img src="img/cart-gini-example.png" alt="Gini Index CART model classification results." width="90%" />
<p class="caption">
Figure 3.4: Gini Index CART model classification results.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:cart-info-plot"></span>
<img src="img/cart-info-example.png" alt="Information Index CART model classification results." width="90%" />
<p class="caption">
Figure 3.5: Information Index CART model classification results.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="naive-bayes" class="section level2">
<h2><span class="header-section-number">3.2</span> Naive Bayes</h2>
<p>Recall Baye’s Theorem from Chapter <a href="lin-algs.html#lin-algs">2</a></p>
<p><span class="math display">\[
\Pr\left(Y\ =\ k\ |X\right)\ =\ \frac{P\left(X|Y\ =\ k\right)P\left(Y\right)}{P\left(X\right)}
\]</span></p>
<p>where we want to answer the question “what is the probability of a particular target classification given the observed features?”</p>
<p>Upon calculating the posterior probability for each classification, you can then select the classification with the highest probability. In the literature, this calculation is the maximum a posteriori (MAP), and we find it by</p>
<p><span class="math display">\[
\begin{align}
MAP(Y) &amp;=\ max\left(P(Y \vert X\right)\\
&amp;=\ max\left(\frac{P\left(X|Y\ =\ k\right)P\left(Y\right)}{P\left(X\right)}\right)\\
&amp;=\ max\left(P\left(X|Y\ =\ k\right)P\left(Y\right)\right)
\end{align}
\]</span></p>
<p>We can ignore the denominator of the original equation because the <span class="math inline">\(P(X)\)</span> is a constant for terms. Also, the reason why this method is called Naive Bayes is that the features are assumed to be independent. To put it another way, instead of computing <span class="math inline">\(P(x_1,x_2,\dots,x_p\ \vert Y)\)</span>, the independence assumption simplifies this calculation to</p>
<p><span class="math display">\[
P\left(X\vert Y\ =\ k\right) = \prod_{j=1}^{P}P\left(X \vert Y\ = k\right)
\]</span></p>
<p>Another aspect of the Naive Bayes method is the distribution of the features. A Gaussian distribution will be used for continuous features, and kern density estimates for discrete features.</p>
<div id="practical-exerecise-1" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Practical Exerecise</h3>
<p>We will use the Pima data for this exercise.</p>
<div id="naive-bayes-model" class="section level4 unnumbered">
<h4>Naive Bayes Model</h4>
<p>We will use the <code>naiveBayes</code> function from the <code>klaR</code> package along with the <code>caret</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">33</span>)
<span class="kw">library</span>(klaR)
nb_tune &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="kw">as.factor</span>(diabetes) <span class="op">~</span><span class="st"> </span>., 
         <span class="dt">data=</span>pima_train,
         <span class="dt">method =</span> <span class="st">&quot;nb&quot;</span>,
         <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;none&quot;</span>),
         <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">fL=</span><span class="dv">0</span>, <span class="dt">usekernel=</span><span class="ot">FALSE</span>, <span class="dt">adjust=</span><span class="dv">1</span>))
nb_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(nb_tune,pima_train,<span class="dt">type =</span> <span class="st">&quot;raw&quot;</span>)
<span class="kw">confusionMatrix</span>(nb_preds,<span class="kw">as.factor</span>(pima_train<span class="op">$</span>diabetes))</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction neg pos
##        neg 169  35
##        pos  35  72
##                                           
##                Accuracy : 0.7749          
##                  95% CI : (0.7244, 0.8201)
##     No Information Rate : 0.6559          
##     P-Value [Acc &gt; NIR] : 3.352e-06       
##                                           
##                   Kappa : 0.5013          
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.8284          
##             Specificity : 0.6729          
##          Pos Pred Value : 0.8284          
##          Neg Pred Value : 0.6729          
##              Prevalence : 0.6559          
##          Detection Rate : 0.5434          
##    Detection Prevalence : 0.6559          
##       Balanced Accuracy : 0.7507          
##                                           
##        &#39;Positive&#39; Class : neg             
## </code></pre>
</div>
</div>
</div>
<div id="k-nearest-neigbors" class="section level2">
<h2><span class="header-section-number">3.3</span> k-Nearest Neigbors</h2>
<p>The basic idea of the <span class="math inline">\(k\)</span>-nearest neighbors (KNN) algorithm is to create a distance matrix of the all the feature variables and choose the <span class="math inline">\(k\)</span> most adjacent data points closest to an evaluated point. Since KNN uses the entire dataset, no learning is necessary from the algorithm. The primary choice of the modeler is what decision metric to use. The primary parameter used is a variation of the Minkowski distance metric. You can compute this metric by</p>
<p><span class="math display">\[
\left(\sum_{i=1}^{P}\vert x_{ai} - x_{bi} \vert^q\right)^\frac{1}{q}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{x_a}\)</span> and <span class="math inline">\(\mathbf{x_b}\)</span> are two sample points in the dataset. When <span class="math inline">\(q\ =\ 1\)</span> this distance metric is the Manhattan (city-block) distance, and when <span class="math inline">\(q\ =\ 2\)</span> this distance is the Euclidean distance. You will use Euclidean distance for continuous predictors and Manhattan distance for categorical or binary predictors.</p>
<div id="curse-of-dimensionality" class="section level3 unnumbered">
<h3>Curse of Dimensionality</h3>
<p>Just like other machine learning methods the KNN method has its disadvantages. One disadvantage deals with high dimensional data. In essence, distances in higher dimensions are larger which ultimately mean that similar points are not necessarily local to each other. Figure <a href="non-lin-algs.html#fig:knn-curse">3.6</a> demonstrates this problem. The figure on the left shows a unit hypercube with a sub-cube that captures a fraction of the data of the original hypercube. The sub-figure on the right shows how much of the range of each coordinate you need to capture within the sub-cube. For instance, if you want to capture 10% of the data, you will need to capture 80% of the range of coordinates for a 10-dimension dataset. This percentage increases exponentially with additional dimensions.</p>
<div class="figure" style="text-align: center"><span id="fig:knn-curse"></span>
<img src="img/knn-curse.png" alt="Illustration of dimensionality curse adapted from Hastie, Tibshirani, and Friedman (2009." width="90%" />
<p class="caption">
Figure 3.6: Illustration of dimensionality curse adapted from Hastie, Tibshirani, and Friedman (2009.
</p>
</div>
</div>
<div id="practical-exerecise-2" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Practical Exerecise</h3>
<div id="regression" class="section level4 unnumbered">
<h4>Regression</h4>
<div id="data-5" class="section level5 unnumbered">
<h5>Data</h5>
<p>We will use the solubility data for this exercise.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn_data &lt;-<span class="st"> </span>solTrainXtrans[,<span class="op">-</span><span class="kw">nearZeroVar</span>(solTrainXtrans)]</code></pre></div>
</div>
<div id="create-the-model" class="section level5 unnumbered">
<h5>Create the model</h5>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">100</span>)
knn_reg_model &lt;-<span class="st"> </span><span class="kw">train</span>(knn_data,
                       solTrainY,
                       <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>,
                       <span class="co">#Center and scaling will occur for new predictors</span>
                       <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
                       <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">.k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">20</span>),
                       <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>)
                       )
knn_reg_model<span class="op">$</span>finalModel</code></pre></div>
<pre><code>## 4-nearest neighbor regression model</code></pre>
<p>The final model selected was a model based on a value of <span class="math inline">\(k\ =\ 4\)</span>. Figure <a href="non-lin-algs.html#fig:knn-reg-plot">3.7</a> shows graphically why this model was the best of the 20 analyzed.</p>
<div class="figure" style="text-align: center"><span id="fig:knn-reg-plot"></span>
<img src="img/knn-reg-plot.png" alt="Plots of RMSE and RSquared for values of k." width="90%" />
<p class="caption">
Figure 3.7: Plots of RMSE and RSquared for values of k.
</p>
</div>
</div>
</div>
<div id="classification" class="section level4 unnumbered">
<h4>Classification</h4>
<p>We will again use the Pima data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">202</span>)
pima_knn &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="kw">as.factor</span>(diabetes)<span class="op">~</span>.,
                  <span class="dt">data =</span> pima_train,
                  <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>,
                  <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,
                  <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
                  <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">.k=</span><span class="dv">1</span><span class="op">:</span><span class="dv">50</span>),
                  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,
                                           <span class="dt">classProbs =</span> <span class="ot">TRUE</span>,
                                           <span class="dt">summaryFunction =</span> twoClassSummary))
pima_knn</code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 311 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## Pre-processing: centered (8), scaled (8) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 280, 281, 280, 279, 280, 281, ... 
## Resampling results across tuning parameters:
## 
##   k   ROC        Sens       Spec     
##    1  0.6758874  0.7990476  0.5527273
##    2  0.7472781  0.7840476  0.5318182
##    3  0.7854405  0.8530952  0.5327273
##    4  0.7962013  0.8435714  0.5709091
##    5  0.7963950  0.8585714  0.5700000
##    6  0.7987013  0.8733333  0.5800000
##    7  0.8115682  0.8785714  0.5681818
##    8  0.8184437  0.8978571  0.5218182
##    9  0.8219935  0.8926190  0.5045455
##   10  0.8193268  0.8926190  0.5127273
##   11  0.8269091  0.8780952  0.5318182
##   12  0.8344470  0.8876190  0.4945455
##   13  0.8369232  0.8973810  0.4936364
##   14  0.8384621  0.8971429  0.4745455
##   15  0.8404080  0.8976190  0.4936364
##   16  0.8378268  0.8973810  0.5036364
##   17  0.8421396  0.9121429  0.4663636
##   18  0.8384405  0.9119048  0.4745455
##   19  0.8382392  0.9019048  0.4745455
##   20  0.8325335  0.9019048  0.4936364
##   21  0.8388268  0.9119048  0.5018182
##   22  0.8414740  0.9119048  0.4845455
##   23  0.8419794  0.9166667  0.4836364
##   24  0.8454058  0.9169048  0.4836364
##   25  0.8452814  0.9169048  0.4636364
##   26  0.8434935  0.9169048  0.4563636
##   27  0.8491331  0.9216667  0.4554545
##   28  0.8474957  0.9314286  0.4645455
##   29  0.8446385  0.9216667  0.4363636
##   30  0.8437154  0.9266667  0.4363636
##   31  0.8392987  0.9266667  0.4263636
##   32  0.8399816  0.9216667  0.4272727
##   33  0.8390368  0.9266667  0.4272727
##   34  0.8381483  0.9266667  0.4190909
##   35  0.8404892  0.9316667  0.4281818
##   36  0.8392532  0.9316667  0.4381818
##   37  0.8412446  0.9269048  0.4281818
##   38  0.8404448  0.9319048  0.4281818
##   39  0.8436937  0.9466667  0.4281818
##   40  0.8449589  0.9416667  0.4281818
##   41  0.8423431  0.9366667  0.4381818
##   42  0.8405693  0.9366667  0.4372727
##   43  0.8415855  0.9464286  0.4372727
##   44  0.8449123  0.9561905  0.4463636
##   45  0.8451190  0.9511905  0.4281818
##   46  0.8455054  0.9416667  0.4372727
##   47  0.8452846  0.9464286  0.4281818
##   48  0.8459361  0.9464286  0.4372727
##   49  0.8466537  0.9414286  0.4181818
##   50  0.8445725  0.9514286  0.4090909
## 
## ROC was used to select the optimal model using the largest value.
## The final value used for the model was k = 27.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pima_knn<span class="op">$</span>finalModel</code></pre></div>
<pre><code>## 27-nearest neighbor model
## Training set outcome distribution:
## 
## neg pos 
## 204 107</code></pre>
<p>The final model was computed from a total of <span class="math inline">\(k\ =\ 27\)</span> neighbors and a comparison of the ROC metric for each neighbor is shown in Figure <a href="non-lin-algs.html#fig:knn-class-plot">3.8</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:knn-class-plot"></span>
<img src="img/knn-class-plot.png" alt="Receiver-Operator Characteristic (ROC) curve results." width="90%" />
<p class="caption">
Figure 3.8: Receiver-Operator Characteristic (ROC) curve results.
</p>
</div>
</div>
<div id="exercise" class="section level4 unnumbered">
<h4>Exercise</h4>
<p>Create a new KNN model using only three predictor variables for the Pima data and compare those results to the current KNN model. Use the plot from the regression tree as a guide to determine which variables to choose.</p>
</div>
</div>
</div>
<div id="support-vector-machines" class="section level2">
<h2><span class="header-section-number">3.4</span> Support Vector Machines</h2>
<p>Support vector machines (SVM) started as a method for classification but has extensions for regression as well. The goal of using SVM is to compute a regression line or decision boundary.</p>
<div id="regression-1" class="section level3 unnumbered">
<h3>Regression</h3>
<p>For regression, the SVM seeks to minimize the cost function.</p>
<p><span class="math display">\[
Cost\sum_{i=1}^nL_\epsilon \left(y_i - \hat{y_i}\right)\ +\ \sum_{j=1}^{P}\beta_{j}^2 
\]</span></p>
<p>where <span class="math inline">\(Cost\)</span> is the residual cost penalty and <span class="math inline">\(L_\epsilon (\cdot)\)</span> is the margin threshold.</p>
<p>Furthermore, we transform the prediction equation for <span class="math inline">\(\hat{y}\)</span> to</p>
<p><span class="math display">\[
\begin{align}
\hat{y} &amp;=\ \beta_0\ +\ \beta_1u_1\ +\ \dots\ +\ \beta_pu_p\\
&amp;=\ \beta_0\ +\ \sum_{j=1}^{P}\beta_ju_j\\
&amp;=\ \beta_0\ +\ \sum_{j=1}^{P}\sum_{i=1}^{n}\alpha_ix_{ij}u_j\\
&amp;=\ \beta_0\ +\ \sum_{i=1}^{n}\alpha_i\left(\sum_{j=1}^{P}x_{ij}u_j\right)
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\alpha_i\)</span> is an unknown parameter estimated by the SVM algorithm. We can generalize the above equation for <span class="math inline">\(\hat{y}\)</span> even further to matrix form with</p>
<p><span class="math display">\[
f(u) = \beta_0\ +\ \sum_{i=1}^{n}\alpha_iK\left(x_i,\mathbf{u}\right)
\]</span></p>
<p>where <span class="math inline">\(K\left(\cdot\right)\)</span> is a kernel function. For SVM, there are a few models to choose for the kernel function to encompass linear and nonlinear functions of the predictors. The most common kernel functions are</p>
<p><span class="math display">\[
\begin{align}
\text{linear} &amp;=\ x_i^{T}\mathbf{u}\\
\text{polynomial} &amp;=\ \left(\phi(x^T\mathbf{u})\ +\ 1\right)^{degree}\\
\text{radial basis function} &amp;=\ e^{\left(-\sigma\vert\vert x\ -\ \mathbf{u} \vert\vert^2\right)}\\
\text{hyperbolic tangent} &amp;=\ tanh\left(\phi(x^T\mathbf{u})\ +\ 1\right)
\end{align}
\]</span></p>
<p>To demonstrate why you would use SVM for some regression situations, examine Figure <a href="non-lin-algs.html#fig:svm-regress">3.9</a> below. The top of the figure shows a comparison of a simple linear regression model using least squares and SVM. The item to notice in this plot is the outlier at the top left of the plot. You will see that the least squares model is more sensitive to this value, while SVM is not. The middle plot shows the relationship of the residuals to the predictive values. What is important to note with this plot is that the grey points are values that contributed to the regression line estimation, which the values shown by the red crosses did not contribute. This lack of contribution is because these non-contributing values are within the <span class="math inline">\(\pm\ \epsilon\ =\ 0.01\)</span> threshold set by the modeler. We explain why this happens in a little bit. The bottom plot shows a nonlinear application of SVM to a sine wave model. Again, there are outlier values in the bottom left of the plot, and the least squares method is more sensitive to these values than SVM. The reason for this is because least squares and other regression models like logistic regression are more global in their behavior, while SVM uses more local contributions from the data for its estimations.</p>
<div class="figure" style="text-align: center"><span id="fig:svm-regress"></span>
<img src="img/applied-pred-Ch7Fig07.png" alt="Example SVM regressions compared to least squares adapted from Kuhn and Johnson (2013)" width="90%" />
<p class="caption">
Figure 3.9: Example SVM regressions compared to least squares adapted from Kuhn and Johnson (2013)
</p>
</div>
<p>As we stated previously, the samples that contribute to the calculation of the regression curve, also called support vectors, are the samples outside of the <span class="math inline">\(\epsilon\)</span> threshold. At first, this finding may seem counterintuitive, so examining Figure <a href="non-lin-algs.html#fig:svm-res-models">3.10</a> could help with building an intuition for this. Specifically looking at the bottom right model, we can see that the residuals that are within the <span class="math inline">\(\epsilon\)</span> threshold are in fact zero and all other residuals contribute linearly to the regression equation.</p>
<div class="figure" style="text-align: center"><span id="fig:svm-res-models"></span>
<img src="img/applied-pred-Ch7Fig06.png" alt="Plots of various contributions to the regression line of model residuals adapted from Kuhn and Johnson (2013)" width="90%" />
<p class="caption">
Figure 3.10: Plots of various contributions to the regression line of model residuals adapted from Kuhn and Johnson (2013)
</p>
</div>
</div>
<div id="classification-1" class="section level3 unnumbered">
<h3>Classification</h3>
<p>For the classification version of SVM, we want to compute an optimal decision boundary between classes that are separable and even classes that are not separable. Figure <a href="non-lin-algs.html#fig:svm-example">3.11</a> shows how SVM attempts to solve this problem. On the left, you will notice two separable classes with infinite classification boundaries. On the right, is how SVM solves this problem. Explicitly, a boundary called the maximum margin classifier is computed. What’s unique about this boundary is that the boundary itself(solid black line) has margins that are set to the closets points for each class. The result of this is that now, unlike the regression model, samples that are closest to the boundary contribute to the classification model and those that are furthest away from the boundary do not contribute at all. The equations for the classification version of SVM are similar to the regression equations.</p>
<p>The general decision boundary equation is</p>
<p><span class="math display">\[
D(u) = \beta_0\ +\ \sum_{i=1}^{n}y_i\alpha_iK(x_i,\mathbf{u})
\]</span></p>
<p>The critical thing to notice is how the classification equation now includes the actual class of <span class="math inline">\(y_i\)</span> which is usually a value of -1 or 1.</p>
<p>The kernel functions are</p>
<p><span class="math display">\[
\begin{align}
\text{linear} &amp;=\ x_i^{T}\mathbf{u}\\
\text{polynomial} &amp;=\ \left(scale(x^T\mathbf{u})\ +\ 1\right)^{degree}\\
\text{radial basis function} &amp;=\ e^{\left(-\sigma\vert\vert x\ -\ \mathbf{u} \vert\vert^2\right)}\\
\text{hyperbolic tangent} &amp;=\ tanh\left(scale(x^T\mathbf{u})\ +\ 1\right)
\end{align}
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:svm-example"></span>
<img src="img/applied-pred-Ch13Fig09.png" alt="Datasets with separable classes adapted from Kuhn and Johnson (2013)" width="90%" />
<p class="caption">
Figure 3.11: Datasets with separable classes adapted from Kuhn and Johnson (2013)
</p>
</div>
</div>
<div id="optimizaton-forumulation" class="section level3 unnumbered">
<h3>Optimizaton Forumulation</h3>
<p>Ultimately, both the regression and classification equations are reformulated into a quadratic programming problem. While it is beyond the scope of this lesson to detail this formulation, the reader should review <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-friedman2001elements">2009</a>)</span>, specifically the section discussing SVMs. What’s key to understanding this formulation is that the tuning of the SVM depends primarily on the cost parameter when estimating the other parameters.</p>
</div>
<div id="practical-exerecise-3" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Practical Exerecise</h3>
<div id="regression-2" class="section level4 unnumbered">
<h4>Regression</h4>
<div id="data-6" class="section level5 unnumbered">
<h5>Data</h5>
<p>We will use the solubility data for this portion.</p>
</div>
<div id="model" class="section level5 unnumbered">
<h5>Model</h5>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">33</span>)
solubility_svm &lt;-<span class="st"> </span><span class="kw">train</span>(solTrainXtrans, solTrainY,
                        <span class="dt">method =</span> <span class="st">&quot;svmRadial&quot;</span>,
                        <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
                        <span class="dt">tuneLength =</span> <span class="dv">14</span>,
                        <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>))
solubility_svm</code></pre></div>
<pre><code>## Support Vector Machines with Radial Basis Function Kernel 
## 
## 951 samples
## 228 predictors
## 
## Pre-processing: centered (228), scaled (228) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 855, 856, 855, 855, 858, 857, ... 
## Resampling results across tuning parameters:
## 
##   C        RMSE       Rsquared   MAE      
##      0.25  0.8014848  0.8681906  0.6039660
##      0.50  0.7102286  0.8890663  0.5327464
##      1.00  0.6613592  0.9000814  0.4934836
##      2.00  0.6314286  0.9066489  0.4684522
##      4.00  0.6186151  0.9091987  0.4541152
##      8.00  0.6064508  0.9127569  0.4457087
##     16.00  0.6018292  0.9140239  0.4427055
##     32.00  0.6018878  0.9138726  0.4429424
##     64.00  0.6007705  0.9141895  0.4423807
##    128.00  0.5998743  0.9145279  0.4443426
##    256.00  0.6025174  0.9136982  0.4463239
##    512.00  0.6039626  0.9132093  0.4469188
##   1024.00  0.6042640  0.9131080  0.4474508
##   2048.00  0.6062670  0.9125208  0.4497784
## 
## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.002740343
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were sigma = 0.002740343 and C = 128.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">solubility_svm<span class="op">$</span>finalModel</code></pre></div>
<pre><code>## Support Vector Machine object of class &quot;ksvm&quot; 
## 
## SV type: eps-svr  (regression) 
##  parameter : epsilon = 0.1  cost C = 128 
## 
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  0.0027403433192481 
## 
## Number of Support Vectors : 638 
## 
## Objective Function Value : -726.094 
## Training error : 0.009299</code></pre>
</div>
</div>
<div id="classification-2" class="section level4 unnumbered">
<h4>Classification</h4>
<div id="data-7" class="section level5 unnumbered">
<h5>Data</h5>
<p>We will us the Pima dataset</p>
</div>
<div id="model-1" class="section level5 unnumbered">
<h5>Model</h5>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">202</span>)
sigmaRange &lt;-<span class="st"> </span><span class="kw">sigest</span>(<span class="kw">as.factor</span>(diabetes) <span class="op">~</span>.,<span class="dt">data=</span>pima_train)
svmGrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">.sigma =</span> sigmaRange[<span class="dv">1</span>],
                       <span class="dt">.C =</span> <span class="dv">2</span><span class="op">^</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>))) 
<span class="kw">set.seed</span>(<span class="dv">386</span>)
pima_svm &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="kw">as.factor</span>(diabetes)<span class="op">~</span>.,
                  <span class="dt">data =</span> pima_train,
                  <span class="dt">method =</span> <span class="st">&quot;svmRadial&quot;</span>,
                  <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,
                  <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>),
                  <span class="dt">tuneGrid =</span> svmGrid,
                  <span class="dt">fit =</span> <span class="ot">FALSE</span>,
                  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,
                                           <span class="dt">classProbs =</span> <span class="ot">TRUE</span>,
                                           <span class="dt">summaryFunction =</span> twoClassSummary))
pima_svm</code></pre></div>
<pre><code>## Support Vector Machines with Radial Basis Function Kernel 
## 
## 311 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## Pre-processing: centered (8), scaled (8) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 280, 279, 279, 279, 280, 280, ... 
## Resampling results across tuning parameters:
## 
##   C        ROC        Sens       Spec     
##    0.0625  0.8434134  0.7795238  0.6945455
##    0.1250  0.8434134  0.7842857  0.6945455
##    0.2500  0.8503506  0.8426190  0.6090909
##    0.5000  0.8476623  0.8573810  0.5909091
##    1.0000  0.8478203  0.8678571  0.5727273
##    2.0000  0.8427922  0.8528571  0.5736364
##    4.0000  0.8400390  0.8626190  0.5154545
##    8.0000  0.8388355  0.8828571  0.4872727
##   16.0000  0.8367100  0.9066667  0.4972727
## 
## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.03176525
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were sigma = 0.03176525 and C = 0.25.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pima_svm<span class="op">$</span>finalModel</code></pre></div>
<pre><code>## Support Vector Machine object of class &quot;ksvm&quot; 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 0.25 
## 
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  0.0317652466552473 
## 
## Number of Support Vectors : 197 
## 
## Objective Function Value : -43.388 
## Probability model included.</code></pre>

</div>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-kuhn2013applied">
<p>Kuhn, Max, and Kjell Johnson. 2013. <em>Applied Predictive Modeling</em>. Vol. 26. Springer.</p>
</div>
<div id="ref-kuhn2014package">
<p>Kuhn, Max, and Kjell Johnson. 2014. <em>AppliedPredictiveModeling: Functions and Data Sets for ’Applied Predictive Modeling’</em>. <a href="https://CRAN.R-project.org/package=AppliedPredictiveModeling" class="uri">https://CRAN.R-project.org/package=AppliedPredictiveModeling</a>.</p>
</div>
<div id="ref-friedman2001elements">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning</em>. 2nd ed. Springer series in statistics New York. <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" class="uri">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lin-algs.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ens-algs.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
