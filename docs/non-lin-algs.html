<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Practical Guide for Machine Learning and R Shiny</title>
  <meta name="description" content="Everything you need (and nothing more) to start a bookdown book.">
  <meta name="generator" content="bookdown 0.6.2 and GitBook 2.6.7">

  <meta property="og:title" content="A Practical Guide for Machine Learning and R Shiny" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="<a href="https://cmoten.github.io/machine-learning-course-ALU" class="uri">https://cmoten.github.io/machine-learning-course-ALU</a>" />
  
  <meta property="og:description" content="Everything you need (and nothing more) to start a bookdown book." />
  <meta name="github-repo" content="cmoten/machine-learning-course-ALU" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Practical Guide for Machine Learning and R Shiny" />
  
  <meta name="twitter:description" content="Everything you need (and nothing more) to start a bookdown book." />
  

<meta name="author" content="Cardy Moten III">


<meta name="date" content="2018-02-15">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="lin-algs.html">
<link rel="next" href="ens-algs.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning & R Shiny</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ml-overview.html"><a href="ml-overview.html"><i class="fa fa-check"></i><b>1</b> Machine Learning Overview</a><ul>
<li class="chapter" data-level="1.1" data-path="ml-overview.html"><a href="ml-overview.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="ml-overview.html"><a href="ml-overview.html#what-machine-learning-is-not"><i class="fa fa-check"></i><b>1.2</b> What machine learning is not</a><ul>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-models-require-a-lot-of-data"><i class="fa fa-check"></i>Machine learning models require a lot of data</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-models-are-not-a-substitute-for-domain-expertise"><i class="fa fa-check"></i>Machine learning models are not a substitute for domain expertise</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-models-require-constant-maintenance"><i class="fa fa-check"></i>Machine learning models require constant maintenance</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ml-overview.html"><a href="ml-overview.html#what-do-i-need-to-know-to-get-started-with-machine-learning"><i class="fa fa-check"></i><b>1.3</b> What do I need to know to get started with machine learning?</a><ul>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#the-role-of-data-in-machine-learning"><i class="fa fa-check"></i>The role of data in machine learning</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#the-difference-between-parametric-and-nonparametric-machine-learning-algorithms"><i class="fa fa-check"></i>The difference between parametric and nonparametric machine learning algorithms</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#the-difference-between-supervised-unsupervised-and-semi-supervised-learning"><i class="fa fa-check"></i>The difference between supervised, unsupervised and semi-supervised learning</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#the-bias-variance-tradeoff"><i class="fa fa-check"></i>The bias-variance tradeoff</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#overfitting-and-underfitting-and-what-to-do-about-it"><i class="fa fa-check"></i>Overfitting and underfitting and what to do about it</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lin-algs.html"><a href="lin-algs.html"><i class="fa fa-check"></i><b>2</b> Linear Algorithms</a><ul>
<li class="chapter" data-level="2.1" data-path="lin-algs.html"><a href="lin-algs.html#gradient-descent"><i class="fa fa-check"></i><b>2.1</b> Gradient Descent</a><ul>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#linear-regression-model"><i class="fa fa-check"></i>Linear Regression Model</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#cost-function"><i class="fa fa-check"></i>Cost Function</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#logistic-regression"><i class="fa fa-check"></i>Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#gradient-descent-algorithm"><i class="fa fa-check"></i>Gradient Descent Algorithm</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#gradient-descent-intuition"><i class="fa fa-check"></i>Gradient Descent Intuition</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="lin-algs.html"><a href="lin-algs.html#practical-exercises"><i class="fa fa-check"></i><b>2.2</b> Practical Exercises</a><ul>
<li class="chapter" data-level="2.2.1" data-path="lin-algs.html"><a href="lin-algs.html#simple-linear-regression-2"><i class="fa fa-check"></i><b>2.2.1</b> Simple Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="lin-algs.html"><a href="lin-algs.html#company-radio-sales"><i class="fa fa-check"></i><b>2.3</b> company radio sales</a></li>
<li class="chapter" data-level="2.4" data-path="lin-algs.html"><a href="lin-algs.html#section"><i class="fa fa-check"></i><b>2.4</b> 1 1 37.8 22.1</a></li>
<li class="chapter" data-level="2.5" data-path="lin-algs.html"><a href="lin-algs.html#section-1"><i class="fa fa-check"></i><b>2.5</b> 2 2 39.3 10.4</a></li>
<li class="chapter" data-level="2.6" data-path="lin-algs.html"><a href="lin-algs.html#section-2"><i class="fa fa-check"></i><b>2.6</b> 3 3 45.9 9.3</a></li>
<li class="chapter" data-level="2.7" data-path="lin-algs.html"><a href="lin-algs.html#section-3"><i class="fa fa-check"></i><b>2.7</b> 4 4 41.3 18.5</a></li>
<li class="chapter" data-level="2.8" data-path="lin-algs.html"><a href="lin-algs.html#section-4"><i class="fa fa-check"></i><b>2.8</b> 5 5 10.8 12.9</a></li>
<li class="chapter" data-level="2.9" data-path="lin-algs.html"><a href="lin-algs.html#section-5"><i class="fa fa-check"></i><b>2.9</b> 6 6 48.9 7.2</a></li>
<li class="chapter" data-level="2.10" data-path="lin-algs.html"><a href="lin-algs.html#iter-1-weight-0.7255664-bias-0.02804636-cost-86.42445"><i class="fa fa-check"></i><b>2.10</b> iter: 1 weight: 0.7255664 bias: 0.02804636 cost: 86.42445</a></li>
<li class="chapter" data-level="2.11" data-path="lin-algs.html"><a href="lin-algs.html#iter-10-weight-0.484637-bias-0.06879067-cost-42.72917"><i class="fa fa-check"></i><b>2.11</b> iter: 10 weight: 0.484637 bias: 0.06879067 cost: 42.72917</a></li>
<li class="chapter" data-level="2.12" data-path="lin-algs.html"><a href="lin-algs.html#iter-20-weight-0.4837035-bias-0.1219333-cost-42.44643"><i class="fa fa-check"></i><b>2.12</b> iter: 20 weight: 0.4837035 bias: 0.1219333 cost: 42.44643</a></li>
<li class="chapter" data-level="2.13" data-path="lin-algs.html"><a href="lin-algs.html#iter-30-weight-0.4820883-bias-0.1747496-cost-42.1673"><i class="fa fa-check"></i><b>2.13</b> iter: 30 weight: 0.4820883 bias: 0.1747496 cost: 42.1673</a><ul>
<li class="chapter" data-level="2.13.1" data-path="lin-algs.html"><a href="lin-algs.html#multiple-linear-regression-2"><i class="fa fa-check"></i><b>2.13.1</b> Multiple Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="lin-algs.html"><a href="lin-algs.html#company-tv-radio-newspaper-sales"><i class="fa fa-check"></i><b>2.14</b> company TV radio newspaper sales</a></li>
<li class="chapter" data-level="2.15" data-path="lin-algs.html"><a href="lin-algs.html#section-6"><i class="fa fa-check"></i><b>2.15</b> 1 1 230.1 37.8 69.2 22.1</a></li>
<li class="chapter" data-level="2.16" data-path="lin-algs.html"><a href="lin-algs.html#section-7"><i class="fa fa-check"></i><b>2.16</b> 2 2 44.5 39.3 45.1 10.4</a></li>
<li class="chapter" data-level="2.17" data-path="lin-algs.html"><a href="lin-algs.html#section-8"><i class="fa fa-check"></i><b>2.17</b> 3 3 17.2 45.9 69.3 9.3</a></li>
<li class="chapter" data-level="2.18" data-path="lin-algs.html"><a href="lin-algs.html#section-9"><i class="fa fa-check"></i><b>2.18</b> 4 4 151.5 41.3 58.5 18.5</a></li>
<li class="chapter" data-level="2.19" data-path="lin-algs.html"><a href="lin-algs.html#section-10"><i class="fa fa-check"></i><b>2.19</b> 5 5 180.8 10.8 58.4 12.9</a></li>
<li class="chapter" data-level="2.20" data-path="lin-algs.html"><a href="lin-algs.html#section-11"><i class="fa fa-check"></i><b>2.20</b> 6 6 8.7 48.9 75.0 7.2</a></li>
<li class="chapter" data-level="2.21" data-path="lin-algs.html"><a href="lin-algs.html#section-12"><i class="fa fa-check"></i><b>2.21</b> [,1]</a></li>
<li class="chapter" data-level="2.22" data-path="lin-algs.html"><a href="lin-algs.html#intercept-5.5184872"><i class="fa fa-check"></i><b>2.22</b> Intercept 5.5184872</a></li>
<li class="chapter" data-level="2.23" data-path="lin-algs.html"><a href="lin-algs.html#tv-0.5767349"><i class="fa fa-check"></i><b>2.23</b> TV 0.5767349</a></li>
<li class="chapter" data-level="2.24" data-path="lin-algs.html"><a href="lin-algs.html#radio-0.4366550"><i class="fa fa-check"></i><b>2.24</b> radio 0.4366550</a></li>
<li class="chapter" data-level="2.25" data-path="lin-algs.html"><a href="lin-algs.html#newspaper-0.1098191"><i class="fa fa-check"></i><b>2.25</b> newspaper 0.1098191</a><ul>
<li class="chapter" data-level="2.25.1" data-path="lin-algs.html"><a href="lin-algs.html#logistic-regression-2"><i class="fa fa-check"></i><b>2.25.1</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.26" data-path="lin-algs.html"><a href="lin-algs.html#studied-slept-passed"><i class="fa fa-check"></i><b>2.26</b> studied slept passed</a></li>
<li class="chapter" data-level="2.27" data-path="lin-algs.html"><a href="lin-algs.html#section-13"><i class="fa fa-check"></i><b>2.27</b> 1 4.855064 9.63996157 1</a></li>
<li class="chapter" data-level="2.28" data-path="lin-algs.html"><a href="lin-algs.html#section-14"><i class="fa fa-check"></i><b>2.28</b> 2 8.625440 0.05892653 0</a></li>
<li class="chapter" data-level="2.29" data-path="lin-algs.html"><a href="lin-algs.html#section-15"><i class="fa fa-check"></i><b>2.29</b> 3 3.828192 0.72319923 0</a></li>
<li class="chapter" data-level="2.30" data-path="lin-algs.html"><a href="lin-algs.html#section-16"><i class="fa fa-check"></i><b>2.30</b> 4 7.150955 3.89942042 1</a></li>
<li class="chapter" data-level="2.31" data-path="lin-algs.html"><a href="lin-algs.html#section-17"><i class="fa fa-check"></i><b>2.31</b> 5 6.477900 8.19818055 1</a></li>
<li class="chapter" data-level="2.32" data-path="lin-algs.html"><a href="lin-algs.html#section-18"><i class="fa fa-check"></i><b>2.32</b> 6 1.922270 1.33142727 0</a></li>
<li class="chapter" data-level="2.33" data-path="lin-algs.html"><a href="lin-algs.html#iter-1-cost-0.6582674"><i class="fa fa-check"></i><b>2.33</b> iter: 1 cost: 0.6582674</a></li>
<li class="chapter" data-level="2.34" data-path="lin-algs.html"><a href="lin-algs.html#iter-1000-cost-0.4469503"><i class="fa fa-check"></i><b>2.34</b> iter: 1000 cost: 0.4469503</a></li>
<li class="chapter" data-level="2.35" data-path="lin-algs.html"><a href="lin-algs.html#iter-2000-cost-0.3773433"><i class="fa fa-check"></i><b>2.35</b> iter: 2000 cost: 0.3773433</a></li>
<li class="chapter" data-level="2.36" data-path="lin-algs.html"><a href="lin-algs.html#iter-3000-cost-0.3408168"><i class="fa fa-check"></i><b>2.36</b> iter: 3000 cost: 0.3408168</a></li>
<li class="chapter" data-level="2.37" data-path="lin-algs.html"><a href="lin-algs.html#section-19"><i class="fa fa-check"></i><b>2.37</b> [,1]</a></li>
<li class="chapter" data-level="2.38" data-path="lin-algs.html"><a href="lin-algs.html#intercept--3.8281563"><i class="fa fa-check"></i><b>2.38</b> Intercept -3.8281563</a></li>
<li class="chapter" data-level="2.39" data-path="lin-algs.html"><a href="lin-algs.html#studied-0.4802045"><i class="fa fa-check"></i><b>2.39</b> studied 0.4802045</a></li>
<li class="chapter" data-level="2.40" data-path="lin-algs.html"><a href="lin-algs.html#slept-0.3435157"><i class="fa fa-check"></i><b>2.40</b> slept 0.3435157</a></li>
<li class="chapter" data-level="2.41" data-path="lin-algs.html"><a href="lin-algs.html#section-20"><i class="fa fa-check"></i><b>2.41</b> [1] 0.91</a></li>
<li class="chapter" data-level="2.42" data-path="lin-algs.html"><a href="lin-algs.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>2.42</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#lda-intuition"><i class="fa fa-check"></i>LDA Intuition</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#lda-estimates-for-one-predictor"><i class="fa fa-check"></i>LDA Estimates for One Predictor</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#lda-with-miltiple-predictors"><i class="fa fa-check"></i>LDA With Miltiple Predictors</a></li>
</ul></li>
<li class="chapter" data-level="2.43" data-path="lin-algs.html"><a href="lin-algs.html#practical-exercise"><i class="fa fa-check"></i><b>2.43</b> Practical Exercise</a><ul>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#data-3"><i class="fa fa-check"></i>Data</a></li>
</ul></li>
<li class="chapter" data-level="2.44" data-path="lin-algs.html"><a href="lin-algs.html#x-y"><i class="fa fa-check"></i><b>2.44</b> x y</a></li>
<li class="chapter" data-level="2.45" data-path="lin-algs.html"><a href="lin-algs.html#section-21"><i class="fa fa-check"></i><b>2.45</b> 1 4.667798 0</a></li>
<li class="chapter" data-level="2.46" data-path="lin-algs.html"><a href="lin-algs.html#section-22"><i class="fa fa-check"></i><b>2.46</b> 2 5.509199 0</a></li>
<li class="chapter" data-level="2.47" data-path="lin-algs.html"><a href="lin-algs.html#section-23"><i class="fa fa-check"></i><b>2.47</b> 3 4.702792 0</a></li>
<li class="chapter" data-level="2.48" data-path="lin-algs.html"><a href="lin-algs.html#section-24"><i class="fa fa-check"></i><b>2.48</b> 4 5.956707 0</a></li>
<li class="chapter" data-level="2.49" data-path="lin-algs.html"><a href="lin-algs.html#section-25"><i class="fa fa-check"></i><b>2.49</b> 5 5.738622 0</a></li>
<li class="chapter" data-level="2.50" data-path="lin-algs.html"><a href="lin-algs.html#section-26"><i class="fa fa-check"></i><b>2.50</b> 6 5.027283 0</a><ul>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#lda-scoring"><i class="fa fa-check"></i>LDA Scoring</a></li>
</ul></li>
<li class="chapter" data-level="2.51" data-path="lin-algs.html"><a href="lin-algs.html#section-27"><i class="fa fa-check"></i><b>2.51</b> [1] 0.5</a></li>
<li class="chapter" data-level="2.52" data-path="lin-algs.html"><a href="lin-algs.html#section-28"><i class="fa fa-check"></i><b>2.52</b> [1] 0.5</a></li>
<li class="chapter" data-level="2.53" data-path="lin-algs.html"><a href="lin-algs.html#section-29"><i class="fa fa-check"></i><b>2.53</b> [1] 4.975416</a></li>
<li class="chapter" data-level="2.54" data-path="lin-algs.html"><a href="lin-algs.html#section-30"><i class="fa fa-check"></i><b>2.54</b> [1] 20.08706</a></li>
<li class="chapter" data-level="2.55" data-path="lin-algs.html"><a href="lin-algs.html#section-31"><i class="fa fa-check"></i><b>2.55</b> [1] 10.15823</a></li>
<li class="chapter" data-level="2.56" data-path="lin-algs.html"><a href="lin-algs.html#section-32"><i class="fa fa-check"></i><b>2.56</b> [1] 21.49317</a></li>
<li class="chapter" data-level="2.57" data-path="lin-algs.html"><a href="lin-algs.html#section-33"><i class="fa fa-check"></i><b>2.57</b> [1] 0.8329315</a></li>
<li class="chapter" data-level="2.58" data-path="lin-algs.html"><a href="lin-algs.html#section-34"><i class="fa fa-check"></i><b>2.58</b> [1] 12.32936</a></li>
<li class="chapter" data-level="2.59" data-path="lin-algs.html"><a href="lin-algs.html#section-35"><i class="fa fa-check"></i><b>2.59</b> [1] -130.3349</a></li>
<li class="chapter" data-level="2.60" data-path="lin-algs.html"><a href="lin-algs.html#preds"><i class="fa fa-check"></i><b>2.60</b> preds</a></li>
<li class="chapter" data-level="2.61" data-path="lin-algs.html"><a href="lin-algs.html#section-36"><i class="fa fa-check"></i><b>2.61</b> 0 1</a></li>
<li class="chapter" data-level="2.62" data-path="lin-algs.html"><a href="lin-algs.html#section-37"><i class="fa fa-check"></i><b>2.62</b> 0 20 0</a></li>
<li class="chapter" data-level="2.63" data-path="lin-algs.html"><a href="lin-algs.html#section-38"><i class="fa fa-check"></i><b>2.63</b> 1 0 20</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="non-lin-algs.html"><a href="non-lin-algs.html"><i class="fa fa-check"></i><b>3</b> Non-linear Algorithms</a><ul>
<li class="chapter" data-level="3.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>3.1</b> Classification and Regression Trees (CART)</a><ul>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#what-are-cart-models"><i class="fa fa-check"></i>What are CART models?</a></li>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#how-does-a-cart-model-learn-from-data"><i class="fa fa-check"></i>How does a CART model learn from data?</a></li>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#pre-processing-requirements"><i class="fa fa-check"></i>Pre-processing requirements?</a></li>
<li class="chapter" data-level="3.1.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#practical-exerecise"><i class="fa fa-check"></i><b>3.1.1</b> Practical Exerecise</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="non-lin-algs.html"><a href="non-lin-algs.html#naive-bayes"><i class="fa fa-check"></i><b>3.2</b> Naive Bayes</a><ul>
<li class="chapter" data-level="3.2.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#practical-exerecise-1"><i class="fa fa-check"></i><b>3.2.1</b> Practical Exerecise</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="non-lin-algs.html"><a href="non-lin-algs.html#k-nearest-neigbors"><i class="fa fa-check"></i><b>3.3</b> k-Nearest Neigbors</a><ul>
<li class="chapter" data-level="3.3.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#practical-exerecise-2"><i class="fa fa-check"></i><b>3.3.1</b> Practical Exerecise</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="non-lin-algs.html"><a href="non-lin-algs.html#support-vector-machines"><i class="fa fa-check"></i><b>3.4</b> Support Vector Machines</a><ul>
<li class="chapter" data-level="3.4.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#practical-exerecise-3"><i class="fa fa-check"></i><b>3.4.1</b> Practical Exerecise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ens-algs.html"><a href="ens-algs.html"><i class="fa fa-check"></i><b>4</b> Ensemble Algorithms</a></li>
<li class="chapter" data-level="5" data-path="ml-pe.html"><a href="ml-pe.html"><i class="fa fa-check"></i><b>5</b> Machine Learning Practical Exercise</a></li>
<li class="chapter" data-level="6" data-path="shiny-tut.html"><a href="shiny-tut.html"><i class="fa fa-check"></i><b>6</b> R Shiny Tutorial</a></li>
<li class="chapter" data-level="7" data-path="shiny-pe.html"><a href="shiny-pe.html"><i class="fa fa-check"></i><b>7</b> R Shiny Practical Exercise</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Practical Guide for Machine Learning and R Shiny</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="non-lin-algs" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Non-linear Algorithms</h1>
<p>We now focus our attention to non-linear machine learning algorithms. As we learn about these algorithms you should notice that many of these are an extension of the linear algorithms we learned in Chapter <a href="lin-algs.html#lin-algs">2</a>.</p>
<div id="classification-and-regression-trees-cart" class="section level2">
<h2><span class="header-section-number">3.1</span> Classification and Regression Trees (CART)</h2>
<p>Our first algorithm we will examine is the CART algorithm. This algoritm is important as it forms the basis for ensemble algorithms such as Random Forests and Bagged Decision Trees which we will learn in Chaper <a href="ens-algs.html#ens-algs">4</a>. CART models are also used for both regression and classification problems.</p>
<div id="what-are-cart-models" class="section level3 unnumbered">
<h3>What are CART models?</h3>
<p>CART models are simply decision trees. That is to say, the CART algorithm searches for points in the data to split the data into rectangular sections that increase the prediction accuracy. The more splits that are made within the data produces smaller and smaller segments up until a designated stopping point to prevent overfitting. A simple example will illustate the intuition behind CART. Figure <a href="non-lin-algs.html#fig:cart-example">3.1</a> demonstrates a simple CART model. Reviewing this output we can see the definition of the model being</p>
<pre><code>if Predictor A &gt;= 1.7 then
   if Predictor B &gt;= 202.1 the Outcome = 1.3
   else Outcome = 5.6
else Outcome = 2.5</code></pre>
<div class="figure" style="text-align: center"><span id="fig:cart-example"></span>
<img src="img/cart-example.png" alt="Example output and decision tree model adapted from Kuhn and Johnson (2013)." width="90%" />
<p class="caption">
Figure 3.1: Example output and decision tree model adapted from Kuhn and Johnson (2013).
</p>
</div>
<p>Using the above decision algorithm, we can make future predictions based of the split values of Predictor A and B.</p>
</div>
<div id="how-does-a-cart-model-learn-from-data" class="section level3 unnumbered">
<h3>How does a CART model learn from data?</h3>
<div id="regression-trees" class="section level4 unnumbered">
<h4>Regression Trees</h4>
<p>For regresdsion trees CART models search through all the data points for each predictor to determine the optimal split point that partitions the data into two groups and the sum of squred errors (SSE) is the lowest possible value for that split. In the previous example, that value was 1.7 for Predictor A. From that first split, the method is repeated within each new region until the model reaches a designated stopping point, for instance <span class="math inline">\(n &lt; 20\)</span> data points in any new region.</p>
<p><span class="math display">\[
SSE\ =\ \sum_{i\in S_1}^{ }\left(y_i-\overline{y_1}\right)^2\ +\ \sum_{i\in S_2}^{ }\left(y_i-\overline{y_2}\right)^2
\]</span></p>
</div>
<div id="classification-trees" class="section level4 unnumbered">
<h4>Classification Trees</h4>
<p>A frequently used measure for classification trees is the GINI index and is computed by</p>
<p><span class="math display">\[
G\ =\ \sum_{k=1}^np_k\times\left(1-p_k\right)
\]</span></p>
<p>where <span class="math inline">\(p_k\)</span> is the classification probability of the <span class="math inline">\(k\)</span>th class. Using a process similar to the regression method, the algorithms searches for the best split point based on the lowest Gini index indicating the purest node for that split. In this case, purity refers to a node having more of one particular class than another.</p>
</div>
<div id="two-class-example" class="section level4 unnumbered">
<h4>Two-class Example</h4>
<p>To illustrate how to compute the Gini indext, we will walk through a simple two-class example. The first step is sort the sample based on the predictor values and then find the midpoint of the optimal split point. This would create a contengincy table like the one below. For this table <span class="math inline">\(n_{11}\)</span> is the proportion of sample observations that are in group 1(samples that are greater than the split value) class 1. The same logic follows for the other three split values. The bold faced values are the sub-totals of the split groups and the classifications.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">Class1</th>
<th align="center">Class2</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(&gt;\)</span> split</td>
<td align="center"><span class="math inline">\(n_{11}\)</span></td>
<td align="center"><span class="math inline">\(n_{12}\)</span></td>
<td><span class="math inline">\(\mathbf{n_{&gt;split}}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\leq\)</span> split</td>
<td align="center"><span class="math inline">\(n_{21}\)</span></td>
<td align="center"><span class="math inline">\(n_{22}\)</span></td>
<td><span class="math inline">\(\mathbf{n_{\leq split}}\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td align="center"><span class="math inline">\(\mathbf{n_{class1}}\)</span></td>
<td align="center"><span class="math inline">\(\mathbf{n_{class2}}\)</span></td>
<td><span class="math inline">\(\mathbf{n}\)</span></td>
</tr>
</tbody>
</table>
<p>Before the split, the initial Gini index is</p>
<p><span class="math display">\[G = 2\left(\frac{n_{class1}}{n}\right)\left(\frac{n_{class2}}{n}\right)\]</span>.</p>
<p>After the split the Gini index changes to</p>
<p><span class="math display">\[
\begin{align}
G &amp;=\ 2\left[\left(\frac{n_{11}}{n_{&gt;split}}\right)\left(\frac{n_{12}}{n_{&gt;split}}\right)\left(\frac{n_{&gt;split}}{n}\right)\ +\ \left(\frac{n_{21}}{n_{\leq split}}\right)\left(\frac{n_{22}}{n_{\leq split}}\right)\left(\frac{n_{\leq split}}{n}\right)\right]\\
&amp;=\ 2\left[\left(\frac{n_{11}}{n}\right)\left(\frac{n_{12}}{n_{&gt;split}}\right)\ +\ \left(\frac{n_{21}}{n}\right)\left(\frac{n_{22}}{n_{\leq split}}\right)\right]
\end{align}
\]</span></p>
<p>We can see from the above equation we see that the Gini index now depends upon the proportion of samples of each class within a region that is weighted by the proportion of sample points in each split group. This new value is compared to the previous value and if the new value is smaller, the split is made, and ignored otherwise.</p>
<p>We will now work through an example problem. Figure <a href="non-lin-algs.html#fig:gini-example">3.2</a> shows the results of predicted classes with regions for a two-class model. There are a total of 208 observations: 111 observations for Class 1 and 97 observations for Class 2. Using this information, we can compute the Gini index before any splits.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n_obs &lt;-<span class="st"> </span><span class="dv">208</span>
n_class_one &lt;-<span class="st"> </span><span class="dv">111</span>
n_class_two &lt;-<span class="st"> </span><span class="dv">97</span>
gini_before &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(n_class_one<span class="op">/</span>n_obs) <span class="op">*</span><span class="st"> </span>(n_class_two<span class="op">/</span>n_obs)</code></pre></div>
<p>Based on the above calculation the pre-split Gini index is 0.498.</p>
<div class="figure" style="text-align: center"><span id="fig:gini-example"></span>
<img src="img/applied-pred-Ch14Fig01.png" alt="Example classification model results." width="90%" />
<p class="caption">
Figure 3.2: Example classification model results.
</p>
</div>
<p>The contingency table for Predictor B of the above figure is below. Using this information we can compute the post-split Gini index</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">Class1</th>
<th align="center">Class2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(B &gt; 0.197\)</span></td>
<td align="center">91</td>
<td align="center">30</td>
</tr>
<tr class="even">
<td><span class="math inline">\(B \leq 0.197\)</span></td>
<td align="center">20</td>
<td align="center">67</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n11 &lt;-<span class="st"> </span><span class="dv">91</span>; n12 &lt;-<span class="st"> </span><span class="dv">30</span>; n21 &lt;-<span class="st"> </span><span class="dv">20</span>; n22 &lt;-<span class="st"> </span><span class="dv">67</span>;
n_group_one &lt;-<span class="st"> </span><span class="dv">121</span>; n_group_two &lt;-<span class="st"> </span><span class="dv">87</span>;
group_one_prop &lt;-<span class="st"> </span>(n11<span class="op">/</span>n_obs)<span class="op">*</span>(n12<span class="op">/</span>n_group_one)
group_two_prop &lt;-<span class="st"> </span>(n21<span class="op">/</span>n_obs)<span class="op">*</span>(n22<span class="op">/</span>n_group_two)
gini_after &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(group_one_prop,group_two_prop)</code></pre></div>
<p>The final Gini index post-split is now 0.365 which indicates an improvement in classification purity. We can also observe that any value <span class="math inline">\(\leq 0.197\)</span> will receive a classification of 2 and a classification of 1 otherwise with regards to this particular split point.</p>
</div>
</div>
<div id="pre-processing-requirements" class="section level3 unnumbered">
<h3>Pre-processing requirements?</h3>
<p>CART models do not require any special pre-processing of the data, but you can center and scale values based on skewness and other factors.</p>
</div>
<div id="practical-exerecise" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Practical Exerecise</h3>
<div id="libraries" class="section level4 unnumbered">
<h4>Libraries</h4>
<p>This exercise will use the <code>AppliedPredictiveModeling</code>, <code>rpart</code>, and <code>caret</code>, packages.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(AppliedPredictiveModeling)
<span class="kw">library</span>(rpart)
<span class="kw">library</span>(caret)
<span class="kw">library</span>(partykit)</code></pre></div>
</div>
<div id="data-4" class="section level4 unnumbered">
<h4>Data</h4>
<p>For this exercise, we will use the solubility data set described in <span class="citation">Kuhn and Johnson (<a href="#ref-kuhn2013applied">2013</a>)</span>. In short the features for this data set are:</p>
<ul>
<li>208 binary “fingerprints” that indicate the presence or absence of a particular chemical sub-structure;</li>
<li>16 count descriptors (such as the number of bonds or the number of Bromine atoms);</li>
<li>4 continuous descriptors (such as molecular weight or surface area) <span class="citation">(Kuhn and Johnson <a href="#ref-kuhn2014package">2014</a>)</span>.</li>
</ul>
<p>The authors centered and scaled the data to account for skewness. The target variable is a vector of log10 solubility values. The goal of this exercise is to predict the solubility value based on the set of features. Below is a view of some of the features and target values</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(solubility)
<span class="kw">str</span>(solTrainXtrans[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,<span class="dv">209</span><span class="op">:</span><span class="dv">228</span>)])</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    951 obs. of  30 variables:
##  $ FP001            : int  0 0 1 0 0 1 0 1 1 1 ...
##  $ FP002            : int  1 1 1 0 0 0 1 0 0 1 ...
##  $ FP003            : int  0 0 1 1 1 1 0 1 1 1 ...
##  $ FP004            : int  0 1 1 0 1 1 1 1 1 1 ...
##  $ FP005            : int  1 1 1 0 1 0 1 0 0 1 ...
##  $ FP006            : int  0 1 0 0 1 0 0 0 1 1 ...
##  $ FP007            : int  0 1 0 1 0 0 0 1 1 1 ...
##  $ FP008            : int  1 1 1 0 0 0 1 0 0 0 ...
##  $ FP009            : int  0 0 0 0 1 1 1 0 1 0 ...
##  $ FP010            : int  0 0 1 0 0 0 0 0 0 0 ...
##  $ MolWeight        : num  5.34 5.9 5.33 4.92 5.44 ...
##  $ NumAtoms         : num  3.37 3.91 3.53 3.3 3.47 ...
##  $ NumNonHAtoms     : num  2.83 3.3 2.77 2.4 2.77 ...
##  $ NumBonds         : num  3.43 3.97 3.53 3.3 3.47 ...
##  $ NumNonHBonds     : num  4.01 4.87 3.71 3.08 3.71 ...
##  $ NumMultBonds     : num  5.26 4.68 3.24 1.38 2.94 ...
##  $ NumRotBonds      : num  0 1.609 1.609 0.693 1.792 ...
##  $ NumDblBonds      : num  0 0 0.567 0.805 0 ...
##  $ NumAromaticBonds : num  2.83 2.56 1.95 0 1.95 ...
##  $ NumHydrogen      : num  3.86 5.32 4.73 4.47 4.47 ...
##  $ NumCarbon        : num  4.18 5.09 4.02 3.51 3.32 ...
##  $ NumNitrogen      : num  0.585 0.642 0 0 0.694 ...
##  $ NumOxygen        : num  0 0.693 1.099 0 0 ...
##  $ NumSulfer        : num  0 0.375 0 0 0 0.375 0 0 0 0 ...
##  $ NumChlorine      : num  0 0 0 0 0.375 ...
##  $ NumHalogen       : num  0 0 0 0 0.375 ...
##  $ NumRings         : num  1.386 1.609 0.693 0.693 0.693 ...
##  $ HydrophilicFactor: num  -1.607 -0.441 -0.385 -2.373 -0.071 ...
##  $ SurfaceArea1     : num  6.81 9.75 8.25 0 9.91 ...
##  $ SurfaceArea2     : num  6.81 12.03 8.25 0 9.91 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(solTrainY)</code></pre></div>
<pre><code>##  num [1:951] -3.97 -3.98 -3.99 -4 -4.06 -4.08 -4.08 -4.1 -4.1 -4.11 ...</code></pre>
<p>The <code>rpart()</code> function in <code>R</code> is a widely used method for computing trees using CART, and we will use this function. Another package <code>party</code> uses the <a href="https://stats.stackexchange.com/questions/12140/conditional-inference-trees-vs-traditional-decision-trees">conditional inference framework</a> to form its trees.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">100</span>)
rpartTune &lt;-<span class="st"> </span><span class="kw">train</span>(solTrainXtrans, solTrainY,
                   <span class="dt">method =</span> <span class="st">&quot;rpart2&quot;</span>,
                   <span class="dt">tuneLength =</span> <span class="dv">10</span>,
                   <span class="dt">trControl=</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>)
                   )
rpartTune<span class="op">$</span>results</code></pre></div>
<pre><code>##    maxdepth     RMSE  Rsquared       MAE     RMSESD RsquaredSD      MAESD
## 1         1 1.617667 0.3745252 1.2657915 0.11511437 0.05777279 0.08191460
## 2         2 1.433114 0.5067404 1.1326186 0.07599686 0.04909341 0.04940391
## 3         3 1.357672 0.5568291 1.0657348 0.07354389 0.05231774 0.06091190
## 4         4 1.263596 0.6166997 0.9974476 0.10201869 0.05547696 0.07947602
## 5         5 1.192831 0.6581800 0.9429124 0.11324197 0.05669830 0.08594278
## 6         6 1.142654 0.6853056 0.9009065 0.10585813 0.05990671 0.08607556
## 7         7 1.111858 0.7020728 0.8707216 0.10580483 0.06389863 0.08126706
## 8         8 1.094535 0.7110088 0.8545809 0.11400541 0.06474333 0.09512021
## 9         9 1.091880 0.7116190 0.8465921 0.11938842 0.06737339 0.10068304
## 10       10 1.068799 0.7236716 0.8232469 0.12842861 0.07102897 0.10641491</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#To match the figure in the Applied Predictive Modeling book, we will choose a max depth of 5.</span>

training_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">cbind</span>(solTrainXtrans,solTrainY))
training_model &lt;-<span class="st"> </span><span class="kw">rpart</span>(solTrainY <span class="op">~</span>., <span class="dt">data =</span> training_data,
                    <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">maxdepth =</span> <span class="dv">5</span>))
model_tree &lt;-<span class="st"> </span><span class="kw">as.party</span>(training_model)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:cart-plot"></span>
<img src="img/final-cart-plot.png" alt="Final CART model regression results." width="90%" />
<p class="caption">
Figure 3.3: Final CART model regression results.
</p>
</div>
</div>
</div>
</div>
<div id="naive-bayes" class="section level2">
<h2><span class="header-section-number">3.2</span> Naive Bayes</h2>
<div id="practical-exerecise-1" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Practical Exerecise</h3>
</div>
</div>
<div id="k-nearest-neigbors" class="section level2">
<h2><span class="header-section-number">3.3</span> k-Nearest Neigbors</h2>
<div id="practical-exerecise-2" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Practical Exerecise</h3>
</div>
</div>
<div id="support-vector-machines" class="section level2">
<h2><span class="header-section-number">3.4</span> Support Vector Machines</h2>
<div id="practical-exerecise-3" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Practical Exerecise</h3>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-kuhn2013applied">
<p>Kuhn, Max, and Kjell Johnson. 2013. <em>Applied Predictive Modeling</em>. Vol. 26. Springer.</p>
</div>
<div id="ref-kuhn2014package">
<p>Kuhn, Max, and Kjell Johnson. 2014. <em>AppliedPredictiveModeling: Functions and Data Sets for ’Applied Predictive Modeling’</em>. <a href="https://CRAN.R-project.org/package=AppliedPredictiveModeling" class="uri">https://CRAN.R-project.org/package=AppliedPredictiveModeling</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lin-algs.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ens-algs.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
