<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Practical Guide for Machine Learning and R Shiny</title>
  <meta name="description" content="Everything you need (and nothing more) to start a bookdown book.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="A Practical Guide for Machine Learning and R Shiny" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="<a href="https://cmoten.github.io/machine-learning-course-ALU" class="uri">https://cmoten.github.io/machine-learning-course-ALU</a>" />
  
  <meta property="og:description" content="Everything you need (and nothing more) to start a bookdown book." />
  <meta name="github-repo" content="cmoten/machine-learning-course-ALU" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Practical Guide for Machine Learning and R Shiny" />
  
  <meta name="twitter:description" content="Everything you need (and nothing more) to start a bookdown book." />
  

<meta name="author" content="Cardy Moten III">


<meta name="date" content="2018-03-07">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ml-overview.html">
<link rel="next" href="non-lin-algs.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning & R Shiny</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ml-overview.html"><a href="ml-overview.html"><i class="fa fa-check"></i><b>1</b> Machine Learning Overview</a><ul>
<li class="chapter" data-level="1.1" data-path="ml-overview.html"><a href="ml-overview.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="ml-overview.html"><a href="ml-overview.html#what-machine-learning-is-not"><i class="fa fa-check"></i><b>1.2</b> What machine learning is not</a><ul>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-models-require-a-lot-of-data"><i class="fa fa-check"></i>Machine learning models require a lot of data</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-models-are-not-a-substitute-for-domain-expertise"><i class="fa fa-check"></i>Machine learning models are not a substitute for domain expertise</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-models-require-constant-maintenance"><i class="fa fa-check"></i>Machine learning models require constant maintenance</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ml-overview.html"><a href="ml-overview.html#what-do-i-need-to-know-to-get-started-with-machine-learning"><i class="fa fa-check"></i><b>1.3</b> What do I need to know to get started with machine learning?</a><ul>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#the-role-of-data-in-machine-learning"><i class="fa fa-check"></i>The role of data in machine learning</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#the-difference-between-parametric-and-nonparametric-machine-learning-algorithms"><i class="fa fa-check"></i>The difference between parametric and nonparametric machine learning algorithms</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#the-difference-between-supervised-unsupervised-and-semi-supervised-learning"><i class="fa fa-check"></i>The difference between supervised, unsupervised and semi-supervised learning</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#the-bias-variance-tradeoff"><i class="fa fa-check"></i>The bias-variance tradeoff</a></li>
<li class="chapter" data-level="" data-path="ml-overview.html"><a href="ml-overview.html#overfitting-and-underfitting-and-what-to-do-about-it"><i class="fa fa-check"></i>Overfitting and underfitting and what to do about it</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lin-algs.html"><a href="lin-algs.html"><i class="fa fa-check"></i><b>2</b> Linear Algorithms</a><ul>
<li class="chapter" data-level="2.1" data-path="lin-algs.html"><a href="lin-algs.html#gradient-descent"><i class="fa fa-check"></i><b>2.1</b> Gradient Descent</a><ul>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#linear-regression-model"><i class="fa fa-check"></i>Linear Regression Model</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#cost-function"><i class="fa fa-check"></i>Cost Function</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#logistic-regression"><i class="fa fa-check"></i>Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#gradient-descent-algorithm"><i class="fa fa-check"></i>Gradient Descent Algorithm</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#gradient-descent-intuition"><i class="fa fa-check"></i>Gradient Descent Intuition</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="lin-algs.html"><a href="lin-algs.html#practical-exercises"><i class="fa fa-check"></i><b>2.2</b> Practical Exercises</a><ul>
<li class="chapter" data-level="2.2.1" data-path="lin-algs.html"><a href="lin-algs.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>2.2.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="2.2.2" data-path="lin-algs.html"><a href="lin-algs.html#multiple-linear-regression-1"><i class="fa fa-check"></i><b>2.2.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="2.2.3" data-path="lin-algs.html"><a href="lin-algs.html#logistic-regression-1"><i class="fa fa-check"></i><b>2.2.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="lin-algs.html"><a href="lin-algs.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>2.3</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#lda-intuition"><i class="fa fa-check"></i>LDA Intuition</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#lda-estimates-for-one-predictor"><i class="fa fa-check"></i>LDA Estimates for One Predictor</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#lda-with-miltiple-predictors"><i class="fa fa-check"></i>LDA With Miltiple Predictors</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="lin-algs.html"><a href="lin-algs.html#practical-exercise"><i class="fa fa-check"></i><b>2.4</b> Practical Exercise</a><ul>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#data-2"><i class="fa fa-check"></i>Data</a></li>
<li class="chapter" data-level="" data-path="lin-algs.html"><a href="lin-algs.html#lda-scoring"><i class="fa fa-check"></i>LDA Scoring</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="non-lin-algs.html"><a href="non-lin-algs.html"><i class="fa fa-check"></i><b>3</b> Non-linear Algorithms</a><ul>
<li class="chapter" data-level="3.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>3.1</b> Classification and Regression Trees (CART)</a><ul>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#what-are-cart-models"><i class="fa fa-check"></i>What are CART models?</a></li>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#how-does-a-cart-model-learn-from-data"><i class="fa fa-check"></i>How does a CART model learn from data?</a></li>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#pre-processing-requirements"><i class="fa fa-check"></i>Pre-processing requirements?</a></li>
<li class="chapter" data-level="3.1.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#practical-exerecise"><i class="fa fa-check"></i><b>3.1.1</b> Practical Exerecise</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="non-lin-algs.html"><a href="non-lin-algs.html#naive-bayes"><i class="fa fa-check"></i><b>3.2</b> Naive Bayes</a><ul>
<li class="chapter" data-level="3.2.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#practical-exerecise-1"><i class="fa fa-check"></i><b>3.2.1</b> Practical Exerecise</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="non-lin-algs.html"><a href="non-lin-algs.html#k-nearest-neigbors"><i class="fa fa-check"></i><b>3.3</b> k-Nearest Neigbors</a><ul>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#curse-of-dimensionality"><i class="fa fa-check"></i>Curse of Dimensionality</a></li>
<li class="chapter" data-level="3.3.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#practical-exerecise-2"><i class="fa fa-check"></i><b>3.3.1</b> Practical Exerecise</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="non-lin-algs.html"><a href="non-lin-algs.html#support-vector-machines"><i class="fa fa-check"></i><b>3.4</b> Support Vector Machines</a><ul>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#regression-1"><i class="fa fa-check"></i>Regression</a></li>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#classification-1"><i class="fa fa-check"></i>Classification</a></li>
<li class="chapter" data-level="" data-path="non-lin-algs.html"><a href="non-lin-algs.html#optimizaton-forumulation"><i class="fa fa-check"></i>Optimizaton Forumulation</a></li>
<li class="chapter" data-level="3.4.1" data-path="non-lin-algs.html"><a href="non-lin-algs.html#practical-exerecise-3"><i class="fa fa-check"></i><b>3.4.1</b> Practical Exerecise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ens-algs.html"><a href="ens-algs.html"><i class="fa fa-check"></i><b>4</b> Ensemble Algorithms</a><ul>
<li class="chapter" data-level="4.1" data-path="ens-algs.html"><a href="ens-algs.html#bagging"><i class="fa fa-check"></i><b>4.1</b> Bagging</a><ul>
<li class="chapter" data-level="" data-path="ens-algs.html"><a href="ens-algs.html#practical-exercise-1"><i class="fa fa-check"></i>Practical Exercise</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ens-algs.html"><a href="ens-algs.html#random-forest"><i class="fa fa-check"></i><b>4.2</b> Random Forest</a><ul>
<li class="chapter" data-level="" data-path="ens-algs.html"><a href="ens-algs.html#practical-exercise-2"><i class="fa fa-check"></i>Practical Exercise</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ens-algs.html"><a href="ens-algs.html#adaboost"><i class="fa fa-check"></i><b>4.3</b> AdaBoost</a><ul>
<li class="chapter" data-level="" data-path="ens-algs.html"><a href="ens-algs.html#practical-exercise-3"><i class="fa fa-check"></i>Practical Exercise</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ens-algs.html"><a href="ens-algs.html#gradient-boosting"><i class="fa fa-check"></i><b>4.4</b> Gradient Boosting</a><ul>
<li class="chapter" data-level="" data-path="ens-algs.html"><a href="ens-algs.html#practical-exercise-4"><i class="fa fa-check"></i>Practical Exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ml-pe.html"><a href="ml-pe.html"><i class="fa fa-check"></i><b>5</b> Machine Learning Practical Exercise</a><ul>
<li class="chapter" data-level="5.1" data-path="ml-pe.html"><a href="ml-pe.html#modeling-workflow"><i class="fa fa-check"></i><b>5.1</b> Modeling Workflow</a></li>
<li class="chapter" data-level="5.2" data-path="ml-pe.html"><a href="ml-pe.html#performance-metrics"><i class="fa fa-check"></i><b>5.2</b> Performance Metrics</a></li>
<li class="chapter" data-level="5.3" data-path="ml-pe.html"><a href="ml-pe.html#practice-projects"><i class="fa fa-check"></i><b>5.3</b> Practice Projects</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="shiny-tut.html"><a href="shiny-tut.html"><i class="fa fa-check"></i><b>6</b> R Shiny Tutorial</a><ul>
<li class="chapter" data-level="6.1" data-path="shiny-tut.html"><a href="shiny-tut.html#what-we-will-learn"><i class="fa fa-check"></i><b>6.1</b> What we will learn</a></li>
<li class="chapter" data-level="6.2" data-path="shiny-tut.html"><a href="shiny-tut.html#shiny-basics"><i class="fa fa-check"></i><b>6.2</b> Shiny Basics</a></li>
<li class="chapter" data-level="6.3" data-path="shiny-tut.html"><a href="shiny-tut.html#design-principles"><i class="fa fa-check"></i><b>6.3</b> Design Principles</a></li>
<li class="chapter" data-level="" data-path="shiny-tut.html"><a href="shiny-tut.html#practical-exerecise-4"><i class="fa fa-check"></i>Practical Exerecise</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Practical Guide for Machine Learning and R Shiny</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lin-algs" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Linear Algorithms</h1>
<div id="gradient-descent" class="section level2">
<h2><span class="header-section-number">2.1</span> Gradient Descent</h2>
<p>One of the most common concepts for all machine learning algorithms is optimization. Of the many optimization methods, the most widely used optimization method in machine learning is gradient descent. This extensive use of gradient descent is because gradient descent is straightforward to learn and compatible with any machine learning algorithm. For this section, we will use gradient descent with linear and logistic regression.</p>
<div id="linear-regression-model" class="section level3 unnumbered">
<h3>Linear Regression Model</h3>
<p>Before discussing the gradient descent algorithm, let’s review the linear regression. Recall the general linear regression equation is:</p>
<p><span class="math display">\[
y\ =\ h\left(x\right)\ =\ \sum_{i=1}^Nw_ix_i\ +\ b\ +\ \epsilon
\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the target variable, <span class="math inline">\(x\)</span> is the feature variables, <span class="math inline">\(w_i\)</span> are the weights of the <span class="math inline">\(i\)</span>th feature variable, <span class="math inline">\(b\)</span> is the bias, and <span class="math inline">\(\epsilon\)</span> is the irreducible error. We use machine learning algorithms to estimate the bias and the weights of the feature variables.</p>
</div>
<div id="cost-function" class="section level3 unnumbered">
<h3>Cost Function</h3>
<div id="simple-linear-regression" class="section level4 unnumbered">
<h4>Simple Linear Regression</h4>
<p>In order to optimize the weight and the bias variable, we need to optimize the cost (loss) function. For linear regression this function, <span class="math inline">\(J\left(w,b\right)\)</span> is the Mean Squared Error (MSE) and we can calculate it by:</p>
<p><span class="math display">\[
MSE\ =\ J\left(w,b\right)\ =\ \frac{1}{m}\sum_{i=1}^m\left(y_i-\left(wx_i+b\right)\right)^2
\]</span></p>
<p>Since we are adjusting the cost function by the weight and the bias parameters, we must take the partial derivative with respect to each of these to calculate the gradient.</p>
<p><span class="math display">\[
J&#39;\left(w,b\right)\ =
\begin{bmatrix}
\frac{\partial J}{\partial w}\\
\frac{\partial J}{\partial b}
\end{bmatrix}
=
\begin{bmatrix}
\frac{1}{m}\sum-2\cdot x_i\left(y_i-\left(wx_i+b\right)\right)\ =\ \delta_w\\
\frac{1}{m}\sum_{ }^{ }-2\cdot\left(y_i-\left(wx_i+b\right)\right)\ =\ \delta_b
\end{bmatrix}
\]</span></p>
</div>
<div id="multiple-linear-regression" class="section level4 unnumbered">
<h4>Multiple Linear Regression</h4>
<p>For multiple linear regression, we introduce a parameter matrix <span class="math inline">\(\theta\)</span> that contains the bias and weight parameters where:</p>
<p><span class="math display">\[
\begin{align}
h_{\theta}\left(x\right) &amp;=\theta_0\ +\ \theta_1x_1\ +\ \dots\ +\ \theta_nx_n\\
h\left(x\right)\ &amp;=\ \sum_{i=0}^n\theta_ix_i\ =\ \theta^Tx
\end{align}
\]</span>The cost function is:</p>
<p><span class="math display">\[
J\left(\theta\right)\ =\ \frac{1}{2m}\sum_{i=1}^m\left(h_{\theta}(x^{\left(i\right)})-y^{\left(i\right)}\right)^2
\]</span></p>
<p>In matrix form, the cost function becomes</p>
<p><span class="math display">\[
J\left(\theta\right)\ =\ \frac{1}{2m}\left(X\theta\ -\ y\right)^T\left(X\theta-y\right)
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is a <span class="math inline">\(m\ \times\ n\)</span> design matrix, <span class="math inline">\(\theta\)</span> is a <span class="math inline">\(n\ \times\ 1\)</span> parameter matrix and <span class="math inline">\(y\)</span> is a <span class="math inline">\(m\ \times\ 1\)</span> vector of observed targets.</p>
<p>Taking the derivative with respect to <span class="math inline">\(\theta\)</span> yields:</p>
<p><span class="math display">\[
J&#39;\left(\theta\right)\ =\ \frac{1}{m}X^T\left(X\theta-y\right)
\]</span></p>
<p>See the video below for an example derivation of the derivative of the cost matrix:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/tGkMr57ZvAk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<div id="logistic-regression" class="section level3 unnumbered">
<h3>Logistic Regression</h3>
<p>Another application of the gradient descent algorithm is for logistic regression. Recall that we use logistic regression when the target variable is categorical, and there are only two possible classifications. We show the general equation as</p>
<p><span class="math display">\[
h_{\theta}\left(x\right)\ =\ g\left(\theta^Tx\right)\ =\ \frac{1}{1+e^{-\theta^Tx}}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
g\left(z\right)\ =\ \frac{1}{1+e^{-z}}
\]</span></p>
<p>The above equation is called a sigmoid or logistic function. Essentially, we first perform a linear regression on the weights and bias and then feed that predicted value into the sigmoid function to map a real value between 0 and 1.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sigmoid &lt;-<span class="st"> </span><span class="cf">function</span>(z){
  res &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>z))
  res
}</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:sigmoid-plot"></span>
<img src="02-Linear-Algorithms_files/figure-html/sigmoid-plot-1.png" alt="Example Sigmoid function plot." width="90%" />
<p class="caption">
Figure 2.1: Example Sigmoid function plot.
</p>
</div>
<p>The cost function for logistic regression will differ now, that the function we are analyzing is non-linear. First let’s assume the following:</p>
<p><span class="math display">\[
\begin{align}
P\left(y\ =\ 1|x;\theta\right)\ &amp;=\ h_{\theta}\left(x\right)\\
P\left(y\ =\ 0\ |x;\theta\right)\ &amp;=\ 1-h_{\theta}\left(x\right)\\
P\left(y|x;\theta\right)\ &amp;=\ \left(h_{\theta}\left(x\right)\right)^y\left(1-h_{\theta}\left(x\right)\right)^{1-y}
\end{align}
\]</span></p>
<p>We now can find the log cross-entropy cost by:</p>
<p><span class="math display">\[
J\left(\theta\right)\ =\ -\frac{1}{m}\sum_{i=0}^m\left[y^{\left(i\right)}\log\left(h_{\theta}(x^{\left(i\right)})\right)\ +\ \left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}(x^{\left(i\right)})\right)\right]
\]</span></p>
<p>where <span class="math inline">\(h_\theta(x)\)</span> is the sigmoid function.</p>
<p>When taking the derivative with respect to <span class="math inline">\(\theta\)</span>, recall that <span class="math inline">\(g&#39;\left(z\right)\ =\ g\left(z\right)\left(1-g\left(z\right)\right)\)</span>. Thus,</p>
<p><span class="math display">\[
\begin{align}
J&#39;\left(\theta\right)\ &amp;=\ \frac{\partial}{\partial J}-\frac{1}{m}\sum_{i=0}^m\left[y^{\left(i\right)}\log\left(h_{\theta}(x^{\left(i\right)})\right)\ +\ \left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}(x^{\left(i\right)})\right)\right]\\
&amp;=-\frac{1}{m}\left(y\frac{1}{g\left(\theta^Tx\right)}-\left(1-y\right)\frac{1}{1-g\left(\theta^Tx\right)}\right)\frac{\partial}{\partial\theta_j}g\left(\theta^Tx\right)\\
&amp;=-\frac{1}{m}\left(y\frac{1}{g\left(\theta^Tx\right)}-\left(1-y\right)\frac{1}{1-g\left(\theta^Tx\right)}\right)g\left(\theta^Tx\right)\left(1-g\left(\theta^Tx\right)\right)\frac{\partial}{\partial\theta_j}\theta^Tx\\
&amp;=-\frac{1}{m}\left(y\left(1-g\left(\theta^Tx\right)\right)-\left(1-y\right)g\left(\theta^Tx\right)\right)x_j\\
&amp;=-\frac{1}{m}\left(y-g\left(\theta^Tx\right)\right)x_j\\
&amp;=\frac{1}{m}\left(h_{\theta}\left(x\right)-y\right)x_j\\
\end{align}
\]</span></p>
<p>What is interesting to note is that this gradient function looks precisely like the gradient function for linear regression. The difference, however, is that the function <span class="math inline">\(h_\theta(x)\)</span> is a sigmoid function and not a linear function of the weights and bias parameters. For further details of the above derivations see <span class="citation">Ng (<a href="#ref-ng2000cs229">2000</a>)</span> and <span class="citation">Fortuner (<a href="#ref-fortuner2017mlcheat">2017</a>)</span>.</p>
</div>
<div id="gradient-descent-algorithm" class="section level3 unnumbered">
<h3>Gradient Descent Algorithm</h3>
<p>Finally, to solve for the optimal weight and bias, we will add a learning parameter, <span class="math inline">\(\alpha\)</span>, to adjust the steps of the gradient.</p>
<p><strong>Simple Linear Regression</strong></p>
<p>The algorithm we will use is:</p>
<p><span class="math display">\[
\text{Repeat until convergence } \{\\
w\ :=\ w-\alpha\delta_w\\
b\ :=\ b\ -\ \alpha\delta_b\\
\}
\]</span></p>
<p><strong>Multiple Linear Regression</strong></p>
<p>For multiple linear regression the algorithm changes to:</p>
<p><span class="math display">\[
\text{Repat until convergence } \{\\
\theta_j\ :=\ \theta_j-\alpha\frac{1}{m}\sum_{i=1}^m\left(h_{\theta}(x^{\left(i\right)})-y^{\left(i\right)}\right)x_j^{\left(i\right)}\\
\}
\]</span></p>
<p>In this algorithm we are simultaneously updating the weights, <span class="math inline">\(\theta_j\)</span>, for all <span class="math inline">\(j\ \in\left(0,\dots,n\right)\)</span>. Recall that <span class="math inline">\(\theta_0\)</span> is the bias term and <span class="math inline">\(x_0^1\ =\ 1\)</span>.</p>
<p>In matrix form, our algorithm will look like this:</p>
<p><span class="math display">\[
\text{Repat until convergence } \{\\
\delta=\ \frac{1}{m}X^T\left(X\theta-y\right)\\
\theta:=\ \theta-\alpha\delta\\
\}
\]</span></p>
<p><strong>Logistic Regression</strong></p>
<p>The matrix form of the stochastic gradient descent algorithm has the form:</p>
<p><span class="math display">\[
\text{Repat until convergence } \{\\
\delta=\ \frac{1}{m}X^T\left(sigmoid\left(X\theta\right)-y\right)\\
\theta:=\ \theta-\alpha\delta\\
\}
\]</span></p>
</div>
<div id="gradient-descent-intuition" class="section level3 unnumbered">
<h3>Gradient Descent Intuition</h3>
<p>Figure <a href="lin-algs.html#fig:grad-descent-intuition">2.2</a> demonstrates the basic intuition behind the gradient algorithm. Fundamentally, if we pick a point along the graph of the cost function, and the gradient is negative, the algorithm will update by moving the more to the right. Conversely, if the gradient is positive, the algorithm will move the cost value more to the left.</p>
<div class="figure" style="text-align: center"><span id="fig:grad-descent-intuition"></span>
<img src="img/gradient-descent-example.png" alt="A simple example of gradient descent." width="90%" />
<p class="caption">
Figure 2.2: A simple example of gradient descent.
</p>
</div>
<p>Figure <a href="lin-algs.html#fig:surface-plot">2.3</a> is a multi-dimensional view of the cost function, and the underlying concept is still the same.</p>
<div class="figure" style="text-align: center"><span id="fig:surface-plot"></span>
<img src="img/surface-plot.png" alt="A surface plot of a quadratic cost function." width="90%" />
<p class="caption">
Figure 2.3: A surface plot of a quadratic cost function.
</p>
</div>
<p>Figure <a href="lin-algs.html#fig:log-entropy">2.4</a> is a plot of the log-entropy function for logistic regression.</p>
<div class="figure" style="text-align: center"><span id="fig:log-entropy"></span>
<img src="img/log-entropy-cost.png" alt="A simple example of gradient descent." width="90%" />
<p class="caption">
Figure 2.4: A simple example of gradient descent.
</p>
</div>
<p>While the ideal cost function to minimize would be a convex function, this is not always practical and there are ways to deal with that as discussed in the following video.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/8zdo6cnCW2w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<div id="practical-exercises" class="section level2">
<h2><span class="header-section-number">2.2</span> Practical Exercises</h2>
<p>This practical exercieses are based on code provided by <span class="citation">Fortuner (<a href="#ref-fortuner2017mlcheat">2017</a>)</span> and <span class="citation">Brownlee (<a href="#ref-brownlee2017mlmastery">2017</a><a href="#ref-brownlee2017mlmastery">b</a>)</span>.</p>
<div id="simple-linear-regression-1" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Simple Linear Regression</h3>
<p><strong>Data</strong></p>
<p>Suppose we have the following dataset in which we have a unique Company ID, radio advertising expenses in dollars, and annual sales as a result of those expenses in dollars.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/Advertising-Radio.csv&quot;</span>,<span class="dt">header=</span><span class="ot">TRUE</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(data)</code></pre></div>
<pre><code>##   company radio sales
## 1       1  37.8  22.1
## 2       2  39.3  10.4
## 3       3  45.9   9.3
## 4       4  41.3  18.5
## 5       5  10.8  12.9
## 6       6  48.9   7.2</code></pre>
<p>A view of the data shows that there appears to be a positive correlation between radio advertising spending and sales.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(data<span class="op">$</span>radio,data<span class="op">$</span>sales,<span class="dt">xlab=</span> <span class="st">&quot;Radio&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Sales&quot;</span>, <span class="dt">col=</span><span class="st">&quot;dodgerblue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</code></pre></div>
<p><img src="02-Linear-Algorithms_files/figure-html/sales-plot-1.png" width="672" /></p>
<p><strong>Making Predictions</strong></p>
<p>For this model, we want to predict sales based on the amount spent for radio advertising. Thus our formula will be</p>
<p><span class="math display">\[\text{Sales} = \text{Weight} \times \text{Radio} + \text{Bias}\]</span></p>
<p>The gradient descent algorithm will attempt to learn the optimal values for the Weight and Bias.</p>
<p><strong>Simple Regression Function</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">simple_regress &lt;-<span class="st"> </span><span class="cf">function</span>(features,weight,bias){
  <span class="kw">return</span>(weight<span class="op">*</span>features <span class="op">+</span><span class="st"> </span>bias)
}</code></pre></div>
<p><strong>Cost function Code</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cost_function &lt;-<span class="st"> </span><span class="cf">function</span>(features,targets,weight,bias){
  num_items &lt;-<span class="st"> </span><span class="kw">length</span>(targets)
  total_error &lt;-<span class="st"> </span><span class="dv">0</span>
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_along</span>(<span class="dv">1</span><span class="op">:</span>num_items)){
    total_error &lt;-<span class="st"> </span>total_error <span class="op">+</span><span class="st"> </span>(targets[i] <span class="op">-</span><span class="st"> </span>(weight <span class="op">*</span><span class="st"> </span>features[i] <span class="op">+</span><span class="st"> </span>bias))<span class="op">^</span><span class="dv">2</span>
  }
  <span class="kw">return</span>(total_error<span class="op">/</span>num_items)
}</code></pre></div>
<p><strong>Gradient Descent Code</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">update_weight &lt;-<span class="st"> </span><span class="cf">function</span>(features,targets,weight,bias,learning_rate){
  delta_weight &lt;-<span class="st"> </span><span class="dv">0</span>
  delta_bias &lt;-<span class="st"> </span><span class="dv">0</span>
  num_items &lt;-<span class="st"> </span><span class="kw">length</span>(targets)
  
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_along</span>(<span class="dv">1</span><span class="op">:</span>num_items)){
    <span class="co">#Calculate gradients</span>
    error &lt;-<span class="st"> </span>(targets[i] <span class="op">-</span><span class="st"> </span>(weight <span class="op">*</span><span class="st"> </span>features[i] <span class="op">+</span><span class="st"> </span>bias))
    delta_weight &lt;-<span class="st"> </span>delta_weight <span class="op">+</span><span class="st"> </span><span class="op">-</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>features[i] <span class="op">*</span><span class="st">  </span>error
    delta_bias &lt;-<span class="st"> </span>delta_bias <span class="op">+</span><span class="st"> </span><span class="op">-</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>error

  }
  
  weight &lt;-<span class="st"> </span>weight <span class="op">-</span><span class="st"> </span>learning_rate <span class="op">*</span><span class="st"> </span>(delta_weight<span class="op">/</span>num_items)
  bias &lt;-<span class="st"> </span>bias <span class="op">-</span><span class="st"> </span>learning_rate <span class="op">*</span><span class="st"> </span>(delta_bias<span class="op">/</span>num_items)
  

  
  res &lt;-<span class="st"> </span><span class="kw">c</span>(weight,bias)
  res
}</code></pre></div>
<p><strong>Training the model</strong></p>
<p>We are now ready to train the final model. To do this we will iterate over a set number of trials and update the weight and bias parameters at each iteration. We will also track the cost history.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train &lt;-<span class="st"> </span><span class="cf">function</span>(features,targets,weight,bias,learning_rate,iters){
  cost_history &lt;-<span class="st"> </span><span class="kw">numeric</span>(iters)
  coef_history &lt;-<span class="st"> </span><span class="kw">list</span>()
  
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_along</span>(<span class="dv">1</span><span class="op">:</span>iters)){
    tmp_coef &lt;-<span class="st"> </span><span class="kw">update_weight</span>(features,targets,weight,bias,learning_rate)
    weight &lt;-<span class="st"> </span>tmp_coef[<span class="dv">1</span>]
    bias &lt;-<span class="st"> </span>tmp_coef[<span class="dv">2</span>]
    coef_history[[i]] &lt;-<span class="st"> </span><span class="kw">c</span>(bias,weight)
    cost &lt;-<span class="st"> </span><span class="kw">cost_function</span>(features,targets,<span class="dt">weight =</span> weight, <span class="dt">bias =</span> bias)
    cost_history[i] &lt;-<span class="st"> </span>cost
    
    <span class="cf">if</span>(i <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">|</span><span class="st"> </span>i <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>){
       <span class="kw">cat</span>(<span class="st">&quot;iter: &quot;</span>, i, <span class="st">&quot;weight: &quot;</span>, weight, <span class="st">&quot;bias: &quot;</span>, bias, <span class="st">&quot;cost: &quot;</span>, cost, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)
    }
   
  }
  res &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">Weight =</span> weight, <span class="dt">Bias =</span> bias, <span class="dt">Cost =</span> cost_history,<span class="dt">Coefs =</span> coef_history)
  res
}

fit &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">features =</span> data<span class="op">$</span>radio,<span class="dt">targets =</span> data<span class="op">$</span>sales,<span class="dt">weight =</span> <span class="fl">0.03</span>, <span class="dt">bias =</span> <span class="fl">0.0014</span>, <span class="dt">learning_rate =</span> <span class="fl">0.001</span>,<span class="dt">iters =</span> <span class="dv">30</span>)</code></pre></div>
<pre><code>## iter:  1 weight:  0.7255664 bias:  0.02804636 cost:  86.42445 
## iter:  10 weight:  0.484637 bias:  0.06879067 cost:  42.72917 
## iter:  20 weight:  0.4837035 bias:  0.1219333 cost:  42.44643 
## iter:  30 weight:  0.4820883 bias:  0.1747496 cost:  42.1673</code></pre>
<p>The plot below shows how the <code>train()</code> funtion iterated through the coefficient history.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(data<span class="op">$</span>radio,data<span class="op">$</span>sales,<span class="dt">xlab=</span> <span class="st">&quot;Radio&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Sales&quot;</span>, <span class="dt">col=</span><span class="st">&quot;dodgerblue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>,<span class="dt">main=</span><span class="st">&quot;Final Plot With Coefficient History&quot;</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">30</span>){
  <span class="kw">abline</span>(<span class="dt">coef=</span>fit<span class="op">$</span>Coefs[[i]], <span class="dt">col =</span> <span class="kw">rgb</span>(<span class="fl">0.8</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.3</span>))
}
<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(fit<span class="op">$</span>Bias,fit<span class="op">$</span>Weight),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="02-Linear-Algorithms_files/figure-html/final-plot-1.png" width="672" /></p>
<p>This is a plot of the cost history.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit<span class="op">$</span>Cost,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Iteration&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Cost&quot;</span>,<span class="dt">main =</span> <span class="st">&quot;Error Rate Per Iteration&quot;</span>)</code></pre></div>
<p><img src="02-Linear-Algorithms_files/figure-html/cost-plot-1.png" width="672" /></p>
<p><strong>Exercises</strong></p>
<ol style="list-style-type: decimal">
<li>Run the command <code>res &lt;- lm(data$sales ~ data$radio)</code> and note the values for the weight and bias.</li>
<li>Plot the fitted line from <code>res</code> with the data and comapre that line to the trained model.</li>
<li>Adjust the fit object to obtain an estimate close to the noted parameters.</li>
</ol>
</div>
<div id="multiple-linear-regression-1" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Multiple Linear Regression</h3>
<p>For this exercise, we will predict total sales based on TV, Radio, and Newspaper advertising costs.</p>
<div id="data" class="section level4 unnumbered">
<h4>Data</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">multi_data &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/Advertising.csv&quot;</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>)
<span class="kw">head</span>(multi_data)</code></pre></div>
<pre><code>##   company    TV radio newspaper sales
## 1       1 230.1  37.8      69.2  22.1
## 2       2  44.5  39.3      45.1  10.4
## 3       3  17.2  45.9      69.3   9.3
## 4       4 151.5  41.3      58.5  18.5
## 5       5 180.8  10.8      58.4  12.9
## 6       6   8.7  48.9      75.0   7.2</code></pre>
<p>Since we are now dealing with multiple variables, we will need to view a pairs plot</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Code for panel.cor found at https://www.r-bloggers.com/scatter-plot-matrices-in-r/</span>

panel.cor &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, <span class="dt">digits =</span> <span class="dv">2</span>, cex.cor, ...)
{
  usr &lt;-<span class="st"> </span><span class="kw">par</span>(<span class="st">&quot;usr&quot;</span>); <span class="kw">on.exit</span>(<span class="kw">par</span>(usr))
  <span class="kw">par</span>(<span class="dt">usr =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>))
  <span class="co"># correlation coefficient</span>
  r &lt;-<span class="st"> </span><span class="kw">cor</span>(x, y)
  txt &lt;-<span class="st"> </span><span class="kw">format</span>(<span class="kw">c</span>(r, <span class="fl">0.123456789</span>), <span class="dt">digits =</span> digits)[<span class="dv">1</span>]
  txt &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;r= &quot;</span>, txt, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)
  <span class="kw">text</span>(<span class="fl">0.5</span>, <span class="fl">0.6</span>, txt)

  <span class="co"># p-value calculation</span>
  p &lt;-<span class="st"> </span><span class="kw">cor.test</span>(x, y)<span class="op">$</span>p.value
  txt2 &lt;-<span class="st"> </span><span class="kw">format</span>(<span class="kw">c</span>(p, <span class="fl">0.123456789</span>), <span class="dt">digits =</span> digits)[<span class="dv">1</span>]
  txt2 &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;p= &quot;</span>, txt2, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)
  <span class="cf">if</span>(p<span class="op">&lt;</span><span class="fl">0.01</span>) txt2 &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;p= &quot;</span>, <span class="st">&quot;&lt;0.01&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)
  <span class="kw">text</span>(<span class="fl">0.5</span>, <span class="fl">0.4</span>, txt2)
}

<span class="kw">pairs</span>(multi_data[,<span class="dv">2</span><span class="op">:</span><span class="dv">5</span>],<span class="dt">lower.panel =</span> panel.smooth, <span class="dt">upper.panel =</span> panel.cor)</code></pre></div>
<p><img src="02-Linear-Algorithms_files/figure-html/pairs-1.png" width="672" /></p>
</div>
<div id="cost-function-code" class="section level4">
<h4><span class="header-section-number">2.2.2.1</span> Cost Function Code</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">multi_cost &lt;-<span class="st"> </span><span class="cf">function</span>(features,target,theta){
  <span class="kw">sum</span>((features <span class="op">%*%</span><span class="st"> </span>theta <span class="op">-</span><span class="st"> </span>target)<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(<span class="dv">2</span><span class="op">*</span><span class="kw">length</span>(target))
}</code></pre></div>
</div>
<div id="training-function-code" class="section level4 unnumbered">
<h4>Training Function Code</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">multi_train &lt;-<span class="st"> </span><span class="cf">function</span>(features,target,theta,learn_rate,iters){
  cost_history &lt;-<span class="st"> </span><span class="kw">double</span>(iters)
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_along</span>(cost_history)){
    error &lt;-<span class="st"> </span>(features <span class="op">%*%</span><span class="st"> </span>theta <span class="op">-</span><span class="st"> </span>target)
    delta &lt;-<span class="st"> </span><span class="kw">t</span>(features) <span class="op">%*%</span><span class="st"> </span>error <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(target)
    theta &lt;-<span class="st"> </span>theta  <span class="op">-</span><span class="st"> </span>learn_rate <span class="op">*</span><span class="st"> </span>delta
    cost_history[i] &lt;-<span class="st"> </span><span class="kw">multi_cost</span>(features,target,theta)
  }
  
  res &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">Coefs =</span> theta, <span class="dt">Costs =</span> cost_history)
  res
}</code></pre></div>
</div>
<div id="results" class="section level4 unnumbered">
<h4>Results</h4>
<p>To make computing the gradient easier, we will normalize the feature data such that <span class="math inline">\(x \in [-1,1]\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">normalize_data &lt;-<span class="st"> </span><span class="cf">function</span>(data){
  cols &lt;-<span class="st"> </span><span class="kw">ncol</span>(data)
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>cols){
    tmp_mean &lt;-<span class="st"> </span><span class="kw">mean</span>(data[,i])
    tmp_range &lt;-<span class="st"> </span><span class="kw">range</span>(data[,i])[<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span><span class="kw">range</span>(data[,i])[<span class="dv">1</span>]
    res &lt;-<span class="st"> </span>(data[,i] <span class="op">-</span><span class="st"> </span>tmp_mean) <span class="op">/</span><span class="st"> </span>tmp_range
    data[,i] &lt;-<span class="st"> </span>res
  }
  data
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">multi_features &lt;-<span class="st"> </span>multi_data[,<span class="dv">2</span><span class="op">:</span><span class="dv">4</span>]
multi_features &lt;-<span class="st"> </span><span class="kw">normalize_data</span>(multi_features)
multi_target &lt;-<span class="st"> </span>multi_data[,<span class="dv">5</span>]
features_matrix &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>,<span class="kw">as.matrix</span>(multi_features))
theta &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,<span class="dt">nrow=</span><span class="kw">ncol</span>(features_matrix))
<span class="kw">rownames</span>(theta) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Intercept&quot;</span>,<span class="kw">names</span>(multi_features))
num_iters &lt;-<span class="st"> </span><span class="dv">1000</span>
learn_rate &lt;-<span class="st"> </span><span class="fl">0.0005</span>
multi_fit &lt;-<span class="st"> </span><span class="kw">multi_train</span>(features_matrix,multi_target,theta,learn_rate,num_iters)
multi_fit<span class="op">$</span>Coefs</code></pre></div>
<pre><code>##                [,1]
## Intercept 5.5184872
## TV        0.5767349
## radio     0.4366550
## newspaper 0.1098191</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(multi_fit<span class="op">$</span>Costs,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;Iteration&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Cost&quot;</span>,<span class="dt">main=</span><span class="st">&quot;Error Rate Per Iteration&quot;</span>)</code></pre></div>
<p><img src="02-Linear-Algorithms_files/figure-html/multi-reults-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">test_fit &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>TV <span class="op">+</span><span class="st"> </span>radio <span class="op">+</span><span class="st"> </span>newspaper, <span class="dt">data =</span> multi_data)</code></pre></div>
</div>
<div id="exercises" class="section level4 unnumbered">
<h4>Exercises</h4>
<ol style="list-style-type: decimal">
<li>Tune the iterations and the learning rate and attempt to reduce the model cost.</li>
<li>Run the command <code>test_fit &lt;- lm(sales ~ TV + radio + sales, data = multi_data)</code></li>
<li>Compute the cost of <code>test_fit</code> (Hint: use <code>names(test_fit)</code> to find out how to extract the coefficients of the model )</li>
</ol>
</div>
</div>
<div id="logistic-regression-1" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Logistic Regression</h3>
<p>During this exercise, we will classify whether studens will pass (1) or fail (0) a test based on the amount of hours spent studying and hours slept.</p>
<div id="data-1" class="section level4 unnumbered">
<h4>Data</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_data &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/data_classification.csv&quot;</span>,<span class="dt">header=</span><span class="ot">TRUE</span>)
<span class="kw">head</span>(log_data)</code></pre></div>
<pre><code>##    studied      slept passed
## 1 4.855064 9.63996157      1
## 2 8.625440 0.05892653      0
## 3 3.828192 0.72319923      0
## 4 7.150955 3.89942042      1
## 5 6.477900 8.19818055      1
## 6 1.922270 1.33142727      0</code></pre>
<p>The plot below shows the current data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">color_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(log_data<span class="op">$</span>passed<span class="op">==</span><span class="dv">1</span>,<span class="st">&quot;orange&quot;</span>,<span class="st">&quot;blue&quot;</span>)
<span class="kw">plot</span>(log_data<span class="op">$</span>slept,log_data<span class="op">$</span>studied,<span class="dt">col=</span>color_vec,<span class="dt">xlab=</span><span class="st">&quot;Hours Slept&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Hours Studied&quot;</span>)
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>,<span class="kw">c</span>(<span class="st">&quot;Pass&quot;</span>,<span class="st">&quot;Fail&quot;</span>),<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;orange&quot;</span>,<span class="st">&quot;blue&quot;</span>),<span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p><img src="02-Linear-Algorithms_files/figure-html/log-plot-1.png" width="672" /></p>
</div>
<div id="generate-a-vector-of-predictions" class="section level4 unnumbered">
<h4>Generate a vector of predictions</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_predict &lt;-<span class="st"> </span><span class="cf">function</span>(features, theta){
  z &lt;-<span class="st"> </span>features <span class="op">%*%</span><span class="st"> </span>theta
  res &lt;-<span class="st"> </span><span class="kw">sigmoid</span>(z)
  res
}</code></pre></div>
</div>
<div id="cost-function-code-1" class="section level4 unnumbered">
<h4>Cost function code</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_cost &lt;-<span class="st"> </span><span class="cf">function</span>(features, theta, targets){
  m &lt;-<span class="st"> </span><span class="kw">length</span>(targets)
  g &lt;-<span class="st"> </span><span class="kw">log_predict</span>(features,theta)
  res &lt;-<span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>((<span class="op">-</span>targets <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(g)) <span class="op">-</span><span class="st"> </span>((<span class="dv">1</span><span class="op">-</span>targets) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>g)))
  res
}</code></pre></div>
</div>
<div id="training-model-code" class="section level4 unnumbered">
<h4>Training model code</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_train &lt;-<span class="st"> </span><span class="cf">function</span>(features,theta,targets,learn_rate, iters){
    cost_history &lt;-<span class="st"> </span><span class="kw">double</span>(iters)
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_along</span>(cost_history)){
    preds &lt;-<span class="st"> </span><span class="kw">log_predict</span>(features,theta)
    error &lt;-<span class="st"> </span>(preds <span class="op">-</span><span class="st"> </span>targets)
    delta &lt;-<span class="st"> </span><span class="kw">t</span>(features) <span class="op">%*%</span><span class="st"> </span>error <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(targets)
    theta &lt;-<span class="st"> </span>theta  <span class="op">-</span><span class="st"> </span>learn_rate <span class="op">*</span><span class="st"> </span>delta
    cost &lt;-<span class="st"> </span><span class="kw">log_cost</span>(features,theta,targets)
    cost_history[i] &lt;-<span class="st"> </span>cost
    
    <span class="cf">if</span>(i <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">|</span><span class="st"> </span>i <span class="op">%%</span><span class="st"> </span><span class="dv">1000</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>){
       <span class="kw">cat</span>(<span class="st">&quot;iter: &quot;</span>, i, <span class="st">&quot;cost: &quot;</span>, cost, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)
    }
  }
  
  res &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">Coefs =</span> theta, <span class="dt">Costs =</span> cost_history)
  res
}</code></pre></div>
</div>
<div id="decision-boundary-code" class="section level4 unnumbered">
<h4>Decision boundary code</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">boundary &lt;-<span class="st"> </span><span class="cf">function</span>(prob){
  res &lt;-<span class="st"> </span><span class="kw">ifelse</span>(prob<span class="op">&gt;=</span>.<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  res
}</code></pre></div>
</div>
<div id="classification-accuracy-code" class="section level4 unnumbered">
<h4>Classification accuracy code</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_accuracy &lt;-<span class="st"> </span><span class="cf">function</span>(preds,targets){
  diff &lt;-<span class="st"> </span>preds <span class="op">-</span><span class="st"> </span>targets
  res &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(diff)<span class="op">/</span><span class="kw">length</span>(diff)
}</code></pre></div>
</div>
<div id="results-1" class="section level4 unnumbered">
<h4>Results</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_features &lt;-<span class="st"> </span>log_data[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]
log_targets &lt;-<span class="st"> </span>log_data[,<span class="dv">3</span>]
log_design &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>,<span class="kw">as.matrix</span>(log_features))
log_theta &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,<span class="dt">nrow=</span><span class="kw">ncol</span>(log_design))
<span class="kw">rownames</span>(log_theta) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Intercept&quot;</span>,<span class="kw">names</span>(log_features))
learn_rate &lt;-<span class="st"> </span><span class="fl">0.02</span>
num_iters &lt;-<span class="st"> </span><span class="dv">3000</span>
log_fit &lt;-<span class="st"> </span><span class="kw">log_train</span>(log_design,log_theta,log_targets,learn_rate,num_iters)</code></pre></div>
<pre><code>## iter:  1 cost:  0.6582674 
## iter:  1000 cost:  0.4469503 
## iter:  2000 cost:  0.3773433 
## iter:  3000 cost:  0.3408168</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_fit<span class="op">$</span>Coefs</code></pre></div>
<pre><code>##                 [,1]
## Intercept -3.8281563
## studied    0.4802045
## slept      0.3435157</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(log_fit<span class="op">$</span>Costs,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</code></pre></div>
<p><img src="02-Linear-Algorithms_files/figure-html/log-results-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">predictions &lt;-<span class="st"> </span><span class="kw">log_predict</span>(log_design,log_fit<span class="op">$</span>Coefs)
classifications &lt;-<span class="st"> </span><span class="kw">boundary</span>(predictions)
fit_accuracy &lt;-<span class="st"> </span><span class="kw">log_accuracy</span>(classifications,log_targets)
fit_accuracy</code></pre></div>
<pre><code>## [1] 0.91</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(predictions,<span class="dt">col=</span>color_vec)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="fl">0.5</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">title</span>(<span class="st">&quot;Actual Classification vs Predicted Probability&quot;</span>)
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>,<span class="kw">c</span>(<span class="st">&quot;Pass&quot;</span>,<span class="st">&quot;Fail&quot;</span>),<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;orange&quot;</span>,<span class="st">&quot;blue&quot;</span>),<span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="dt">horiz =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="02-Linear-Algorithms_files/figure-html/log-res-plot-1.png" width="672" /></p>
</div>
<div id="exercises-1" class="section level4 unnumbered">
<h4>Exercises</h4>
<ol style="list-style-type: decimal">
<li>Tune the iterations and the learning rate and attempt to improve the model accuracy.</li>
<li>Run the command <code>test_log &lt;- glm(passed~slept+studied,family = 'binomial',data=log_data)</code></li>
<li>Compare the cost and accuracy of <code>test_log</code> with <code>log_fit</code>.</li>
</ol>
</div>
</div>
</div>
<div id="linear-discriminant-analysis" class="section level2">
<h2><span class="header-section-number">2.3</span> Linear Discriminant Analysis</h2>
<p>Another method to classify target variables is to use linear discriminant analysis (LDA). Similar to logistic regression we want to find <span class="math inline">\(\Pr\left(Y\ =\ k\ |X\ =\ x\right)\)</span>. Simply put, we want to determine the probability that the target variable <span class="math inline">\(Y\)</span> maps to <span class="math inline">\(K\ \ge\ 2\)</span> classes given a value <span class="math inline">\(X\ =\ x\)</span>. Using Bayes theorem, we can find this probability by</p>
<p><span class="math display">\[
\Pr\left(Y\ =\ k\ |X\ =\ x\right)\ =\ p_k\left(x\right)\ =\ \frac{\pi_kf_k\left(x\right)}{\sum_{l=1}^K\pi_lf_l\left(x\right)}
\]</span></p>
<p>Where <span class="math inline">\(\pi_k\)</span> is the probability of <span class="math inline">\(Y=k\)</span> and <span class="math inline">\(f_k(x)\)</span> is the likelihood function of <span class="math inline">\(P\left(X\ =\ x\ |Y\ =\ k\right)\)</span>. In most cases <span class="math inline">\(f_k(x)\)</span> is assumed to be Normal with mean <span class="math inline">\(\mu_k\)</span> and standard deviation <span class="math inline">\(\sigma_k\)</span>.</p>
<p>A reasonable question to ask is why we would use LDA when we could use logistic regression? There are a few reasons:</p>
<ol style="list-style-type: decimal">
<li>Logistic regression is for binary classification. You will need to use LDA and other non-linear variants for more than two classes.</li>
<li>Logistic regression parameter estimates are brittle with well-separated classes. LDA is more robust to this type of data.</li>
<li>Logistic regression is also brittle with small samples. LDA performs better especially if the predictors are approximately normally distributed. See <span class="citation">Brownlee (<a href="#ref-brownlee2017mlmastery">2017</a><a href="#ref-brownlee2017mlmastery">b</a>)</span> and <span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span> for more details.</li>
</ol>
<div id="lda-intuition" class="section level3 unnumbered">
<h3>LDA Intuition</h3>
<p>Figure <a href="lin-algs.html#fig:normal-lda">2.5</a>, adapted from <span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span>, shows the fundamental principle of LDA. On the left are two normal densities. The dashed vertical line indicates the Bayes decision boundary for classification of new data. In this example, an observation’s classification is green if its value is less than zero and red otherwise. On the right are 20 observations drawn from each class. The solid vertical line is the LDA boundary while the dashed vertical line is the Bayes decision boundary. Thus, we can observe that the LDA boundary will vary from the Bayes decision boundary. Also, we can note that some observations will overlap between classes.</p>
<div class="figure" style="text-align: center"><span id="fig:normal-lda"></span>
<img src="img/stat-learn-4.4.png" alt="Normal densities with a Bayes decsion boundry adapted from James et al. (2013)." width="90%" />
<p class="caption">
Figure 2.5: Normal densities with a Bayes decsion boundry adapted from James et al. (2013).
</p>
</div>
<p>Investigating this overlap deeper, Figure <a href="lin-algs.html#fig:lda-groups">2.6</a>, adapted from <span class="citation">Kuhn and Johnson (<a href="#ref-kuhn2013applied">2013</a>)</span>, shows the goal of LDA. Mostly, the purpose of LDA is to determine a boundary that maximizes the variance between groups of data</p>
<div class="figure" style="text-align: center"><span id="fig:lda-groups"></span>
<img src="img/applied-pred-Ch12Fig06.png" alt="A comparison of between and within group variance adapted from Kuhn and Johnson (2013)." width="90%" />
<p class="caption">
Figure 2.6: A comparison of between and within group variance adapted from Kuhn and Johnson (2013).
</p>
</div>
</div>
<div id="lda-estimates-for-one-predictor" class="section level3 unnumbered">
<h3>LDA Estimates for One Predictor</h3>
<p>When we have only one predictor, we want to obtain estimates for <span class="math inline">\(f_k(x)\)</span> and <span class="math inline">\(p_k(x)\)</span> and classify an observation for into a class in which <span class="math inline">\(p_k(x)\)</span> has the greatest value. As stated previously, we will assume <span class="math inline">\(f_k(x)\)</span> is Gaussian which means</p>
<p><span class="math display">\[
f\left(x\right)\ =\ \frac{1}{\sqrt{2\pi\sigma_k}}\exp\left(-\frac{1}{2\sigma_k^2}\left(x-\mu_k\right)^2\right)
\]</span></p>
<p>where <span class="math inline">\(\mu_k\)</span> and <span class="math inline">\(\sigma_k^{2}\)</span> are the mean and variance for class <span class="math inline">\(k\)</span>. We will also assume the variance is the same for all <span class="math inline">\(K\)</span> classes which means <span class="math inline">\(\sigma_1^2\ =\ \dots\ =\ \sigma_k^2\)</span> <span class="citation">(James et al. <a href="#ref-james2013introduction">2013</a>)</span>. Using these assumptions our Bayes formulation is now</p>
<p><span class="math display">\[
\begin{align}
p_k\left(x\right)\ &amp;=\ \frac{\pi_kf_k\left(x\right)}{\sum_{l=1}^K\pi_lf_l\left(x\right)}\\
&amp;= \frac{\pi_k\frac{1}{\sqrt{2\pi\sigma}}\exp\left(-\frac{1}{2\sigma^2}\left(x-\mu_k\right)^2\right)}{\sum_{l=1}^K\pi_l\frac{1}{\sqrt{2\pi\sigma}}\exp\left(-\frac{1}{2\sigma^2}\left(x-\mu_l\right)^2\right)}
\end{align}
\]</span></p>
<p>To determine which class has the highest likelihood for a particular observation, we will convert <span class="math inline">\(p_k(x)\)</span> into a scoring function <span class="math inline">\(\delta_k(x)\)</span> which is called the discriminant scoring function. The key to understanding the derivation of <span class="math inline">\(\delta_k(x)\)</span> is that we will keep only the parameters that affect the maximum classification probability and ignore those parameters that are constant for all <span class="math inline">\(K\)</span> classes.</p>
<p><span class="math display">\[
p_k\left(x\right)\ =\ \frac{\pi_{k\ }\frac{1}{\sqrt{2\pi\sigma}}\exp\left(-\frac{1}{2\sigma^2}\left(x-\mu_k\right)^2\right)}{\sum_{l=1}^K\pi_l\frac{1}{\sqrt{2\pi\sigma}}\exp\left(-\frac{1}{2\sigma^2}\left(x-\mu_l\right)^2\right)\ }
\]</span></p>
<p>Observing the original form of <span class="math inline">\(p_k(x)\)</span> we notice that the denominator is the same for all classes so we can safely ignore it.</p>
<p><span class="math display">\[
p_k^{&#39;}\left(x\right)\ =\ \pi_k\frac{1}{\sqrt{2\pi\sigma}}\exp\left(-\frac{1}{2\sigma^2}\left(x-\mu_k\right)^2\right)
\]</span></p>
<p>Next we will take the log of <span class="math inline">\(p_k^{&#39;}(x)\)</span></p>
<p><span class="math display">\[
p_k^{&#39;&#39;}\left(x\right)\ =\ \ln\left(\pi_k\right)\ +\ \ln\left(\frac{1}{\sqrt{2\pi\sigma}}\right)\ +\ \ln\left(\exp\left(-\frac{1}{2\sigma^2}\left(x-\mu_k\right)^2\right)\right)
\]</span></p>
<p>Using the similar logic we used previously, we notice the <span class="math inline">\(ln\left(\frac{1}{\sqrt{2\pi\sigma}}\right)\)</span> term is constant across all <span class="math inline">\(K\)</span> classes, and we can omit this term.</p>
<p><span class="math display">\[
\begin{align}
p_k^{&#39;&#39;&#39;}\ &amp;=\ \ln\left(\pi_k\right)\ +\ \ln\left(\exp\left(-\frac{1}{2\sigma^2}\left(x-\mu_k\right)^2\right)\right)\\
&amp;=\ \ln\left(\pi_k\right)\ +\ -\frac{1}{2\sigma^2}\left(x-\mu_k\right)^2\\
&amp;=\ \ln\left(\pi_k\right)\ +\ -\frac{1}{2\sigma^2}\left(x^2\ -\ 2x\mu_k\ +\ \mu_k^2\right)\\
&amp;=\ \ln\left(\pi_k\right)\ -\frac{x^2}{2\sigma^2}\ +\ \frac{2x\mu_k}{2\sigma^2}\ -\ \frac{\mu_k^2}{2\sigma^2}
\end{align}
\]</span></p>
<p>We can eliminate the <span class="math inline">\(-\frac{x^2}{2\sigma^2}\)</span> term since it is constant across all <span class="math inline">\(K\)</span> classes leaving us with a final value of</p>
<p><span class="math display">\[
\delta_k\left(x\right)\ =\ \frac{x\mu_k}{\sigma^2}\ -\ \frac{\mu_k^2}{2\sigma^2}\ +\ \ln\left(\pi_k\right)
\]</span></p>
<p>In practice, the parameters <span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(\sigma_k^{2}\)</span> and <span class="math inline">\(\pi_k\)</span> are estimated from the data by the following methods:</p>
<p><span class="math display">\[
\begin{align}
\hat{\pi_k}\ &amp;=\ \frac{n_k}{n}\\
\hat{\mu_k}\ &amp;=\ \frac{1}{n_k}\sum_{i:y_i=k}^{ }x_i\\
\hat{\sigma^2}\ &amp;=\ \frac{1}{n-K}\sum_{k=1}^K\sum_{i:y_i=k}^{ }\left(x-\hat{\mu_k}\right)^2
\end{align}
\]</span></p>
<p>To get a little more insight into this discriminant scoring function, suppose <span class="math inline">\(K\ =\ 2\)</span> and <span class="math inline">\(\pi_1\ =\ \pi_2\)</span>, then we will assign an observation to class 1 if <span class="math inline">\(2x(\mu_1 - \mu_2)\ =\ \mu_1^{2}\ - \mu_2^{2}\)</span> and class 2 otherwise. Also the Bayes decision boundary will be set at</p>
<p><span class="math display">\[
\begin{align}
2x\left(\mu_1\ -\ \mu_2\right)\ &amp;=\ \mu_1^2\ -\ \mu_2^2\\
x\ &amp;=\ \frac{\mu_1^2\ -\ \mu_2^2}{2\left(\mu_1\ -\ \mu_2\right)}\\
x\ &amp;=\ \frac{\mu_1\ +\ \mu_2}{2}
\end{align}
\]</span></p>
</div>
<div id="lda-with-miltiple-predictors" class="section level3 unnumbered">
<h3>LDA With Miltiple Predictors</h3>
<p>For this case, we assume that <span class="math inline">\(X = (X_1,X_2, \dots, X_p)\)</span> are drawn from a multivariate Gaussian distribution. Thus the density function <span class="math inline">\(f_k(x)\)</span> will take the form</p>
<p><span class="math display">\[
f_k\left(x\right)\ =\ \frac{1}{\left(2\pi\right)^{\frac{p}{2}}\left|\Sigma\right|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}\left(x-\mu_k\right)^T\Sigma^{-1}\left(x-\mu_k\right)\right)
\]</span></p>
<p>The main difference here than the one predictor model is the common covariance matrix <span class="math inline">\(\Sigma\)</span>. Performing the same algebra as previous, now using matrices, the discriminant function <span class="math inline">\(\delta_k(x)\)</span> becomes</p>
<p><span class="math display">\[
\delta_k\left(x\right)\ =\ x^T\Sigma^{-1}\mu_k\ -\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k+\log\left(\pi_k\right)
\]</span></p>
</div>
</div>
<div id="practical-exercise" class="section level2">
<h2><span class="header-section-number">2.4</span> Practical Exercise</h2>
<p>This contrived example consists of normally distributed values for data separated into two distinct classes as shown by the plot below.</p>
<div id="data-2" class="section level3 unnumbered">
<h3>Data</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lda_data &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/lda-toy.csv&quot;</span>,<span class="dt">header=</span><span class="ot">TRUE</span>)
<span class="kw">head</span>(lda_data)</code></pre></div>
<pre><code>##          x y
## 1 4.667798 0
## 2 5.509199 0
## 3 4.702792 0
## 4 5.956707 0
## 5 5.738622 0
## 6 5.027283 0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">col_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(lda_data<span class="op">$</span>y<span class="op">==</span><span class="dv">0</span>,<span class="st">&quot;orange&quot;</span>,<span class="st">&quot;blue&quot;</span>)
<span class="kw">plot</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">40</span>,lda_data<span class="op">$</span>x,<span class="dt">xlab =</span> <span class="st">&quot;predictor&quot;</span>,<span class="dt">ylab =</span> <span class="st">&quot;value&quot;</span>, <span class="dt">col =</span> col_vec)</code></pre></div>
<p><img src="02-Linear-Algorithms_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="lda-scoring" class="section level3 unnumbered">
<h3>LDA Scoring</h3>
<p>We will score each <span class="math inline">\(x\)</span> value and determine is score using the discriminat scoring function <span class="math inline">\(\delta_k(x)\)</span>. Afterwards, we will predict a class for the scored value and compare it to the actual class value <span class="math inline">\(y\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">num_class &lt;-<span class="st"> </span><span class="dv">2</span>
ldazero_index &lt;-<span class="st"> </span><span class="kw">which</span>(lda_data<span class="op">$</span>y<span class="op">==</span><span class="dv">0</span>)
prob_zero &lt;-<span class="st"> </span><span class="kw">length</span>(lda_data<span class="op">$</span>y[ldazero_index])<span class="op">/</span><span class="kw">length</span>(lda_data<span class="op">$</span>y)
prob_zero</code></pre></div>
<pre><code>## [1] 0.5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prob_one &lt;-<span class="st"> </span><span class="kw">length</span>(lda_data<span class="op">$</span>y[<span class="op">-</span>ldazero_index])<span class="op">/</span><span class="kw">length</span>(lda_data<span class="op">$</span>y)
prob_one</code></pre></div>
<pre><code>## [1] 0.5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu_zero &lt;-<span class="st"> </span><span class="kw">sum</span>(lda_data<span class="op">$</span>x[ldazero_index])<span class="op">/</span><span class="kw">length</span>(lda_data<span class="op">$</span>x[ldazero_index])
mu_zero</code></pre></div>
<pre><code>## [1] 4.975416</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu_one &lt;-<span class="st"> </span><span class="kw">sum</span>(lda_data<span class="op">$</span>x[<span class="op">-</span>ldazero_index])<span class="op">/</span><span class="kw">length</span>(lda_data<span class="op">$</span>x[<span class="op">-</span>ldazero_index])
mu_one</code></pre></div>
<pre><code>## [1] 20.08706</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">squaredev_zero &lt;-<span class="st"> </span><span class="kw">sum</span>((lda_data<span class="op">$</span>x[ldazero_index]<span class="op">-</span>mu_zero)<span class="op">^</span><span class="dv">2</span>)
squaredev_one &lt;-<span class="st"> </span><span class="kw">sum</span>((lda_data<span class="op">$</span>x[<span class="op">-</span>ldazero_index]<span class="op">-</span>mu_one)<span class="op">^</span><span class="dv">2</span>)
squaredev_zero</code></pre></div>
<pre><code>## [1] 10.15823</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">squaredev_one</code></pre></div>
<pre><code>## [1] 21.49317</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lda_var &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="kw">length</span>(lda_data<span class="op">$</span>x) <span class="op">-</span><span class="st"> </span>num_class) <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(squaredev_one,squaredev_zero)
lda_var</code></pre></div>
<pre><code>## [1] 0.8329315</code></pre>
<p>Now we create the discriminant scoring function</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">disc_score &lt;-<span class="st"> </span><span class="cf">function</span>(x,mu,sigma,prob){
 res &lt;-<span class="st"> </span>(x<span class="op">*</span>(mu<span class="op">/</span>sigma)) <span class="op">-</span><span class="st"> </span>(mu<span class="op">^</span><span class="dv">2</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>sigma)) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(prob)
 res
}
<span class="kw">disc_score</span>(lda_data<span class="op">$</span>x[<span class="dv">1</span>],mu_zero,lda_var,prob_zero)</code></pre></div>
<pre><code>## [1] 12.32936</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">disc_score</span>(lda_data<span class="op">$</span>x[<span class="dv">1</span>],mu_one,lda_var,prob_one)</code></pre></div>
<pre><code>## [1] -130.3349</code></pre>
<p>Finally we will make predictions and compare them to our training set data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">score_zero &lt;-<span class="st"> </span><span class="kw">disc_score</span>(lda_data<span class="op">$</span>x,mu_zero,lda_var,prob_zero)
score_one &lt;-<span class="st"> </span><span class="kw">disc_score</span>(lda_data<span class="op">$</span>x,mu_one,lda_var,prob_one)
preds &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="kw">length</span>(lda_data<span class="op">$</span>x))
<span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_along</span>(preds)){
  <span class="cf">if</span>(score_zero[i] <span class="op">&gt;</span><span class="st"> </span>score_one[i]){
    <span class="cf">next</span>
  }
  <span class="cf">else</span>{
    preds[i] &lt;-<span class="st"> </span><span class="dv">1</span>
  }
}
<span class="kw">table</span>(lda_data<span class="op">$</span>y,preds)</code></pre></div>
<pre><code>##    preds
##      0  1
##   0 20  0
##   1  0 20</code></pre>
<p>An examination of the table shows that we achieved 100% accuracy.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-ng2000cs229">
<p>Ng, Andrew. 2000. “CS229 Lecture Notes.”</p>
</div>
<div id="ref-fortuner2017mlcheat">
<p>Fortuner, Brendan. 2017. “Machine Learning Cheatsheet.” <a href="http://ml-cheatsheet.readthedocs.io/en/latest/index.html" class="uri">http://ml-cheatsheet.readthedocs.io/en/latest/index.html</a>.</p>
</div>
<div id="ref-brownlee2017mlmastery">
<p>Brownlee, Jason. 2017b. <em>Master Machine Learning Algorithms</em>. <a href="https://machinelearningmastery.com/master-machine-learning-algorithms/" class="uri">https://machinelearningmastery.com/master-machine-learning-algorithms/</a>.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Springer. <a href="http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf">http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf</a>.</p>
</div>
<div id="ref-kuhn2013applied">
<p>Kuhn, Max, and Kjell Johnson. 2013. <em>Applied Predictive Modeling</em>. Vol. 26. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ml-overview.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="non-lin-algs.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
